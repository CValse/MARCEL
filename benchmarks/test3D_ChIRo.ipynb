{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a92808ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon May 13 11:22:38 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A10G                    On  | 00000000:00:1D.0 Off |                    0 |\n",
      "|  0%   23C    P8              19W / 300W |      0MiB / 23028MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d846afd-a3fd-49fa-ab4f-bb6ef3dc37f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.0+cu121'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a17adde-c69c-4aa1-827a-8f6508b2fe20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in links: https://data.pyg.org/whl/torch-2.3.0+cu121.html\n",
      "Requirement already satisfied: torch-geometric in /home/ubuntu/.local/lib/python3.10/site-packages (2.5.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.31.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.9.5)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2024.3.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.4)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.4.2)\n",
      "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.26.4)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.13.0)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.8)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (4.66.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (6.0.5)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.4.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.9.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (23.2.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric) (2.1.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2024.2.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2.2.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (3.5.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (1.4.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch-geometric -f https://data.pyg.org/whl/torch-2.3.0+cu121.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1050ea3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in links: https://data.pyg.org/whl/torch-2.3.0+cu121.html\n",
      "Requirement already satisfied: torch-scatter in /home/ubuntu/.local/lib/python3.10/site-packages (2.1.2+pt23cu121)\n",
      "Requirement already satisfied: torch-sparse in /home/ubuntu/.local/lib/python3.10/site-packages (0.6.18+pt23cu121)\n",
      "Requirement already satisfied: torch-cluster in /home/ubuntu/.local/lib/python3.10/site-packages (1.6.3+pt23cu121)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-sparse) (1.13.0)\n",
      "Requirement already satisfied: numpy<2.3,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from scipy->torch-sparse) (1.26.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch-scatter torch-sparse torch-cluster -f https://data.pyg.org/whl/torch-2.3.0+cu121.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21cee94b-989c-4354-a213-5f6d29e4f2e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: ase in /home/ubuntu/.local/lib/python3.10/site-packages (3.22.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from ase) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from ase) (1.13.0)\n",
      "Requirement already satisfied: matplotlib>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from ase) (3.8.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.0->ase) (2.9.0.post0)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.0->ase) (10.3.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.0->ase) (1.4.5)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.0->ase) (4.51.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.0->ase) (0.12.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.0->ase) (3.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.0->ase) (24.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.0->ase) (1.2.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.1.0->ase) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install ase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c377884",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install --upgrade pytorch-lightning\n",
    "#!pip install transformers\n",
    "#!pip install lightning-bolts\n",
    "#!pip install lightning-utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45edc818",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from dataclasses import asdict\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "# from pl_bolts.optimizers import LinearWarmupCosineAnnealingLR\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingLR\n",
    "\n",
    "from config import Config\n",
    "from data.ee import EE\n",
    "from data.bde import BDE\n",
    "from data.drugs import Drugs\n",
    "from data.kraken import Kraken\n",
    "from happy_config import ConfigLoader\n",
    "from loaders.samplers import EnsembleSampler, EnsembleMultiBatchSampler\n",
    "from loaders.multibatch import MultiBatchLoader\n",
    "from utils.early_stopping import EarlyStopping, generate_checkpoint_filename\n",
    "\n",
    "from models.model_3d import Model3D\n",
    "from models.models_3d.chiro import ChIRo\n",
    "from models.models_3d.painn import PaiNN\n",
    "from models.models_3d.schnet import SchNet\n",
    "from models.models_3d.gemnet import GemNetT\n",
    "from models.models_3d.dimenet import DimeNet, DimeNetPlusPlus\n",
    "from models.models_3d.clofnet import ClofNet\n",
    "from models.models_3d.leftnet import LEFTNet\n",
    "from models.models_3d.chytorch_discrete import ChytorchDiscrete\n",
    "#from models.models_3d.chytorch_conformer import ChytorchConformer\n",
    "\n",
    "import pickle\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9b09137",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from pytorch_lightning.loggers import CSVLogger, TensorBoardLogger\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import LightningModule, Trainer, LightningDataModule\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CyclicLR, CosineAnnealingLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8091e7ee-675d-485d-9b73-e4f9a05565a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.model_3d import GroupedScaledMAELoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e757818c",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################\n",
    "import os\n",
    "import numpy as np\n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = gpus\n",
    "torch.cuda.empty_cache()\n",
    "#print(os.environ['CUDA_VISIBLE_DEVICES'])\n",
    "\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# If you are using a GPU, you should also set the seed for CUDA operations\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "np.random.seed(seed)\n",
    "################################################## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee749ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def r2_score_torch(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute the R-squared score.\n",
    "\n",
    "    Parameters:\n",
    "        y_true (torch.Tensor): The true target values.\n",
    "        y_pred (torch.Tensor): The predicted target values.\n",
    "\n",
    "    Returns:\n",
    "        float: The R-squared score.\n",
    "    \"\"\"\n",
    "    # Calculate the mean of the true target values\n",
    "    y_mean = torch.mean(y_true)\n",
    "    # Calculate the total sum of squares (TSS)\n",
    "    tss = torch.sum((y_true - y_mean) ** 2)\n",
    "    # Calculate the residual sum of squares (RSS)\n",
    "    rss = torch.sum((y_true - y_pred) ** 2)\n",
    "    # Calculate R-squared score\n",
    "    r2 = 1 - rss / tss\n",
    "\n",
    "    return r2.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08ff1411",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataModule(LightningDataModule):\n",
    "    def __init__(self, hparams, dataset=None, multitask = False, chiro_data = False):\n",
    "        super().__init__()\n",
    "        self.hparams.update(hparams.__dict__) if hasattr(hparams, \"__dict__\") else self.hparams.update(hparams)\n",
    "        self._saved_dataloaders = dict()\n",
    "        self.dataset = dataset\n",
    "        self.multitask = multitask\n",
    "        \n",
    "        if self.dataset is None:\n",
    "            self.variable_name = None\n",
    "            unique_variables = 1\n",
    "\n",
    "            if self.hparams.dataset == 'Drugs':\n",
    "                dataset = Drugs('/mnt/data/MARCEL/datasets/Drugs', max_num_conformers=self.hparams.max_num_conformers, chiro_data = chiro_data).shuffle()\n",
    "            elif self.hparams.dataset == 'Kraken':\n",
    "                dataset = Kraken('/mnt/data/MARCEL/datasets/Kraken', max_num_conformers=self.hparams.max_num_conformers, chiro_data = chiro_data).shuffle()\n",
    "            elif self.hparams.dataset == 'BDE':\n",
    "                dataset = BDE('/mnt/data/MARCEL/datasets/BDE', chiro_data = chiro_data).shuffle()\n",
    "                self.variable_name = 'is_ligand'\n",
    "                unique_variables = 2\n",
    "            elif self.hparams.dataset == 'EE':\n",
    "                dataset = EE('/mnt/data/MARCEL/datasets/EE', max_num_conformers=self.hparams.max_num_conformers, chiro_data = chiro_data).shuffle()\n",
    "                self.variable_name = 'config_id'\n",
    "                unique_variables = 2\n",
    "\n",
    "            if self.multitask:\n",
    "                self.hparams.target = 'all'\n",
    "                pass\n",
    "            else:\n",
    "                #autoscaling\n",
    "                target_id = dataset.descriptors.index(self.hparams.target)\n",
    "                dataset.y = dataset.y[:, target_id]\n",
    "                #mean = dataset.y.mean(dim=0, keepdim=True)\n",
    "                #std = dataset.y.std(dim=0, keepdim=True)\n",
    "                #dataset.y = ((dataset.y - mean) / std).to('cuda')\n",
    "                #mean = mean.to('cuda')\n",
    "                #std = std.to('cuda')\n",
    "            \n",
    "                #data.dataset.data.y = data.dataset.y\n",
    "            \n",
    "            self.dataset = dataset\n",
    "            self.max_atomic_num = self.dataset.data.x[:, 0].max().item() + 1\n",
    "            self.unique_variables = unique_variables\n",
    "            print('--done---')\n",
    "\n",
    "    def split_compute(self):\n",
    "\n",
    "        split = self.dataset.get_idx_split(train_ratio=self.hparams.train_ratio, \n",
    "                                      valid_ratio=self.hparams.valid_ratio, \n",
    "                                      seed=self.hparams.seed)\n",
    "        self.train_dataset = self.dataset[split['train']]\n",
    "        self.valid_dataset = self.dataset[split['valid']]\n",
    "        self.test_dataset = self.dataset[split['test']]\n",
    "\n",
    "        print(f'{len(self.train_dataset)} training data, {len(self.test_dataset)} test data and {len(self.valid_dataset)} validation data')\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return self._get_dataloader(self.train_dataset, \"train\")\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self._get_dataloader(self.valid_dataset, \"val\")\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return self._get_dataloader(self.test_dataset, \"test\")\n",
    "    \n",
    "    def _get_dataloader(self, dataset, stage, store_dataloader=True):\n",
    "        store_dataloader = store_dataloader\n",
    "        \n",
    "        if stage in self._saved_dataloaders and store_dataloader:\n",
    "            return self._saved_dataloaders[stage]\n",
    "\n",
    "        if self.hparams.model3d_augmentation:\n",
    "            strategy = 'random'\n",
    "        else:\n",
    "            strategy = 'first'\n",
    "            \n",
    "        if stage == \"train\":\n",
    "            shuffle=True                              \n",
    "        else:\n",
    "            shuffle=False\n",
    "            if stage == \"train\"=='test':\n",
    "                strategy = 'first'\n",
    "\n",
    "        if self.variable_name is None:\n",
    "            dl = DataLoader(dataset, batch_sampler=EnsembleSampler(dataset, \n",
    "                                                                   batch_size=self.hparams.batch_size, \n",
    "                                                                   strategy=strategy, \n",
    "                                                                   shuffle=shuffle),\n",
    "                           num_workers=20)\n",
    "        else:\n",
    "            dl = MultiBatchLoader(dataset, batch_sampler=EnsembleMultiBatchSampler(dataset, \n",
    "                                                                                   batch_size=self.hparams.batch_size, \n",
    "                                                                                   strategy=strategy, \n",
    "                                                                                   shuffle=shuffle, \n",
    "                                                                                   variable_name=self.variable_name),\n",
    "                                 num_workers=20)\n",
    "        if store_dataloader:\n",
    "            self._saved_dataloaders[stage] = dl\n",
    "        return dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5796c8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def compute_pnorm(model: nn.Module) -> float:\n",
    "    \"\"\"\n",
    "    Computes the norm of the parameters of a model.\n",
    "    :param model: A PyTorch model.\n",
    "    :return: The norm of the parameters of the model.\n",
    "    \"\"\"\n",
    "    return math.sqrt(sum([p.norm().item() ** 2 for p in model.parameters() if p.requires_grad]))\n",
    "\n",
    "\n",
    "def compute_gnorm(model: nn.Module) -> float:\n",
    "    \"\"\"\n",
    "    Computes the norm of the gradients of a model.\n",
    "    :param model: A PyTorch model.\n",
    "    :return: The norm of the gradients of the model.\n",
    "    \"\"\"\n",
    "    return math.sqrt(sum([p.grad.norm().item() ** 2 for p in model.parameters() if p.grad is not None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc1085bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelLM(LightningModule):\n",
    "    def __init__(self, max_atomic_num=None, whole_dataset = None, unique_variables=1, multitask = False, **kwargs):\n",
    "        super().__init__()\n",
    "        #self.kwargs.update(kwargs.__dict__) if hasattr(kwargs, \"__dict__\") else self.kwargs.update(kwargs)\n",
    "        print(kwargs.get('model3d'))\n",
    "        if kwargs.get('model3d').model == 'SchNet':\n",
    "            model_factory = lambda: SchNet(max_atomic_num=max_atomic_num, \n",
    "                                           **asdict(kwargs.get('model3d').schnet))\n",
    "        elif kwargs.get('model3d').model == 'DimeNet':\n",
    "            model_factory = lambda: DimeNet(max_atomic_num=max_atomic_num, \n",
    "                                            **asdict(kwargs.get('model3d').dimenet))\n",
    "        elif kwargs.get('model3d').model == 'DimeNet++':\n",
    "            model_factory = lambda: DimeNetPlusPlus(max_atomic_num=max_atomic_num, \n",
    "                                                    **asdict(kwargs.get('model3d').dimenetplusplus))\n",
    "        elif kwargs.get('model3d').model == 'GemNet':\n",
    "            model_factory = lambda: GemNetT(max_atomic_num=max_atomic_num, \n",
    "                                            **asdict(kwargs.get('model3d').gemnet))\n",
    "        elif kwargs.get('model3d').model == 'ChIRo':\n",
    "            model_factory = lambda: ChIRo(**asdict(kwargs.get('model3d').chiro))\n",
    "            \n",
    "        elif kwargs.get('model3d').model == 'PaiNN':\n",
    "            model_factory = lambda: PaiNN(max_atomic_num=max_atomic_num, \n",
    "                                          **asdict(kwargs.get('model3d').painn))\n",
    "        elif kwargs.get('model3d').model == 'ClofNet':\n",
    "            model_factory = lambda: ClofNet(max_atomic_num=max_atomic_num, \n",
    "                                            **asdict(kwargs.get('model3d').clofnet))\n",
    "        elif kwargs.get('model3d').model == 'LEFTNet':\n",
    "            model_factory = lambda: LEFTNet(max_atomic_num=max_atomic_num, \n",
    "                                            **asdict(kwargs.get('model3d').leftnet))\n",
    "        elif kwargs.get('model3d').model == 'ChytorchDiscrete':\n",
    "            model_factory = lambda: ChytorchDiscrete(max_neighbors=max_atomic_num, \n",
    "                                                     **asdict(kwargs.get('model3d').chytorch_discrete))\n",
    "        elif kwargs.get('model3d').model == 'ChytorchConformer':\n",
    "            model_factory = lambda: ChytorchConformer(**asdict(kwargs.get('model3d').chytorch_conformer))\n",
    "            \n",
    "        elif kwargs.get('model3d').model == 'ChytorchRotary':\n",
    "            model_factory = lambda: ChytorchRotary(max_neighbors=max_atomic_num, \n",
    "                                                   **asdict(kwargs.get('model3d').chytorch_rotary))\n",
    "        self.device_= torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.multitask = multitask\n",
    "        self.net = Model3D(model_factory, \n",
    "                           hidden_dim=kwargs.get('hidden_dim'), \n",
    "                           out_dim=1,\n",
    "                           unique_variables=unique_variables, \n",
    "                           device='cuda',\n",
    "                           multitask=self.multitask).to('cuda')\n",
    "        \n",
    "        self.whole_dataset = whole_dataset\n",
    "\n",
    "        if self.multitask:\n",
    "            self.loss_fn = GroupedScaledMAELoss(torch.ones(self.whole_dataset.y.shape[1], \n",
    "                                                           dtype=torch.long))\n",
    "        else:\n",
    "            self.loss_fn = nn.MSELoss() #LOGITS #GroupedScaledMAELoss(torch.ones(4, dtype=torch.long))\n",
    "        \n",
    "        self.lr = kwargs.get('learning_rate')\n",
    "        self.wd = kwargs.get('learning_rate')\n",
    "        \n",
    "        self._reset_losses_dict()\n",
    "        self._reset_inference_results()\n",
    "        self.save_hyperparameters(ignore=[\"cosine_annealing_lr\",\"linear_warmup_cosine_annealing_lr\",\n",
    "                                          \"model1d\",\"model2d\",\"model3d\",\"model4d\",\"modelfprf\",\"one_cycle_lr\",\n",
    "                                          \"reduce_lr_on_plateau\",\"whole_dataset\",\"device\",\"scheduler\"])\n",
    "\n",
    "    def forward(self, batch):\n",
    "        out = self.net(batch)\n",
    "        return out\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        o = AdamW(self.parameters(), lr=self.lr, weight_decay=self.wd)\n",
    "        s = CyclicLR(o, self.lr, 2e-4, 1000, mode='triangular', cycle_momentum=False)\n",
    "        # instantiate the WeakMethod in the lr scheduler object into the custom scale function attribute\n",
    "        #s._scale_fn_custom = s._scale_fn_ref()\n",
    "        # remove the reference so there are no more WeakMethod references in the object\n",
    "        #s._scale_fn_ref = None\n",
    "        return [o], [{'scheduler': s, 'interval': 'step', 'name': 'lr_scheduler'}]\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        pnorm = compute_pnorm(self.net)\n",
    "        gnorm = compute_gnorm(self.net)\n",
    "        self.log(f'(training) pnorm', pnorm)\n",
    "        self.log(f'(training) gnorm', gnorm)\n",
    "        return self.step(batch, \"train\")\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self.step(batch, \"val\")\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        return self.step(batch, \"test\")\n",
    "        \n",
    "    def step(self, batch, stage):\n",
    "        start = time()\n",
    "        if type(batch) is not list:\n",
    "            batch = [batch]\n",
    "        molecule_idx = batch[0].molecule_idx.to('cuda')\n",
    "        dataset = self.whole_dataset.y.to('cuda')\n",
    "        targets = dataset[molecule_idx].squeeze()\n",
    "\n",
    "        with torch.set_grad_enabled(stage == \"train\"):\n",
    "            if self.multitask:\n",
    "                batch_multi = batch.copy()\n",
    "                if self.whole_dataset.y.shape[1]==1:\n",
    "                    print('only one property-switch to singletask')\n",
    "                else:\n",
    "                    for cnt, bat_i in enumerate(batch_multi):\n",
    "                        for i in range(self.whole_dataset.y.shape[1]-1):\n",
    "                            bat_tmp = batch_multi[cnt].batch#.detach().clone()\n",
    "                            batch_multi[cnt] = batch_multi[cnt].concat(batch[cnt])\n",
    "                            batch_multi[cnt].batch = torch.hstack([bat_tmp, batch[cnt].batch + (bat_tmp.max()+1)])\n",
    "                \n",
    "                targets_flat = targets.flatten()\n",
    "                prompts = torch.tensor([i for i in range(data.dataset.y.shape[1])]*targets.shape[0],\n",
    "                                      dtype=torch.int32,\n",
    "                                      device=targets.device)\n",
    "                for cnt, bat_i in enumerate(batch_multi):\n",
    "                    batch_multi[cnt].tokens = prompts\n",
    "                pred = self(batch_multi)\n",
    "                loss = self.loss_fn(pred.squeeze(), targets_flat, prompts)\n",
    "                if stage == \"test\":\n",
    "                    self.inference_results['token'].append(prompts.squeeze())\n",
    "                    self.inference_results['y_pred'].append(pred.squeeze())\n",
    "                    self.inference_results['y_true'].append(targets_flat.squeeze())\n",
    "                    return None\n",
    "                r2=r2_score_torch(targets_flat.cpu(),pred.squeeze().cpu().detach())\n",
    "            else:\n",
    "                pred = self(batch)\n",
    "                loss = self.loss_fn(pred.squeeze(), targets)\n",
    "            \n",
    "                if stage == \"test\":\n",
    "                    self.inference_results['y_pred'].append(pred.squeeze())\n",
    "                    self.inference_results['y_true'].append(targets.squeeze())\n",
    "                    return None\n",
    "\n",
    "                r2=r2_score_torch(targets.cpu(),pred.squeeze().cpu().detach())\n",
    "\n",
    "            self.logging_info[f'{stage}_loss'].append(loss.item())\n",
    "            self.logging_info[f'{stage}_r2'].append(r2)\n",
    "            self.logging_info[f'{stage}_time'].append(time()-start)\n",
    "            \n",
    "            if stage == 'train':\n",
    "                self.log(\"lr\", self.trainer.optimizers[0].param_groups[0][\"lr\"], on_step=True, on_epoch=False, prog_bar=True, logger=True, sync_dist=True)\n",
    "                self.log(f'{stage}_step_loss', loss.item(), on_step=True, on_epoch=False, prog_bar=True, logger=True, sync_dist=True)\n",
    "            #model.save_checkpoint('checkpoint.pth')\n",
    "            return loss\n",
    "        \n",
    "    def on_validation_epoch_end(self):\n",
    "        if not self.trainer.sanity_checking:                    \n",
    "            result_dict = {\n",
    "                \"epoch\": float(self.current_epoch),\n",
    "                \"train_epoch_loss\": torch.tensor(self.logging_info[\"train_loss\"]).mean().item(),\n",
    "                \"train_epoch_r2\": torch.tensor(self.logging_info[\"train_r2\"]).mean().item(),\n",
    "                \"val_epoch_loss\": torch.tensor(self.logging_info[\"val_loss\"]).mean().item(),\n",
    "                \"val_epoch_r2\": torch.tensor(self.logging_info[\"val_r2\"]).mean().item(),\n",
    "                \"train_epoch_time\": sum(self.logging_info[\"train_time\"]),\n",
    "                \"val_epoch_time\": sum(self.logging_info[\"val_time\"]),\n",
    "                }\n",
    "            self.log_dict(result_dict, logger=True, sync_dist=True)\n",
    "            \n",
    "        self._reset_losses_dict()\n",
    "    \n",
    "    def on_test_epoch_end(self) -> None:\n",
    "        for key in self.inference_results.keys():\n",
    "            self.inference_results[key] = torch.cat(self.inference_results[key], dim=0)\n",
    "    \n",
    "    def _reset_losses_dict(self):\n",
    "        self.logging_info = {\n",
    "            \"train_loss\": [],\n",
    "            \"train_r2\": [], \n",
    "            \"train_mse\": [], \n",
    "            \"val_loss\": [],\n",
    "            \"val_r2\": [],\n",
    "            \"train_sample_size\": [], \n",
    "            \"val_sample_size\": [],\n",
    "            \"train_time\": [],\n",
    "            \"val_time\": [],\n",
    "        }\n",
    "        \n",
    "    def _reset_inference_results(self):\n",
    "        self.inference_results = {'token': [],\n",
    "                                  'y_pred': [],\n",
    "                                  'y_true': []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0c7d3e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataname = 'Drugs'  #['BDE','Drugs','Kraken'],\n",
    "target = 'ip' #[['BindingEnergy'],['ip', 'ea', 'chi'],['sterimol_B5', 'sterimol_L', 'sterimol_burB5', 'sterimol_burL']]\n",
    "modeltype = 'ChIRo'  #['SchNet','GemNet','PaiNN','ClofNet','LEFTNet','DimeNet++','ChytorchDiscrete', 'ChIRo']  #oom: 'GemNet'\n",
    "\n",
    "writer = SummaryWriter(log_dir='/mnt/code/logs/')\n",
    "#loader = ConfigLoader(model=Config, config='params/params_1d.json')\n",
    "#config = loader()\n",
    "\n",
    "config = Config\n",
    "config.dataset = dataname\n",
    "config.target = target\n",
    "config.device = 'cuda:0'\n",
    "\n",
    "\n",
    "#config.hidden_dim =128\n",
    "#config.dropout = 0.5\n",
    "\n",
    "#config.train_ratio = 0.7\n",
    "#config.valid_ratio = 0.1\n",
    "config.batch_size = 64 #256\n",
    "config.max_num_conformers=19 #20\n",
    "#config.patience = 200\n",
    "\n",
    "#config.learning_rate = 0.00001\n",
    "#config.weight_decay = 1e-4\n",
    "#config.scheduler = None\n",
    "#config.reduce_lr_on_plateau = ReduceLROnPlateau()\n",
    "\n",
    "#config.num_epochs = 2000\n",
    "#config.cosine_annealing_lr = CosineAnnealingLR()\n",
    "#config.linear_warmup_cosine_annealing_lr = LinearWarmupCosineAnnealingLR()\n",
    "\n",
    "\n",
    "#not used in 1d\n",
    "\n",
    "#config.one_cycle_lr = OneCycleLR()\n",
    "#config.seed = 123\n",
    "#activation = 'relu'\n",
    "\n",
    "######3DMODEL\n",
    "config.model3d.model=modeltype\n",
    "config.model3d.augmentation = True\n",
    "\n",
    "#config.model3d.schnet = SchNet()\n",
    "#config.model3d.dimenet = DimeNet()\n",
    "#config.model3d.dimenetplusplus = DimeNetPlusPlus()\n",
    "#config.model3d.gemnet = GemNet()\n",
    "#config.model3d.painn = PaiNN()\n",
    "#config.model3d.clofnet = ClofNet()\n",
    "#config.model3d.leftnet= LEFTNet()\n",
    "#config.model3d.chytorch_discrete = ChytorchDiscrete()\n",
    "#config.model3d.chytorch_conformer = ChytorchConformer()\n",
    "#config.model3d.chytorch_rotary = ChytorchRotary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cce3a7da-8b67-4b4c-98dc-c92f28bcc67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def config_to_dict(config_class):\n",
    "    config_dict = {}\n",
    "    for attr_name in dir(config_class):\n",
    "        if not attr_name.startswith(\"__\") and not callable(getattr(config_class, attr_name)):\n",
    "            #if attr_name=='model1d':\n",
    "             #   attr_val = getattr(config_class, attr_name)\n",
    "             #   for subattr, subvalue in zip(['model', 'input_type', 'embedding_dim', 'num_layers', 'num_heads'],\n",
    "             #                                [attr_val.model, attr_val.input_type, attr_val.embedding_dim, \n",
    "             #                                 attr_val.num_layers, attr_val.num_heads]):\n",
    "             #       config_dict[f'{attr_name}_{subattr}'] = subvalue\n",
    "            #elif attr_name in ['model2d','model3d','model4d','modelfprf',\n",
    "            #                   'linear_warmup_cosine_annealing_lr','cosine_annealing_lr',\n",
    "            #                   'reduce_lr_on_plateau','one_cycle_lr']:\n",
    "            #    pass\n",
    "            #else:\n",
    "            config_dict[attr_name] = getattr(config_class, attr_name)\n",
    "    return config_dict\n",
    "config_dict = config_to_dict(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d61b8117-6368-49f0-bab2-1b12bea10ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "subkeys = [\"dataset\",\n",
    "           \"max_num_conformers\",\n",
    "           \"target\",\n",
    "           \"train_ratio\",\n",
    "           \"valid_ratio\",\n",
    "           \"seed\",\n",
    "           \"model3d\", #.augmentation\"\n",
    "           \"batch_size\"]\n",
    "config_dict_datamodule = {}\n",
    "for k,v in config_dict.items():\n",
    "    if k in subkeys:\n",
    "        if k==\"model3d\":\n",
    "            config_dict_datamodule[f'{k}_augmentation']=config_dict[k].augmentation\n",
    "        else:\n",
    "            config_dict_datamodule[k]=v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d24cfce3-1969-43a4-809d-36d81a4c8508",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 64,\n",
       " 'dataset': 'Drugs',\n",
       " 'max_num_conformers': 19,\n",
       " 'model3d_augmentation': True,\n",
       " 'seed': 123,\n",
       " 'target': 'ip',\n",
       " 'train_ratio': 0.7,\n",
       " 'valid_ratio': 0.1}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_dict_datamodule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4cb75383",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The given 'InMemoryDataset' only references a subset of examples of the full dataset, but 'data' will contain information of the full dataset. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--done---\n",
      "377446 training data, 108571 test data and 53663 validation data\n"
     ]
    }
   ],
   "source": [
    "data = DataModule(config_dict_datamodule, multitask = False, chiro_data = True)\n",
    "data.prepare_data()\n",
    "data.split_compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bdcc9684-4e13-4584-a94f-0f50e470b769",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[16492790, 52], edge_index=[2, 35542338], edge_attr=[35542338, 14], pos=[16492790, 3], bond_distances=[17771169], bond_distance_index=[2, 17771169], bond_angles=[24529818], bond_angle_index=[3, 24529818], dihedral_angles=[31464956], dihedral_angle_index=[4, 31464956], name=[539680], id=[539680], smiles=[539680], y=[539680, 4], molecule_idx=[539680])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dataset.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f24d052c-c317-4703-991f-e577c8b5dbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#databatch = [databatch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8cf296d0-dbdb-424e-aac4-f4853ec23c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#z, hgs, pos, bat = databatch[0].x[:, 0]+1, databatch[0].x[:, 4], databatch[0].pos, databatch[0].batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5435ff99-f4f4-482b-a442-cfbb608bf555",
   "metadata": {},
   "outputs": [],
   "source": [
    "#z.shape, hgs.shape, pos.shape, bat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3edb7560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model3D(model='ChIRo', augmentation=True, schnet=SchNet(hidden_dim=128, num_filters=5, num_interactions=6, num_gaussians=50, cutoff=10, readout='mean', dipole=False), dimenet=DimeNet(hidden_channels=128, out_channels=128, num_blocks=6, num_bilinear=8, num_spherical=7, num_radial=6), dimenetplusplus=DimeNetPlusPlus(hidden_channels=128, out_channels=128, num_blocks=4, int_emb_size=64, basis_emb_size=8, out_emb_channels=256, num_spherical=7, num_radial=6), gemnet=GemNet(num_spherical=7, num_radial=6, num_blocks=4, emb_size_atom=128, emb_size_edge=128, emb_size_trip=64, emb_size_rbf=16, emb_size_cbf=16, emb_size_bil_trip=64, num_before_skip=1, num_after_skip=1, num_concat=1, num_atoms=1, num_atom=2, bond_feat_dim=0), painn=PaiNN(hidden_dim=128, num_interactions=6, num_rbf=64, cutoff=12.0, readout='add', shared_interactions=False, shared_filters=False), clofnet=ClofNet(cutoff=6.5, num_layers=6, hidden_channels=128, num_radial=32), leftnet=LEFTNet(cutoff=6.5, num_layers=6, hidden_channels=128, num_radial=32), chiro=ChIRo(), chytorch_discrete=ChytorchDiscrete(max_distance=83, d_model=128, nhead=16, num_layers=8, dim_feedforward=512, shared_weights=False, dropout=0.1, norm_first=True, post_norm=True, zero_bias=True), chytorch_conformer=ChytorchConformer(nkernel=128, shared_layers=True, d_model=128, nhead=16, num_layers=8, dim_feedforward=512, dropout=0.1, norm_first=True, post_norm=True), chytorch_rotary=ChytorchRotary(d_model=128, nhead=16, num_layers=8, dim_feedforward=512, shared_weights=False, dropout=0.1, norm_first=False, post_norm=False))\n"
     ]
    }
   ],
   "source": [
    "model = ModelLM(max_atomic_num=data.max_atomic_num, \n",
    "                whole_dataset = data.dataset, \n",
    "                unique_variables=data.unique_variables, \n",
    "                multitask = False, **config_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04848537-eb05-4bc1-97a3-f6eabf0291dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6af591bd-453f-4444-8e90-7b25538cf0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl= data.train_dataloader()\n",
    "b = next(iter(dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2df37ec2-4983-4546-af0c-8612d1e2ecdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.model_3d import get_local_structure_map\n",
    "LS_map, alpha_indices = get_local_structure_map(b.dihedral_angle_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f419598b-d51b-4724-999e-0dc197ccca44",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = b\n",
    "dat = dat.to('cuda')\n",
    "LS_map = LS_map.to('cuda')\n",
    "alpha_indices = alpha_indices.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "52afec7f-f4a1-4df2-9933-56d869e1b787",
   "metadata": {},
   "outputs": [],
   "source": [
    "submod = model.net.models[0].to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "08ce97c5-12f2-4972-b1e5-026e0e94d849",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/code/benchmarks/models/models_3d/chiro.py:233: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:78.)\n",
      "  phase_shift_linear_output_normalized = phase_shift_linear_output / torch.max(phase_shift_norm, torch.cuda.FloatTensor(1).fill_(1e-12).squeeze(\n"
     ]
    }
   ],
   "source": [
    "emb = submod(dat, LS_map, alpha_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "774072d6-e783-4289-8c91-cc72b3576cc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 128])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "736be18b-351a-47df-a6ac-c8928e016161",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Symbolic(\n",
       "  (head): Linear(in_features=128, out_features=1, bias=False)\n",
       "  (scaler): Linear(in_features=128, out_features=1, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.net.linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "571b5a68-e268-4871-b0e8-6692cc771030",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from models.model_3d import Symbolic\n",
    "#linear = Symbolic(32, 1, bias = False)  #(hidden_dim * unique_variables, out_dim)#to symbolic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5859c263-8e6d-4cb9-bf89-0beacb703856",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear = model.net.linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4e273445-e23b-44a1-a818-0175074672d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "outs = linear(emb).squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e5eebaa3-7bc2-4c90-ba47-36c7fb3e94a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9e82ee2b-5cfe-4480-95d4-d6e3a5cd3f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model._log_hyperparams = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "69530b9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#PARAMS = 834563\n"
     ]
    }
   ],
   "source": [
    "print(f'#PARAMS = {sum(p.numel() for p in model.parameters() if p.requires_grad)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ffad2ebc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelLM(\n",
       "  (net): Model3D(\n",
       "    (models): ModuleList(\n",
       "      (0): ChIRo(\n",
       "        (Graph_Embedder): GraphNodeEmbedder(\n",
       "          (EConv): NNConv(52, 128, aggr=add, nn=MLP(\n",
       "            (activation_hidden): LeakyReLU(negative_slope=0.01)\n",
       "            (activation_out): Identity()\n",
       "            (linear_layers): ModuleList(\n",
       "              (0): Linear(in_features=14, out_features=64, bias=True)\n",
       "              (1): Linear(in_features=64, out_features=6656, bias=True)\n",
       "            )\n",
       "          ))\n",
       "          (GAT_layers): ModuleList(\n",
       "            (0): GATConv(128, 32, heads=8)\n",
       "            (1): GATConv(32, 128, heads=8)\n",
       "          )\n",
       "        )\n",
       "        (InternalCoordinateEncoder): InternalCoordinateEncoder(\n",
       "          (Encoder_D): MLP(\n",
       "            (activation_hidden): LeakyReLU(negative_slope=0.01)\n",
       "            (activation_out): Identity()\n",
       "            (linear_layers): ModuleList(\n",
       "              (0): Linear(in_features=257, out_features=32, bias=True)\n",
       "              (1): Linear(in_features=32, out_features=32, bias=True)\n",
       "              (2): Linear(in_features=32, out_features=128, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (Encoder_phi): MLP(\n",
       "            (activation_hidden): LeakyReLU(negative_slope=0.01)\n",
       "            (activation_out): Identity()\n",
       "            (linear_layers): ModuleList(\n",
       "              (0): Linear(in_features=386, out_features=32, bias=True)\n",
       "              (1): Linear(in_features=32, out_features=32, bias=True)\n",
       "              (2): Linear(in_features=32, out_features=128, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (Encoder_c): MLP(\n",
       "            (activation_hidden): LeakyReLU(negative_slope=0.01)\n",
       "            (activation_out): Identity()\n",
       "            (linear_layers): ModuleList(\n",
       "              (0): Linear(in_features=512, out_features=32, bias=True)\n",
       "              (1): Linear(in_features=32, out_features=32, bias=True)\n",
       "              (2): Linear(in_features=32, out_features=1, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (Encoder_sinusoidal_shift): MLP(\n",
       "            (activation_hidden): LeakyReLU(negative_slope=0.01)\n",
       "            (activation_out): Identity()\n",
       "            (linear_layers): ModuleList(\n",
       "              (0): Linear(in_features=512, out_features=256, bias=True)\n",
       "              (1-2): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
       "              (3): Linear(in_features=256, out_features=2, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (Encoder_alpha): MLP(\n",
       "            (activation_hidden): LeakyReLU(negative_slope=0.01)\n",
       "            (activation_out): Identity()\n",
       "            (linear_layers): ModuleList(\n",
       "              (0): Linear(in_features=257, out_features=32, bias=True)\n",
       "              (1): Linear(in_features=32, out_features=32, bias=True)\n",
       "              (2): Linear(in_features=32, out_features=128, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (linear): Symbolic(\n",
       "      (head): Linear(in_features=128, out_features=1, bias=False)\n",
       "      (scaler): Linear(in_features=128, out_features=1, bias=False)\n",
       "    )\n",
       "  )\n",
       "  (loss_fn): MSELoss()\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1a20c54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_name = f\"tmp_{dataname}_{target}_{modeltype}_multitask_v1\"\n",
    "\n",
    "dir_load_model = None\n",
    "log_dir_folder = '/mnt/code/logs/'\n",
    "log_dir_folder = os.path.join(log_dir_folder, dir_name)\n",
    "if os.path.exists(log_dir_folder):\n",
    "    if os.path.exists(os.path.join(log_dir_folder, \"last.ckpt\")):\n",
    "        dir_load_model = os.path.join(log_dir_folder, \"last.ckpt\")\n",
    "    csv_path = os.path.join(log_dir_folder, \"metrics.csv\")\n",
    "    while os.path.exists(csv_path):\n",
    "        csv_path = csv_path + '.bak'\n",
    "    if os.path.exists(os.path.join(log_dir_folder, \"metrics.csv\")):\n",
    "        os.rename(os.path.join(log_dir_folder, \"metrics.csv\"), csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "50c750fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_to_monitor = \"val_epoch_loss\"\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=log_dir_folder,\n",
    "    monitor=metric_to_monitor,\n",
    "    mode = 'min',\n",
    "    save_top_k=1,\n",
    "    save_last=True,\n",
    "    every_n_epochs=5,\n",
    "    save_weights_only=True,\n",
    "    verbose=True,\n",
    "    filename=\"best-model-{epoch}-{val_epoch_loss:.4f}\",\n",
    ")\n",
    "\n",
    "\n",
    "early_stopping = early_stop_callback = EarlyStopping(\n",
    "        monitor=metric_to_monitor,  # The metric you want to monitor\n",
    "        patience=config.patience,  # Number of epochs with no improvement after which training will be stopped\n",
    "        verbose=True,\n",
    "        mode='min'  # Minimizing the validation loss\n",
    "    )\n",
    "\n",
    "tb_logger = TensorBoardLogger(log_dir_folder, name=\"tensorbord\")#, version=\"\", default_hp_metric=False)\n",
    "csv_logger = CSVLogger(log_dir_folder, name=\"\", version=\"\")\n",
    "\n",
    "model_params = dict(\n",
    "    devices=1, #args['ngpus'],\n",
    "    accelerator='gpu', #args['accelerator'],\n",
    "    default_root_dir=log_dir_folder, #args['log_dir'],\n",
    "    logger=[tb_logger, csv_logger],\n",
    "    enable_progress_bar=True)\n",
    "\n",
    "\n",
    "model_params.update(dict(\n",
    "    max_epochs=config.num_epochs,#1000,\n",
    "    callbacks=[checkpoint_callback, early_stopping],\n",
    "    #enable_checkpointing=False,\n",
    "    gradient_clip_val=10,#args['clip_norm'],\n",
    "    #precision=\"16-mixed\",\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "843bb182",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision('medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "787f4d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Missing logger folder: /mnt/code/logs/tmp_Drugs_ip_ChIRo_multitask_v1/tensorbord\n",
      "/usr/local/lib/python3.10/dist-packages/lightning_fabric/loggers/csv_logs.py:268: Experiment logs directory /mnt/code/logs/tmp_Drugs_ip_ChIRo_multitask_v1/ exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!\n",
      "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py:653: Checkpoint directory /mnt/code/logs/tmp_Drugs_ip_ChIRo_multitask_v1 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type    | Params\n",
      "------------------------------------\n",
      "0 | net     | Model3D | 834 K \n",
      "1 | loss_fn | MSELoss | 0     \n",
      "------------------------------------\n",
      "834 K     Trainable params\n",
      "0         Non-trainable params\n",
      "834 K     Total params\n",
      "3.338     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c4409ddbed543d58800c4877486b703",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_epoch_loss improved. New best score: 2.995\n",
      "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(**model_params)\n",
    "trainer.fit(model, datamodule=data, ckpt_path=dir_load_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8e696c-e0f3-46d1-a392-bcfb57b64d3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5721a554-db16-425a-be5e-9247975381df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2b1687-429a-4e9e-9e97-d725da884896",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "98760096-e74b-4ba8-b072-9d43d749c4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor_candidates = checkpoint_callback._monitor_candidates(trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ba5b9cc4-d3a3-4299-b96c-289dfc605f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = checkpoint_callback.format_checkpoint_name(monitor_candidates, checkpoint_callback.CHECKPOINT_NAME_LAST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "370d4ca6-b286-4f08-b8d6-4aa4f2389ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = trainer._checkpoint_connector.dump_checkpoint(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "54e9f7f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object at key 'datamodule_hyper_parameters' of type '<class 'dict'>' cannot be pickled.\n",
      "Exception: cannot pickle 'getset_descriptor' object\n",
      "\n",
      "Non-picklable objects:\n",
      "- Key: 'datamodule_hyper_parameters', Type: '<class 'dict'>'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import io\n",
    "\n",
    "def identify_non_picklable_objects(checkpoint):\n",
    "    non_picklable_objects = []\n",
    "    for key, value in checkpoint.items():\n",
    "        try:\n",
    "            # Attempt to pickle the object\n",
    "            torch.save(value, io.BytesIO())\n",
    "        except Exception as e:\n",
    "            # If an exception occurs, it means the object is not picklable\n",
    "            non_picklable_objects.append((key, type(value)))\n",
    "            print(f\"Object at key '{key}' of type '{type(value)}' cannot be pickled.\")\n",
    "            print(f\"Exception: {e}\")\n",
    "    return non_picklable_objects\n",
    "\n",
    "# Identify non-picklable objects\n",
    "non_picklable_objects = identify_non_picklable_objects(checkpoint)\n",
    "\n",
    "# Print out non-picklable objects\n",
    "print(\"\\nNon-picklable objects:\")\n",
    "for key, obj_type in non_picklable_objects:\n",
    "    print(f\"- Key: '{key}', Type: '{obj_type}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ef5e1720-4340-46a7-8bce-257ae35c71dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Key: 'epoch', Type: '<class 'int'>'\n",
      "- Key: 'global_step', Type: '<class 'int'>'\n",
      "- Key: 'pytorch-lightning_version', Type: '<class 'str'>'\n",
      "- Key: 'state_dict', Type: '<class 'collections.OrderedDict'>'\n",
      "- Key: 'loops', Type: '<class 'dict'>'\n",
      "- Key: 'hparams_name', Type: '<class 'str'>'\n",
      "- Key: 'hyper_parameters', Type: '<class 'dict'>'\n",
      "- Key: 'datamodule_hyper_parameters', Type: '<class 'dict'>'\n"
     ]
    }
   ],
   "source": [
    "for key, obj_type in checkpoint.items():\n",
    "    print(f\"- Key: '{key}', Type: '{type(obj_type)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcca3d98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
