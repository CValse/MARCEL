{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a92808ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue May 14 10:12:57 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A10G                    On  | 00000000:00:1D.0 Off |                    0 |\n",
      "|  0%   24C    P8              15W / 300W |      0MiB / 23028MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d846afd-a3fd-49fa-ab4f-bb6ef3dc37f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.0+cu121'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a01f0a5b-7b2a-488a-9431-2ab1845f815a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a17adde-c69c-4aa1-827a-8f6508b2fe20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in links: https://data.pyg.org/whl/torch-2.3.0+cu121.html\n",
      "Requirement already satisfied: torch-geometric in /home/ubuntu/.local/lib/python3.10/site-packages (2.5.3)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.26.4)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.9.5)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.8)\n",
      "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.13.0)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2024.3.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.4)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.4.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (4.66.4)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.31.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (4.0.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.9.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2024.2.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2.2.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (3.5.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (1.4.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch-geometric -f https://data.pyg.org/whl/torch-2.3.0+cu121.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1050ea3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in links: https://data.pyg.org/whl/torch-2.3.0+cu121.html\n",
      "Requirement already satisfied: torch-scatter in /home/ubuntu/.local/lib/python3.10/site-packages (2.1.2+pt23cu121)\n",
      "Requirement already satisfied: torch-sparse in /home/ubuntu/.local/lib/python3.10/site-packages (0.6.18+pt23cu121)\n",
      "Requirement already satisfied: torch-cluster in /home/ubuntu/.local/lib/python3.10/site-packages (1.6.3+pt23cu121)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-sparse) (1.13.0)\n",
      "Requirement already satisfied: numpy<2.3,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from scipy->torch-sparse) (1.26.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch-scatter torch-sparse torch-cluster -f https://data.pyg.org/whl/torch-2.3.0+cu121.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c201f252-ced4-463a-8579-1d78388d9fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "from dataclasses import asdict\n",
    "# from pl_bolts.optimizers import LinearWarmupCosineAnnealingLR\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingLR\n",
    "\n",
    "from config import Config\n",
    "from data.ee import EE_2D\n",
    "from data.bde import BDE\n",
    "from data.drugs import Drugs\n",
    "from data.kraken import Kraken\n",
    "from happy_config import ConfigLoader\n",
    "from loaders.samplers import EnsembleSampler, EnsembleMultiBatchSampler\n",
    "from loaders.multibatch import MultiBatchLoader\n",
    "from utils.early_stopping import EarlyStopping, generate_checkpoint_filename\n",
    "\n",
    "from models.model_2d import GIN, GPS, Model2D\n",
    "from models.models_2d.chemprop import ChemProp, transform_reversely_indexed_data\n",
    "#from models.models_2d.chytorch2d import Chytorch\n",
    "import os\n",
    "import shutil\n",
    "import math\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9b09137",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from pytorch_lightning.loggers import CSVLogger, TensorBoardLogger\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import LightningModule, Trainer, LightningDataModule\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CyclicLR, CosineAnnealingLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8091e7ee-675d-485d-9b73-e4f9a05565a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.model_3d import GroupedScaledMAELoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e757818c",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################\n",
    "import os\n",
    "import numpy as np\n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = gpus\n",
    "torch.cuda.empty_cache()\n",
    "#print(os.environ['CUDA_VISIBLE_DEVICES'])\n",
    "\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# If you are using a GPU, you should also set the seed for CUDA operations\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "np.random.seed(seed)\n",
    "################################################## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee749ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def r2_score_torch(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute the R-squared score.\n",
    "\n",
    "    Parameters:\n",
    "        y_true (torch.Tensor): The true target values.\n",
    "        y_pred (torch.Tensor): The predicted target values.\n",
    "\n",
    "    Returns:\n",
    "        float: The R-squared score.\n",
    "    \"\"\"\n",
    "    # Calculate the mean of the true target values\n",
    "    y_mean = torch.mean(y_true)\n",
    "    # Calculate the total sum of squares (TSS)\n",
    "    tss = torch.sum((y_true - y_mean) ** 2)\n",
    "    # Calculate the residual sum of squares (RSS)\n",
    "    rss = torch.sum((y_true - y_pred) ** 2)\n",
    "    # Calculate R-squared score\n",
    "    r2 = 1 - rss / tss\n",
    "\n",
    "    return r2.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "08ff1411",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataModule(LightningDataModule):\n",
    "    def __init__(self, hparams, dataset=None, multitask = False):\n",
    "        super().__init__()\n",
    "        self.hparams.update(hparams.__dict__) if hasattr(hparams, \"__dict__\") else self.hparams.update(hparams)\n",
    "        self._saved_dataloaders = dict()\n",
    "        self.dataset = dataset\n",
    "        self.multitask = multitask\n",
    "        \n",
    "        if self.dataset is None:\n",
    "            self.variable_name = None\n",
    "            unique_variables = 1\n",
    "            self.pre_transform = None\n",
    "            self.hparams.max_num_conformers = 18\n",
    "            \n",
    "            if self.hparams.model2d_model == 'GPS':\n",
    "                self.pre_transform = T.AddRandomWalkPE(walk_length=config.model2d.gps.walk_length, attr_name='pe')\n",
    "            elif self.hparams.model2d_model == 'ChemProp':\n",
    "                self.pre_transform = transform_reversely_indexed_data\n",
    "                \n",
    "            if self.hparams.dataset == 'Drugs':\n",
    "                dataset = Drugs('/mnt/data/MARCEL/datasets/Drugs', \n",
    "                                max_num_conformers=self.hparams.max_num_conformers,\n",
    "                                pre_transform=self.pre_transform).shuffle()\n",
    "            elif self.hparams.dataset == 'Kraken':\n",
    "                dataset = Kraken('/mnt/data/MARCEL/datasets/Kraken', \n",
    "                                 max_num_conformers=self.hparams.max_num_conformers,\n",
    "                                 pre_transform=self.pre_transform).shuffle()\n",
    "            elif self.hparams.dataset == 'BDE':\n",
    "                dataset = BDE('/mnt/data/MARCEL/datasets/BDE', \n",
    "                              pre_transform=self.pre_transform,\n",
    "                              max_num_conformers=self.hparams.max_num_conformers,).shuffle()\n",
    "                self.variable_name = 'is_ligand'\n",
    "                unique_variables = 2\n",
    "            elif self.hparams.dataset == 'tmQMg':\n",
    "                dataset = tmQMg('/mnt/data/MARCEL/datasets/tmQMg',\n",
    "                                pre_transform=self.pre_transform).shuffle()\n",
    "                unique_variables = 1\n",
    "            elif self.hparams.dataset == 'EE':\n",
    "                dataset = EE('/mnt/data/MARCEL/datasets/EE', max_num_conformers=self.hparams.max_num_conformers,\n",
    "                            pre_transform=self.pre_transform).shuffle()\n",
    "                self.variable_name = 'config_id'\n",
    "                unique_variables = 2\n",
    "\n",
    "            if self.multitask:\n",
    "                self.hparams.target = 'all'\n",
    "                pass\n",
    "            else:\n",
    "                #autoscaling\n",
    "                target_id = dataset.descriptors.index(self.hparams.target)\n",
    "                dataset.y = dataset.y[:, target_id]\n",
    "                #mean = dataset.y.mean(dim=0, keepdim=True)\n",
    "                #std = dataset.y.std(dim=0, keepdim=True)\n",
    "                #dataset.y = ((dataset.y - mean) / std).to('cuda')\n",
    "                #mean = mean.to('cuda')\n",
    "                #std = std.to('cuda')\n",
    "            \n",
    "                #data.dataset.data.y = data.dataset.y\n",
    "            \n",
    "            self.dataset = dataset\n",
    "            self.max_atomic_num = self.dataset.data.x[:, 0].max().item() + 1\n",
    "            self.unique_variables = unique_variables\n",
    "            print('--done---')\n",
    "\n",
    "    def split_compute(self):\n",
    "\n",
    "        split = self.dataset.get_idx_split(train_ratio=self.hparams.train_ratio, \n",
    "                                      valid_ratio=self.hparams.valid_ratio, \n",
    "                                      seed=self.hparams.seed)\n",
    "        self.train_dataset = self.dataset[split['train']]\n",
    "        self.valid_dataset = self.dataset[split['valid']]\n",
    "        self.test_dataset = self.dataset[split['test']]\n",
    "\n",
    "        print(f'{len(self.train_dataset)} training data, {len(self.test_dataset)} test data and {len(self.valid_dataset)} validation data')\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return self._get_dataloader(self.train_dataset, \"train\")\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self._get_dataloader(self.valid_dataset, \"val\")\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return self._get_dataloader(self.test_dataset, \"test\")\n",
    "    \n",
    "    def _get_dataloader(self, dataset, stage, store_dataloader=True):\n",
    "        store_dataloader = store_dataloader\n",
    "        \n",
    "        if stage in self._saved_dataloaders and store_dataloader:\n",
    "            return self._saved_dataloaders[stage]\n",
    "            \n",
    "        if stage == \"train\":\n",
    "            shuffle=True                              \n",
    "        else:\n",
    "            shuffle=False\n",
    "        strategy = 'first'\n",
    "\n",
    "        if self.variable_name is None:\n",
    "            dl = DataLoader(dataset, batch_sampler=EnsembleSampler(dataset, \n",
    "                                                                   batch_size=self.hparams.batch_size, \n",
    "                                                                   strategy=strategy, \n",
    "                                                                   shuffle=shuffle),\n",
    "                           num_workers=20)\n",
    "        else:\n",
    "            dl = MultiBatchLoader(dataset, batch_sampler=EnsembleMultiBatchSampler(dataset, \n",
    "                                                                                   batch_size=self.hparams.batch_size, \n",
    "                                                                                   strategy=strategy, \n",
    "                                                                                   shuffle=shuffle, \n",
    "                                                                                   variable_name=self.variable_name),\n",
    "                                 num_workers=20)\n",
    "        if store_dataloader:\n",
    "            self._saved_dataloaders[stage] = dl\n",
    "        return dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5796c8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def compute_pnorm(model: nn.Module) -> float:\n",
    "    \"\"\"\n",
    "    Computes the norm of the parameters of a model.\n",
    "    :param model: A PyTorch model.\n",
    "    :return: The norm of the parameters of the model.\n",
    "    \"\"\"\n",
    "    return math.sqrt(sum([p.norm().item() ** 2 for p in model.parameters() if p.requires_grad]))\n",
    "\n",
    "\n",
    "def compute_gnorm(model: nn.Module) -> float:\n",
    "    \"\"\"\n",
    "    Computes the norm of the gradients of a model.\n",
    "    :param model: A PyTorch model.\n",
    "    :return: The norm of the gradients of the model.\n",
    "    \"\"\"\n",
    "    return math.sqrt(sum([p.grad.norm().item() ** 2 for p in model.parameters() if p.grad is not None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "fc1085bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelLM(LightningModule):\n",
    "    def __init__(self, max_atomic_num=None, whole_dataset = None, unique_variables=1, multitask = False, **kwargs):\n",
    "        super().__init__()\n",
    "        #self.kwargs.update(kwargs.__dict__) if hasattr(kwargs, \"__dict__\") else self.kwargs.update(kwargs)\n",
    "\n",
    "        if kwargs.get('model2d').model == 'GIN':\n",
    "            kwargs.get('model2d').gin.virtual_node = False\n",
    "            model_factory = lambda: GIN(hidden_dim=kwargs.get('hidden_dim'), \n",
    "                                        act=kwargs.get('activation'),**asdict(kwargs.get('model2d').gin))\n",
    "\n",
    "        elif kwargs.get('model2d').model == 'GIN-VN':\n",
    "            kwargs.get('model2d').gin.virtual_node = True\n",
    "            model_factory = lambda: GIN(hidden_dim=kwargs.get('hidden_dim'), \n",
    "                                        act=kwargs.get('activation'),**asdict(kwargs.get('model2d').gin))\n",
    "        elif kwargs.get('model2d').model == 'GPS':\n",
    "            model_factory = lambda: GPS(hidden_dim=kwargs.get('hidden_dim'), \n",
    "                                        dropout=kwargs.get('dropout'), \n",
    "                                        act=kwargs.get('activation'),**asdict(kwargs.get('model2d').gps))\n",
    "            pre_transform = T.AddRandomWalkPE(walk_length=kwargs.get('model2d').gps.walk_length, \n",
    "                                              attr_name='pe')\n",
    "\n",
    "        elif kwargs.get('model2d').model == 'ChemProp':\n",
    "            model_factory = lambda: ChemProp(hidden_dim=kwargs.get('hidden_dim'), \n",
    "                                             act=kwargs.get('activation'),\n",
    "                                             **asdict(kwargs.get('model2d').chemprop))\n",
    "            pre_transform = transform_reversely_indexed_data\n",
    "\n",
    "        elif kwargs.get('model2d').model == 'Chytorch':\n",
    "            model_factory = lambda: Chytorch(**asdict(kwargs.get('model2d').chytorch2d))\n",
    "            \n",
    "        self.device_= torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.multitask = multitask\n",
    "        self.net = Model2D(model_factory, \n",
    "                           kwargs.get('hidden_dim'), \n",
    "                           out_dim=1,\n",
    "                           dropout=kwargs.get('dropout'), \n",
    "                           device='cuda', \n",
    "                           unique_variables=unique_variables).to('cuda')\n",
    "        \n",
    "        self.whole_dataset = whole_dataset\n",
    "\n",
    "        if self.multitask:\n",
    "            self.loss_fn = GroupedScaledMAELoss(torch.ones(self.whole_dataset.y.shape[1], \n",
    "                                                           dtype=torch.long))\n",
    "        else:\n",
    "            self.loss_fn = nn.MSELoss() #LOGITS #GroupedScaledMAELoss(torch.ones(4, dtype=torch.long))\n",
    "        \n",
    "        self.lr = kwargs.get('learning_rate')\n",
    "        self.wd = kwargs.get('weight_decay')\n",
    "        \n",
    "        self._reset_losses_dict()\n",
    "        self._reset_inference_results()\n",
    "        self.save_hyperparameters(ignore=[\"cosine_annealing_lr\",\"linear_warmup_cosine_annealing_lr\",\n",
    "                                          \"model1d\",\"model2d\",\"model3d\",\"model4d\",\"modelfprf\",\"one_cycle_lr\",\n",
    "                                          \"reduce_lr_on_plateau\",\"whole_dataset\",\"device\",\"scheduler\"])\n",
    "\n",
    "    def forward(self, batch):\n",
    "        out = self.net(batch)\n",
    "        return out\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        o = AdamW(self.parameters(), lr=self.lr, weight_decay=self.wd)\n",
    "        s = CyclicLR(o, self.lr, 2e-4, 1000, mode='triangular', cycle_momentum=False)\n",
    "        # instantiate the WeakMethod in the lr scheduler object into the custom scale function attribute\n",
    "        #s._scale_fn_custom = s._scale_fn_ref()\n",
    "        # remove the reference so there are no more WeakMethod references in the object\n",
    "        #s._scale_fn_ref = None\n",
    "        return [o], [{'scheduler': s, 'interval': 'step', 'name': 'lr_scheduler'}]\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        pnorm = compute_pnorm(self.net)\n",
    "        gnorm = compute_gnorm(self.net)\n",
    "        self.log(f'(training) pnorm', pnorm)\n",
    "        self.log(f'(training) gnorm', gnorm)\n",
    "        return self.step(batch, \"train\")\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self.step(batch, \"val\")\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        return self.step(batch, \"test\")\n",
    "        \n",
    "    def step(self, batch, stage):\n",
    "        start = time()\n",
    "        if type(batch) is not list:\n",
    "            batch = [batch]\n",
    "        molecule_idx = batch[0].molecule_idx.to('cuda')\n",
    "        dataset = self.whole_dataset.y.to('cuda')\n",
    "        targets = dataset[molecule_idx].squeeze()\n",
    "\n",
    "        with torch.set_grad_enabled(stage == \"train\"):\n",
    "            if self.multitask:\n",
    "                batch_multi = batch.copy()\n",
    "                if self.whole_dataset.y.shape[1]==1:\n",
    "                    print('only one property-switch to singletask')\n",
    "                else:\n",
    "                    for cnt, bat_i in enumerate(batch_multi):\n",
    "                        for i in range(self.whole_dataset.y.shape[1]-1):\n",
    "                            bat_tmp = batch_multi[cnt].batch#.detach().clone()\n",
    "                            batch_multi[cnt] = batch_multi[cnt].concat(batch[cnt])\n",
    "                            batch_multi[cnt].batch = torch.hstack([bat_tmp, batch[cnt].batch + (bat_tmp.max()+1)])\n",
    "                \n",
    "                targets_flat = targets.flatten()\n",
    "                prompts = torch.tensor([i for i in range(data.dataset.y.shape[1])]*targets.shape[0],\n",
    "                                      dtype=torch.int32,\n",
    "                                      device=targets.device)\n",
    "                for cnt, bat_i in enumerate(batch_multi):\n",
    "                    batch_multi[cnt].tokens = prompts\n",
    "                pred = self(batch_multi)\n",
    "                loss = self.loss_fn(pred.squeeze(), targets_flat, prompts)\n",
    "                if stage == \"test\":\n",
    "                    self.inference_results['token'].append(prompts.squeeze())\n",
    "                    self.inference_results['y_pred'].append(pred.squeeze())\n",
    "                    self.inference_results['y_true'].append(targets_flat.squeeze())\n",
    "                    return None\n",
    "                r2=r2_score_torch(targets_flat.cpu(),pred.squeeze().cpu().detach())\n",
    "            else:\n",
    "                pred = self(batch)\n",
    "                loss = self.loss_fn(pred.squeeze(), targets)\n",
    "            \n",
    "                if stage == \"test\":\n",
    "                    self.inference_results['y_pred'].append(pred.squeeze())\n",
    "                    self.inference_results['y_true'].append(targets.squeeze())\n",
    "                    return None\n",
    "\n",
    "                r2=r2_score_torch(targets.cpu(),pred.squeeze().cpu().detach())\n",
    "\n",
    "            self.logging_info[f'{stage}_loss'].append(loss.item())\n",
    "            self.logging_info[f'{stage}_r2'].append(r2)\n",
    "            self.logging_info[f'{stage}_time'].append(time()-start)\n",
    "            \n",
    "            if stage == 'train':\n",
    "                self.log(\"lr\", self.trainer.optimizers[0].param_groups[0][\"lr\"], on_step=True, on_epoch=False, prog_bar=True, logger=True, sync_dist=True)\n",
    "                self.log(f'{stage}_step_loss', loss.item(), on_step=True, on_epoch=False, prog_bar=True, logger=True, sync_dist=True)\n",
    "            #model.save_checkpoint('checkpoint.pth')\n",
    "            return loss\n",
    "        \n",
    "    def on_validation_epoch_end(self):\n",
    "        if not self.trainer.sanity_checking:                    \n",
    "            result_dict = {\n",
    "                \"epoch\": float(self.current_epoch),\n",
    "                \"train_epoch_loss\": torch.tensor(self.logging_info[\"train_loss\"]).mean().item(),\n",
    "                \"train_epoch_r2\": torch.tensor(self.logging_info[\"train_r2\"]).mean().item(),\n",
    "                \"val_epoch_loss\": torch.tensor(self.logging_info[\"val_loss\"]).mean().item(),\n",
    "                \"val_epoch_r2\": torch.tensor(self.logging_info[\"val_r2\"]).mean().item(),\n",
    "                \"train_epoch_time\": sum(self.logging_info[\"train_time\"]),\n",
    "                \"val_epoch_time\": sum(self.logging_info[\"val_time\"]),\n",
    "                }\n",
    "            self.log_dict(result_dict, logger=True, sync_dist=True)\n",
    "            \n",
    "        self._reset_losses_dict()\n",
    "    \n",
    "    def on_test_epoch_end(self) -> None:\n",
    "        for key in self.inference_results.keys():\n",
    "            self.inference_results[key] = torch.cat(self.inference_results[key], dim=0)\n",
    "    \n",
    "    def _reset_losses_dict(self):\n",
    "        self.logging_info = {\n",
    "            \"train_loss\": [],\n",
    "            \"train_r2\": [], \n",
    "            \"train_mse\": [], \n",
    "            \"val_loss\": [],\n",
    "            \"val_r2\": [],\n",
    "            \"train_sample_size\": [], \n",
    "            \"val_sample_size\": [],\n",
    "            \"train_time\": [],\n",
    "            \"val_time\": [],\n",
    "        }\n",
    "        \n",
    "    def _reset_inference_results(self):\n",
    "        self.inference_results = {'token': [],\n",
    "                                  'y_pred': [],\n",
    "                                  'y_true': []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0c7d3e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataname = 'Drugs'  #['BDE','Drugs','Kraken','tmQMg],\n",
    "target = 'ea' #[['BindingEnergy'],['ip', 'ea', 'chi'],['sterimol_B5', 'sterimol_L', 'sterimol_burB5', 'sterimol_burL']]'tzvp_dipole_moment'\n",
    "modeltype = 'GIN'  #['GIN','GIN-VN','GPS','ChemProp',''Chytorch']\n",
    "\n",
    "#writer = SummaryWriter(log_dir='/mnt/code/logs/')\n",
    "#loader = ConfigLoader(model=Config, config='params/params_1d.json')\n",
    "#config = loader()\n",
    "\n",
    "config = Config\n",
    "config.dataset = dataname\n",
    "config.target = target\n",
    "config.device = 'cuda:0'\n",
    "\n",
    "\n",
    "#config.hidden_dim =128\n",
    "#config.dropout = 0.5\n",
    "\n",
    "#config.train_ratio = 0.7\n",
    "#config.valid_ratio = 0.1\n",
    "config.batch_size = 64 #256\n",
    "#config.patience = 200\n",
    "\n",
    "#config.learning_rate = 0.00001\n",
    "#config.weight_decay = 1e-4\n",
    "#config.scheduler = None\n",
    "#config.reduce_lr_on_plateau = ReduceLROnPlateau()\n",
    "\n",
    "#config.num_epochs = 2000\n",
    "#config.cosine_annealing_lr = CosineAnnealingLR()\n",
    "#config.linear_warmup_cosine_annealing_lr = LinearWarmupCosineAnnealingLR()\n",
    "\n",
    "\n",
    "#not used in 1d\n",
    "\n",
    "#config.one_cycle_lr = OneCycleLR()\n",
    "#config.seed = 123\n",
    "#activation = 'relu'\n",
    "\n",
    "######3DMODEL\n",
    "config.model3d.model=modeltype\n",
    "config.model3d.augmentation = True\n",
    "\n",
    "#config.model3d.schnet = SchNet()\n",
    "#config.model3d.dimenet = DimeNet()\n",
    "#config.model3d.dimenetplusplus = DimeNetPlusPlus()\n",
    "#config.model3d.gemnet = GemNet()\n",
    "#config.model3d.painn = PaiNN()\n",
    "#config.model3d.clofnet = ClofNet()\n",
    "#config.model3d.leftnet= LEFTNet()\n",
    "#config.model3d.chytorch_discrete = ChytorchDiscrete()\n",
    "#config.model3d.chytorch_conformer = ChytorchConformer()\n",
    "#config.model3d.chytorch_rotary = ChytorchRotary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "cce3a7da-8b67-4b4c-98dc-c92f28bcc67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def config_to_dict(config_class):\n",
    "    config_dict = {}\n",
    "    for attr_name in dir(config_class):\n",
    "        if not attr_name.startswith(\"__\") and not callable(getattr(config_class, attr_name)):\n",
    "            config_dict[attr_name] = getattr(config_class, attr_name)\n",
    "    return config_dict\n",
    "config_dict = config_to_dict(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "bf5916a2-bd4d-40e7-b4d2-d2633d74c20e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activation': 'relu',\n",
       " 'batch_size': 64,\n",
       " 'cosine_annealing_lr': CosineAnnealingLR(eta_min=1e-06),\n",
       " 'dataset': 'Drugs',\n",
       " 'device': 'cuda:0',\n",
       " 'dropout': 0.5,\n",
       " 'hidden_dim': 128,\n",
       " 'learning_rate': 0.001,\n",
       " 'linear_warmup_cosine_annealing_lr': LinearWarmupCosineAnnealingLR(warmup_steps=200, max_epochs=2000),\n",
       " 'max_num_conformers': 20,\n",
       " 'max_num_molecules': None,\n",
       " 'model1d': Model1D(model='LSTM', input_type='SMILES', embedding_dim=128, num_layers=4, num_heads=4),\n",
       " 'model2d': Model2D(model='GIN', gin=GIN(num_layers=6, virtual_node=False), gps=GPS(num_layers=6, walk_length=20, num_heads=4), chemprop=ChemProp(num_layers=6)),\n",
       " 'model3d': Model3D(model='GIN', augmentation=True, schnet=SchNet(hidden_dim=128, num_filters=5, num_interactions=6, num_gaussians=50, cutoff=10, readout='mean', dipole=False), dimenet=DimeNet(hidden_channels=128, out_channels=128, num_blocks=6, num_bilinear=8, num_spherical=7, num_radial=6), dimenetplusplus=DimeNetPlusPlus(hidden_channels=128, out_channels=128, num_blocks=4, int_emb_size=64, basis_emb_size=8, out_emb_channels=256, num_spherical=7, num_radial=6), gemnet=GemNet(num_spherical=7, num_radial=6, num_blocks=4, emb_size_atom=128, emb_size_edge=128, emb_size_trip=64, emb_size_rbf=16, emb_size_cbf=16, emb_size_bil_trip=64, num_before_skip=1, num_after_skip=1, num_concat=1, num_atoms=1, num_atom=2, bond_feat_dim=0), painn=PaiNN(hidden_dim=128, num_interactions=6, num_rbf=64, cutoff=12.0, readout='add', shared_interactions=False, shared_filters=False), clofnet=ClofNet(cutoff=6.5, num_layers=6, hidden_channels=128, num_radial=32), leftnet=LEFTNet(cutoff=6.5, num_layers=6, hidden_channels=128, num_radial=32), chiro=ChIRo(), chytorch_discrete=ChytorchDiscrete(max_distance=83, d_model=128, nhead=16, num_layers=8, dim_feedforward=512, shared_weights=False, dropout=0.1, norm_first=True, post_norm=True, zero_bias=True), chytorch_conformer=ChytorchConformer(nkernel=128, shared_layers=True, d_model=128, nhead=16, num_layers=8, dim_feedforward=512, dropout=0.1, norm_first=True, post_norm=True), chytorch_rotary=ChytorchRotary(d_model=128, nhead=16, num_layers=8, dim_feedforward=512, shared_weights=False, dropout=0.1, norm_first=False, post_norm=False)),\n",
       " 'model4d': Model4D(graph_encoder='SchNet', set_encoder='Attention', schnet=SchNet(hidden_dim=128, num_filters=5, num_interactions=6, num_gaussians=50, cutoff=10, readout='mean', dipole=False), dimenet=DimeNet(hidden_channels=128, out_channels=128, num_blocks=6, num_bilinear=8, num_spherical=7, num_radial=6), dimenetplusplus=DimeNetPlusPlus(hidden_channels=128, out_channels=128, num_blocks=4, int_emb_size=64, basis_emb_size=8, out_emb_channels=256, num_spherical=7, num_radial=6), gemnet=GemNet(num_spherical=7, num_radial=6, num_blocks=4, emb_size_atom=128, emb_size_edge=128, emb_size_trip=64, emb_size_rbf=16, emb_size_cbf=16, emb_size_bil_trip=64, num_before_skip=1, num_after_skip=1, num_concat=1, num_atoms=1, num_atom=2, bond_feat_dim=0), painn=PaiNN(hidden_dim=128, num_interactions=6, num_rbf=64, cutoff=12.0, readout='add', shared_interactions=False, shared_filters=False), clofnet=ClofNet(cutoff=6.5, num_layers=6, hidden_channels=128, num_radial=32), transformer=TransformerPooling(num_heads=8, num_layers=2)),\n",
       " 'modelfprf': ModelFPRF(n_jobs=8, n_estimators=500, min_samples_leaf=2, min_samples_split=10, min_impurity_decrease=0, warm_start=True),\n",
       " 'num_epochs': 2000,\n",
       " 'one_cycle_lr': OneCycleLR(max_lr=0.001, steps_per_epoch=100),\n",
       " 'patience': 200,\n",
       " 'reduce_lr_on_plateau': ReduceLROnPlateau(mode='min', factor=0.8, patience=20),\n",
       " 'scheduler': None,\n",
       " 'seed': 123,\n",
       " 'target': 'ea',\n",
       " 'train_ratio': 0.7,\n",
       " 'valid_ratio': 0.1,\n",
       " 'weight_decay': 0.0001}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_dict#['model2d'].model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d61b8117-6368-49f0-bab2-1b12bea10ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "subkeys = [\"dataset\",\n",
    "           \"max_num_conformers\",\n",
    "           \"target\",\n",
    "           \"train_ratio\",\n",
    "           \"valid_ratio\",\n",
    "           \"seed\",\n",
    "           \"model2d\", #.augmentation\"\n",
    "           \"batch_size\"]\n",
    "config_dict_datamodule = {}\n",
    "for k,v in config_dict.items():\n",
    "    if k in subkeys:\n",
    "        if k==\"model2d\":\n",
    "            config_dict_datamodule[f'{k}_model']=config_dict[k].model\n",
    "        else:\n",
    "            config_dict_datamodule[k]=v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d24cfce3-1969-43a4-809d-36d81a4c8508",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 64,\n",
       " 'dataset': 'Drugs',\n",
       " 'max_num_conformers': 20,\n",
       " 'model2d_model': 'GIN',\n",
       " 'seed': 123,\n",
       " 'target': 'ea',\n",
       " 'train_ratio': 0.7,\n",
       " 'valid_ratio': 0.1}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_dict_datamodule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4cb75383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--done---\n",
      "376106 training data, 107537 test data and 53939 validation data\n"
     ]
    }
   ],
   "source": [
    "data = DataModule(config_dict_datamodule, multitask = False)\n",
    "data.prepare_data()\n",
    "data.split_compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d8823ba9-b6ac-49fb-bf5e-6eb601e220a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[16427415, 9], edge_index=[2, 35401392], edge_attr=[35401392, 3], pos=[16427415, 3], name=[537582], id=[537582], smiles=[537582], y=[537582, 4], molecule_idx=[537582])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dataset.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0d562cc6-949d-4df2-9785-efb3c86329a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[16427415, 9], edge_index=[2, 35401392], edge_attr=[35401392, 3], pos=[16427415, 3], name=[537582], id=[537582], smiles=[537582], y=[537582, 4], molecule_idx=[537582])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dataset.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5435ff99-f4f4-482b-a442-cfbb608bf555",
   "metadata": {},
   "outputs": [],
   "source": [
    "#z.shape, hgs.shape, pos.shape, bat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c0eb8188-b3c9-4fa1-b2ef-f52a70539e29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GIN'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_dict['model2d'].model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3edb7560",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = ModelLM(max_atomic_num=data.max_atomic_num, \n",
    "                whole_dataset = data.dataset, \n",
    "                unique_variables=data.unique_variables, \n",
    "                multitask = False, **config_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9e82ee2b-5cfe-4480-95d4-d6e3a5cd3f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model._log_hyperparams = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "69530b9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#PARAMS = 240513\n"
     ]
    }
   ],
   "source": [
    "print(f'#PARAMS = {sum(p.numel() for p in model.parameters() if p.requires_grad)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ffad2ebc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelLM(\n",
       "  (net): Model2D(\n",
       "    (models): ModuleList(\n",
       "      (0): GIN(\n",
       "        (act): ReLU()\n",
       "        (conv): ModuleList(\n",
       "          (0-5): 6 x GINEConv(nn=Sequential(\n",
       "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (4): ReLU()\n",
       "          ))\n",
       "        )\n",
       "        (atom_encoder): AtomEncoder(\n",
       "          (atom_embedding_list): ModuleList(\n",
       "            (0): Embedding(119, 128)\n",
       "            (1): Embedding(7, 128)\n",
       "            (2-3): 2 x Embedding(12, 128)\n",
       "            (4): Embedding(10, 128)\n",
       "            (5-6): 2 x Embedding(6, 128)\n",
       "            (7-8): 2 x Embedding(2, 128)\n",
       "          )\n",
       "        )\n",
       "        (bond_encoder): BondEncoder(\n",
       "          (bond_embedding_list): ModuleList(\n",
       "            (0): Embedding(5, 128)\n",
       "            (1): Embedding(6, 128)\n",
       "            (2): Embedding(2, 128)\n",
       "          )\n",
       "        )\n",
       "        (lin): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (linear): Linear(in_features=128, out_features=1, bias=True)\n",
       "  )\n",
       "  (loss_fn): MSELoss()\n",
       ")"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1a20c54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_name = f\"tmp_2D_{dataname}_{target}_{modeltype}_multitask_v0\"\n",
    "\n",
    "dir_load_model = None\n",
    "log_dir_folder = '/mnt/code/logs/'\n",
    "log_dir_folder = os.path.join(log_dir_folder, dir_name)\n",
    "if os.path.exists(log_dir_folder):\n",
    "    if os.path.exists(os.path.join(log_dir_folder, \"last.ckpt\")):\n",
    "        dir_load_model = os.path.join(log_dir_folder, \"last.ckpt\")\n",
    "    csv_path = os.path.join(log_dir_folder, \"metrics.csv\")\n",
    "    while os.path.exists(csv_path):\n",
    "        csv_path = csv_path + '.bak'\n",
    "    if os.path.exists(os.path.join(log_dir_folder, \"metrics.csv\")):\n",
    "        os.rename(os.path.join(log_dir_folder, \"metrics.csv\"), csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "50c750fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_to_monitor = \"val_epoch_loss\"\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=log_dir_folder,\n",
    "    monitor=metric_to_monitor,\n",
    "    mode = 'min',\n",
    "    save_top_k=1,\n",
    "    save_last=True,\n",
    "    every_n_epochs=5,\n",
    "    save_weights_only=True,\n",
    "    verbose=True,\n",
    "    filename=\"best-model-{epoch}-{val_epoch_loss:.4f}\",\n",
    ")\n",
    "\n",
    "\n",
    "early_stopping = early_stop_callback = EarlyStopping(\n",
    "        monitor=metric_to_monitor,  # The metric you want to monitor\n",
    "        patience=config.patience,  # Number of epochs with no improvement after which training will be stopped\n",
    "        verbose=True,\n",
    "        mode='min'  # Minimizing the validation loss\n",
    "    )\n",
    "\n",
    "tb_logger = TensorBoardLogger(log_dir_folder, name=\"tensorbord\")#, version=\"\", default_hp_metric=False)\n",
    "csv_logger = CSVLogger(log_dir_folder, name=\"\", version=\"\")\n",
    "\n",
    "model_params = dict(\n",
    "    devices=1, #args['ngpus'],\n",
    "    accelerator='gpu', #args['accelerator'],\n",
    "    default_root_dir=log_dir_folder, #args['log_dir'],\n",
    "    logger=[tb_logger, csv_logger],\n",
    "    enable_progress_bar=True)\n",
    "\n",
    "\n",
    "model_params.update(dict(\n",
    "    max_epochs=config.num_epochs,#1000,\n",
    "    callbacks=[checkpoint_callback, early_stopping],\n",
    "    #enable_checkpointing=False,\n",
    "    gradient_clip_val=10,#args['clip_norm'],\n",
    "    #precision=\"16-mixed\",\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "843bb182",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision('medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787f4d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type    | Params\n",
      "------------------------------------\n",
      "0 | net     | Model2D | 240 K \n",
      "1 | loss_fn | MSELoss | 0     \n",
      "------------------------------------\n",
      "240 K     Trainable params\n",
      "0         Non-trainable params\n",
      "240 K     Total params\n",
      "0.962     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dddd93109e34739a2c57cc9c65decd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_epoch_loss improved. New best score: 0.868\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_epoch_loss improved by 0.009 >= min_delta = 0.0. New best score: 0.859\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_epoch_loss improved by 0.012 >= min_delta = 0.0. New best score: 0.848\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_epoch_loss improved by 0.014 >= min_delta = 0.0. New best score: 0.833\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 4110: 'val_epoch_loss' reached 0.85116 (best 0.85116), saving model to '/mnt/code/logs/tmp_2D_Drugs_ea_GIN_multitask_v0/best-model-epoch=4-val_epoch_loss=0.8512.ckpt' as top 1\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(**model_params)\n",
    "trainer.fit(model, datamodule=data, ckpt_path=dir_load_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a051ec-e2f2-4933-9037-7c08cd5f4eb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
