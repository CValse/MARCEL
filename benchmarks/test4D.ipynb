{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a92808ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue May 14 14:11:42 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A10G                    On  | 00000000:00:1D.0 Off |                    0 |\n",
      "|  0%   28C    P8              16W / 300W |      0MiB / 23028MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d846afd-a3fd-49fa-ab4f-bb6ef3dc37f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.0+cu121'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a17adde-c69c-4aa1-827a-8f6508b2fe20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in links: https://data.pyg.org/whl/torch-2.3.0+cu121.html\n",
      "Requirement already satisfied: torch-geometric in /home/ubuntu/.local/lib/python3.10/site-packages (2.5.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (4.66.4)\n",
      "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.13.0)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.4)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.31.0)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.8)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2024.3.1)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.9.5)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.26.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (4.0.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (23.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (6.0.5)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.9.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric) (2.1.5)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2024.2.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (3.5.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (1.4.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch-geometric -f https://data.pyg.org/whl/torch-2.3.0+cu121.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1050ea3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in links: https://data.pyg.org/whl/torch-2.3.0+cu121.html\n",
      "Requirement already satisfied: torch-scatter in /home/ubuntu/.local/lib/python3.10/site-packages (2.1.2+pt23cu121)\n",
      "Requirement already satisfied: torch-sparse in /home/ubuntu/.local/lib/python3.10/site-packages (0.6.18+pt23cu121)\n",
      "Requirement already satisfied: torch-cluster in /home/ubuntu/.local/lib/python3.10/site-packages (1.6.3+pt23cu121)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-sparse) (1.13.0)\n",
      "Requirement already satisfied: numpy<2.3,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from scipy->torch-sparse) (1.26.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch-scatter torch-sparse torch-cluster -f https://data.pyg.org/whl/torch-2.3.0+cu121.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21cee94b-989c-4354-a213-5f6d29e4f2e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: ase in /home/ubuntu/.local/lib/python3.10/site-packages (3.22.1)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from ase) (1.13.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from ase) (1.26.4)\n",
      "Requirement already satisfied: matplotlib>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from ase) (3.8.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.0->ase) (24.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.0->ase) (2.9.0.post0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.0->ase) (3.1.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.0->ase) (4.51.0)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.0->ase) (10.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.0->ase) (0.12.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.0->ase) (1.2.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.0->ase) (1.4.5)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.1.0->ase) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install ase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c377884",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install --upgrade pytorch-lightning\n",
    "#!pip install transformers\n",
    "#!pip install lightning-bolts\n",
    "#!pip install lightning-utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45edc818",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from dataclasses import asdict\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "# from pl_bolts.optimizers import LinearWarmupCosineAnnealingLR\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingLR\n",
    "\n",
    "from config import Config\n",
    "from data.ee import EE\n",
    "from data.bde import BDE\n",
    "from data.drugs import Drugs\n",
    "from data.kraken import Kraken\n",
    "from data.tmqmg import tmQMg\n",
    "from happy_config import ConfigLoader\n",
    "from loaders.samplers import EnsembleSampler, EnsembleMultiBatchSampler\n",
    "from loaders.multibatch import MultiBatchLoader\n",
    "from utils.early_stopping import EarlyStopping, generate_checkpoint_filename\n",
    "\n",
    "from models.model_3d import Model3D\n",
    "from models.models_3d.chiro import ChIRo\n",
    "from models.models_3d.painn import PaiNN\n",
    "from models.models_3d.schnet import SchNet\n",
    "from models.models_3d.gemnet import GemNetT\n",
    "from models.models_3d.dimenet import DimeNet, DimeNetPlusPlus\n",
    "from models.models_3d.clofnet import ClofNet\n",
    "from models.models_3d.leftnet import LEFTNet\n",
    "from models.models_3d.chytorch_discrete import ChytorchDiscrete\n",
    "from models.model_4d import Model4D, SumPooling, MeanPooling, TransformerPooling, DeepSets, SelfAttentionPooling\n",
    "from loaders.utils import reorder_molecule_idx\n",
    "\n",
    "import pickle\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9b09137",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from pytorch_lightning.loggers import CSVLogger, TensorBoardLogger\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import LightningModule, Trainer, LightningDataModule\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CyclicLR, CosineAnnealingLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8091e7ee-675d-485d-9b73-e4f9a05565a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.model_3d import GroupedScaledMAELoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e757818c",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################\n",
    "import os\n",
    "import numpy as np\n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = gpus\n",
    "torch.cuda.empty_cache()\n",
    "#print(os.environ['CUDA_VISIBLE_DEVICES'])\n",
    "\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# If you are using a GPU, you should also set the seed for CUDA operations\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "np.random.seed(seed)\n",
    "################################################## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee749ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def r2_score_torch(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute the R-squared score.\n",
    "\n",
    "    Parameters:\n",
    "        y_true (torch.Tensor): The true target values.\n",
    "        y_pred (torch.Tensor): The predicted target values.\n",
    "\n",
    "    Returns:\n",
    "        float: The R-squared score.\n",
    "    \"\"\"\n",
    "    # Calculate the mean of the true target values\n",
    "    y_mean = torch.mean(y_true)\n",
    "    # Calculate the total sum of squares (TSS)\n",
    "    tss = torch.sum((y_true - y_mean) ** 2)\n",
    "    # Calculate the residual sum of squares (RSS)\n",
    "    rss = torch.sum((y_true - y_pred) ** 2)\n",
    "    # Calculate R-squared score\n",
    "    r2 = 1 - rss / tss\n",
    "\n",
    "    return r2.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08ff1411",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataModule(LightningDataModule):\n",
    "    def __init__(self, hparams, dataset=None, multitask = False):\n",
    "        super().__init__()\n",
    "        self.hparams.update(hparams.__dict__) if hasattr(hparams, \"__dict__\") else self.hparams.update(hparams)\n",
    "        self._saved_dataloaders = dict()\n",
    "        self.dataset = dataset\n",
    "        self.multitask = multitask\n",
    "        \n",
    "        if self.dataset is None:\n",
    "            self.variable_name = None\n",
    "            unique_variables = 1\n",
    "\n",
    "            if self.hparams.dataset == 'Drugs':\n",
    "                dataset = Drugs('/mnt/data/MARCEL/datasets/Drugs', max_num_conformers=self.hparams.max_num_conformers).shuffle()\n",
    "            elif self.hparams.dataset == 'Kraken':\n",
    "                dataset = Kraken('/mnt/data/MARCEL/datasets/Kraken', max_num_conformers=self.hparams.max_num_conformers).shuffle()\n",
    "            elif self.hparams.dataset == 'BDE':\n",
    "                dataset = BDE('/mnt/data/MARCEL/datasets/BDE').shuffle()\n",
    "                self.variable_name = 'is_ligand'\n",
    "                unique_variables = 2\n",
    "            elif self.hparams.dataset == 'tmQMg':\n",
    "                dataset = tmQMg('/mnt/data/MARCEL/datasets/tmQMg').shuffle()\n",
    "                unique_variables = 1\n",
    "            elif self.hparams.dataset == 'EE':\n",
    "                dataset = EE('/mnt/data/MARCEL/datasets/EE', max_num_conformers=self.hparams.max_num_conformers).shuffle()\n",
    "                self.variable_name = 'config_id'\n",
    "                unique_variables = 2\n",
    "\n",
    "            if self.multitask:\n",
    "                self.hparams.target = 'all'\n",
    "                pass\n",
    "            else:\n",
    "                #autoscaling\n",
    "                target_id = dataset.descriptors.index(self.hparams.target)\n",
    "                dataset.y = dataset.y[:, target_id]\n",
    "                #mean = dataset.y.mean(dim=0, keepdim=True)\n",
    "                #std = dataset.y.std(dim=0, keepdim=True)\n",
    "                #dataset.y = ((dataset.y - mean) / std).to('cuda')\n",
    "                #mean = mean.to('cuda')\n",
    "                #std = std.to('cuda')\n",
    "            \n",
    "                #data.dataset.data.y = data.dataset.y\n",
    "            \n",
    "            self.dataset = dataset\n",
    "            self.max_atomic_num = self.dataset.data.x[:, 0].max().item() + 1\n",
    "            self.unique_variables = unique_variables\n",
    "            print('--done---')\n",
    "\n",
    "    def split_compute(self):\n",
    "\n",
    "        split = self.dataset.get_idx_split(train_ratio=self.hparams.train_ratio, \n",
    "                                      valid_ratio=self.hparams.valid_ratio, \n",
    "                                      seed=self.hparams.seed)\n",
    "        self.train_dataset = self.dataset[split['train']]\n",
    "        self.valid_dataset = self.dataset[split['valid']]\n",
    "        self.test_dataset = self.dataset[split['test']]\n",
    "\n",
    "        print(f'{len(self.train_dataset)} training data, {len(self.test_dataset)} test data and {len(self.valid_dataset)} validation data')\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return self._get_dataloader(self.train_dataset, \"train\")\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self._get_dataloader(self.valid_dataset, \"val\")\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return self._get_dataloader(self.test_dataset, \"test\")\n",
    "    \n",
    "    def _get_dataloader(self, dataset, stage, store_dataloader=True):\n",
    "        store_dataloader = store_dataloader\n",
    "        \n",
    "        if stage in self._saved_dataloaders and store_dataloader:\n",
    "            return self._saved_dataloaders[stage]\n",
    "\n",
    "        strategy = 'all'\n",
    "            \n",
    "        if stage == \"train\":\n",
    "            shuffle=True                              \n",
    "        else:\n",
    "            shuffle=False\n",
    "\n",
    "        if self.variable_name is None:\n",
    "            dl = DataLoader(dataset, batch_sampler=EnsembleSampler(dataset, \n",
    "                                                                   batch_size=self.hparams.batch_size, \n",
    "                                                                   strategy=strategy, \n",
    "                                                                   shuffle=shuffle),\n",
    "                           num_workers=20)\n",
    "        else:\n",
    "            dl = MultiBatchLoader(dataset, batch_sampler=EnsembleMultiBatchSampler(dataset, \n",
    "                                                                                   batch_size=self.hparams.batch_size, \n",
    "                                                                                   strategy=strategy, \n",
    "                                                                                   shuffle=shuffle, \n",
    "                                                                                   variable_name=self.variable_name),\n",
    "                                 num_workers=20)\n",
    "        if store_dataloader:\n",
    "            self._saved_dataloaders[stage] = dl\n",
    "        return dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5796c8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def compute_pnorm(model: nn.Module) -> float:\n",
    "    \"\"\"\n",
    "    Computes the norm of the parameters of a model.\n",
    "    :param model: A PyTorch model.\n",
    "    :return: The norm of the parameters of the model.\n",
    "    \"\"\"\n",
    "    return math.sqrt(sum([p.norm().item() ** 2 for p in model.parameters() if p.requires_grad]))\n",
    "\n",
    "\n",
    "def compute_gnorm(model: nn.Module) -> float:\n",
    "    \"\"\"\n",
    "    Computes the norm of the gradients of a model.\n",
    "    :param model: A PyTorch model.\n",
    "    :return: The norm of the gradients of the model.\n",
    "    \"\"\"\n",
    "    return math.sqrt(sum([p.grad.norm().item() ** 2 for p in model.parameters() if p.grad is not None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fc1085bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelLM(LightningModule):\n",
    "    def __init__(self, whole_dataset = None, unique_variables=1, multitask = False, **kwargs):\n",
    "        super().__init__()\n",
    "        #self.kwargs.update(kwargs.__dict__) if hasattr(kwargs, \"__dict__\") else self.kwargs.update(kwargs)\n",
    "        #print(kwargs.get('model4d'))\n",
    "\n",
    "        max_atomic_num = whole_dataset.data.x[:, 0].max().item() + 1\n",
    "        \n",
    "        if kwargs.get('model4d').model == 'SchNet':\n",
    "            graph_model_factory = lambda: SchNet(max_atomic_num=max_atomic_num, \n",
    "                                           **asdict(kwargs.get('model4d').schnet))\n",
    "        elif kwargs.get('model4d').model == 'DimeNet':\n",
    "            graph_model_factory = lambda: DimeNet(max_atomic_num=max_atomic_num, \n",
    "                                            **asdict(kwargs.get('model4d').dimenet))\n",
    "        elif kwargs.get('model4d').model == 'DimeNet++':\n",
    "            graph_model_factory = lambda: DimeNetPlusPlus(max_atomic_num=max_atomic_num, \n",
    "                                                    **asdict(kwargs.get('model4d').dimenetplusplus))\n",
    "        elif kwargs.get('model4d').model == 'GemNet':\n",
    "            graph_model_factory = lambda: GemNetT(max_atomic_num=max_atomic_num, \n",
    "                                            **asdict(kwargs.get('model4d').gemnet))\n",
    "        elif kwargs.get('model4d').model == 'ChIRo':\n",
    "            graph_model_factory = lambda: ChIRo(**asdict(kwargs.get('model4d').chiro))\n",
    "            \n",
    "        elif kwargs.get('model4d').model == 'PaiNN':\n",
    "            graph_model_factory = lambda: PaiNN(max_atomic_num=max_atomic_num, \n",
    "                                          **asdict(kwargs.get('model4d').painn))\n",
    "        elif kwargs.get('model4d').model == 'ClofNet':\n",
    "            graph_model_factory = lambda: ClofNet(max_atomic_num=max_atomic_num, \n",
    "                                            **asdict(kwargs.get('model4d').clofnet))\n",
    "        elif kwargs.get('model4d').model == 'LEFTNet':\n",
    "            graph_model_factory = lambda: LEFTNet(max_atomic_num=max_atomic_num, \n",
    "                                            **asdict(kwargs.get('model4d').leftnet))\n",
    "        elif kwargs.get('model4d').model == 'ChytorchDiscrete':\n",
    "            graph_model_factory = lambda: ChytorchDiscrete(max_neighbors=max_atomic_num, \n",
    "                                                     **asdict(kwargs.get('model4d').chytorch_discrete))\n",
    "        elif kwargs.get('model4d').model == 'ChytorchConformer':\n",
    "            graph_model_factory = lambda: ChytorchConformer(**asdict(kwargs.get('model4d').chytorch_conformer))\n",
    "            \n",
    "        elif kwargs.get('model4d').model == 'ChytorchRotary':\n",
    "            graph_model_factory = lambda: ChytorchRotary(max_neighbors=max_atomic_num, \n",
    "                                                   **asdict(kwargs.get('model4d').chytorch_rotary))\n",
    "\n",
    "        if kwargs.get('model4d').set_encoder == 'Sum':\n",
    "            set_model_factory = lambda: SumPooling()\n",
    "        elif kwargs.get('model4d').set_encoder == 'Mean':\n",
    "            set_model_factory = lambda: MeanPooling()\n",
    "        elif kwargs.get('model4d').set_encoder == 'DeepSets':\n",
    "            set_model_factory = lambda: DeepSets(hidden_dim=kwargs.get('hidden_dim'))\n",
    "        elif kwargs.get('model4d').set_encoder == 'Attention':\n",
    "            set_model_factory = lambda: SelfAttentionPooling(hidden_dim=kwargs.get('hidden_dim'))\n",
    "        elif kwargs.get('model4d').set_encoder == 'Transformer':\n",
    "            set_model_factory = lambda: TransformerPooling(\n",
    "                hidden_dim=kwargs.get('hidden_dim'), **asdict(kwargs.get('model4d').transformer))\n",
    "\n",
    "        \n",
    "        self.device_= torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.multitask = multitask\n",
    "        \n",
    "        self.net = Model4D(hidden_dim=kwargs.get('hidden_dim'), out_dim=1,\n",
    "                           graph_model_factory=graph_model_factory,\n",
    "                           set_model_factory=set_model_factory,\n",
    "                           unique_variables=unique_variables, \n",
    "                           device='cuda',\n",
    "                           multitask=self.multitask).to('cuda')\n",
    "        \n",
    "        self.whole_dataset = whole_dataset\n",
    "\n",
    "        if self.multitask:\n",
    "            self.loss_fn = GroupedScaledMAELoss(torch.ones(self.whole_dataset.y.shape[1], \n",
    "                                                           dtype=torch.long))\n",
    "        else:\n",
    "            self.loss_fn = nn.MSELoss() #LOGITS #GroupedScaledMAELoss(torch.ones(4, dtype=torch.long))\n",
    "        \n",
    "        self.lr = kwargs.get('learning_rate')\n",
    "        self.wd = kwargs.get('learning_rate')\n",
    "        \n",
    "        self._reset_losses_dict()\n",
    "        self._reset_inference_results()\n",
    "        self.save_hyperparameters(ignore=[\"cosine_annealing_lr\",\"linear_warmup_cosine_annealing_lr\",\n",
    "                                          \"model1d\",\"model2d\",\"model3d\",\"model4d\",\"modelfprf\",\"one_cycle_lr\",\n",
    "                                          \"reduce_lr_on_plateau\",\"whole_dataset\",\"device\",\"scheduler\"])\n",
    "\n",
    "    def forward(self, batch, molecule_indices):\n",
    "        out = self.net(batch, molecule_indices)\n",
    "        return out\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        o = AdamW(self.parameters(), lr=self.lr, weight_decay=self.wd)\n",
    "        s = CyclicLR(o, self.lr, 2e-4, 1000, mode='triangular', cycle_momentum=False)\n",
    "        # instantiate the WeakMethod in the lr scheduler object into the custom scale function attribute\n",
    "        #s._scale_fn_custom = s._scale_fn_ref()\n",
    "        # remove the reference so there are no more WeakMethod references in the object\n",
    "        #s._scale_fn_ref = None\n",
    "        return [o], [{'scheduler': s, 'interval': 'step', 'name': 'lr_scheduler'}]\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        pnorm = compute_pnorm(self.net)\n",
    "        gnorm = compute_gnorm(self.net)\n",
    "        self.log(f'(training) pnorm', pnorm)\n",
    "        self.log(f'(training) gnorm', gnorm)\n",
    "        return self.step(batch, \"train\")\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self.step(batch, \"val\")\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        return self.step(batch, \"test\")\n",
    "        \n",
    "    def step(self, batch, stage):\n",
    "        start = time()\n",
    "        if type(batch) is not list:\n",
    "            batch = [batch]\n",
    "        molecule_indices = [reorder_molecule_idx(batch[i].molecule_idx).to('cuda') for i in range(len(batch))]\n",
    "        #molecule_idx = batch[0].molecule_idx.to('cuda')\n",
    "\n",
    "        unique_raw_molecule_idx = torch.unique_consecutive(batch[0].molecule_idx)\n",
    "        dataset = self.whole_dataset.y.to('cuda')\n",
    "        targets = dataset[unique_raw_molecule_idx].squeeze()\n",
    "\n",
    "        with torch.set_grad_enabled(stage == \"train\"):\n",
    "            if self.multitask:\n",
    "                batch_multi = batch.copy()\n",
    "                if self.whole_dataset.y.shape[1]==1:\n",
    "                    print('only one property-switch to singletask')\n",
    "                else:\n",
    "                    for cnt, bat_i in enumerate(batch_multi):\n",
    "                        for i in range(self.whole_dataset.y.shape[1]-1):\n",
    "                            bat_tmp = batch_multi[cnt].batch#.detach().clone()\n",
    "                            batch_multi[cnt] = batch_multi[cnt].concat(batch[cnt])\n",
    "                            batch_multi[cnt].batch = torch.hstack([bat_tmp, batch[cnt].batch + (bat_tmp.max()+1)])\n",
    "                \n",
    "                targets_flat = targets.flatten()\n",
    "                prompts = torch.tensor([i for i in range(data.dataset.y.shape[1])]*targets.shape[0],\n",
    "                                      dtype=torch.int32,\n",
    "                                      device=targets.device)\n",
    "                for cnt, bat_i in enumerate(batch_multi):\n",
    "                    batch_multi[cnt].tokens = prompts\n",
    "                \n",
    "                pred = self(batch_multi, molecule_indices)\n",
    "                loss = self.loss_fn(pred.squeeze(), targets_flat, prompts)\n",
    "                if stage == \"test\":\n",
    "                    self.inference_results['token'].append(prompts.squeeze())\n",
    "                    self.inference_results['y_pred'].append(pred.squeeze())\n",
    "                    self.inference_results['y_true'].append(targets_flat.squeeze())\n",
    "                    return None\n",
    "                r2=r2_score_torch(targets_flat.cpu(),pred.squeeze().cpu().detach())\n",
    "            else:\n",
    "                pred = self(batch, molecule_indices)\n",
    "                loss = self.loss_fn(pred.squeeze(), targets)\n",
    "            \n",
    "                if stage == \"test\":\n",
    "                    self.inference_results['y_pred'].append(pred.squeeze())\n",
    "                    self.inference_results['y_true'].append(targets.squeeze())\n",
    "                    return None\n",
    "\n",
    "                r2=r2_score_torch(targets.cpu(),pred.squeeze().cpu().detach())\n",
    "\n",
    "            self.logging_info[f'{stage}_loss'].append(loss.item())\n",
    "            self.logging_info[f'{stage}_r2'].append(r2)\n",
    "            self.logging_info[f'{stage}_time'].append(time()-start)\n",
    "            \n",
    "            if stage == 'train':\n",
    "                self.log(\"lr\", self.trainer.optimizers[0].param_groups[0][\"lr\"], on_step=True, on_epoch=False, prog_bar=True, logger=True, sync_dist=True)\n",
    "                self.log(f'{stage}_step_loss', loss.item(), on_step=True, on_epoch=False, prog_bar=True, logger=True, sync_dist=True)\n",
    "            #model.save_checkpoint('checkpoint.pth')\n",
    "            return loss\n",
    "        \n",
    "    def on_validation_epoch_end(self):\n",
    "        if not self.trainer.sanity_checking:                    \n",
    "            result_dict = {\n",
    "                \"epoch\": float(self.current_epoch),\n",
    "                \"train_epoch_loss\": torch.tensor(self.logging_info[\"train_loss\"]).mean().item(),\n",
    "                \"train_epoch_r2\": torch.tensor(self.logging_info[\"train_r2\"]).mean().item(),\n",
    "                \"val_epoch_loss\": torch.tensor(self.logging_info[\"val_loss\"]).mean().item(),\n",
    "                \"val_epoch_r2\": torch.tensor(self.logging_info[\"val_r2\"]).mean().item(),\n",
    "                \"train_epoch_time\": sum(self.logging_info[\"train_time\"]),\n",
    "                \"val_epoch_time\": sum(self.logging_info[\"val_time\"]),\n",
    "                }\n",
    "            self.log_dict(result_dict, logger=True, sync_dist=True)\n",
    "            \n",
    "        self._reset_losses_dict()\n",
    "    \n",
    "    def on_test_epoch_end(self) -> None:\n",
    "        for key in self.inference_results.keys():\n",
    "            self.inference_results[key] = torch.cat(self.inference_results[key], dim=0)\n",
    "    \n",
    "    def _reset_losses_dict(self):\n",
    "        self.logging_info = {\n",
    "            \"train_loss\": [],\n",
    "            \"train_r2\": [], \n",
    "            \"train_mse\": [], \n",
    "            \"val_loss\": [],\n",
    "            \"val_r2\": [],\n",
    "            \"train_sample_size\": [], \n",
    "            \"val_sample_size\": [],\n",
    "            \"train_time\": [],\n",
    "            \"val_time\": [],\n",
    "        }\n",
    "        \n",
    "    def _reset_inference_results(self):\n",
    "        self.inference_results = {'token': [],\n",
    "                                  'y_pred': [],\n",
    "                                  'y_true': []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cce3a7da-8b67-4b4c-98dc-c92f28bcc67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def config_to_dict(config_class):\n",
    "    config_dict = {}\n",
    "    for attr_name in dir(config_class):\n",
    "        if not attr_name.startswith(\"__\") and not callable(getattr(config_class, attr_name)):\n",
    "            config_dict[attr_name] = getattr(config_class, attr_name)\n",
    "    return config_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "89f77b87-9af9-4fcb-82ae-1ee22dae3577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BDE\n",
      "BindingEnergy\n",
      "ChytorchDiscrete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The given 'InMemoryDataset' only references a subset of examples of the full dataset, but 'data' will contain information of the full dataset. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--done---\n",
      "79637 training data, 22623 test data and 11838 validation data\n",
      "#PARAMS = 3395776\n",
      "BDE\n",
      "BindingEnergy\n",
      "ChIRo\n",
      "--done---\n",
      "79322 training data, 23377 test data and 11399 validation data\n",
      "#PARAMS = 1834246\n",
      "BDE\n",
      "BindingEnergy\n",
      "SchNet\n",
      "--done---\n",
      "80246 training data, 22803 test data and 11049 validation data\n",
      "#PARAMS = 470364\n",
      "BDE\n",
      "BindingEnergy\n",
      "GemNet\n",
      "--done---\n",
      "79657 training data, 23180 test data and 11261 validation data\n",
      "#PARAMS = 3945344\n",
      "BDE\n",
      "BindingEnergy\n",
      "PaiNN\n",
      "--done---\n",
      "78886 training data, 23442 test data and 11770 validation data\n",
      "#PARAMS = 2660352\n",
      "BDE\n",
      "BindingEnergy\n",
      "ClofNet\n",
      "--done---\n",
      "80455 training data, 22592 test data and 11051 validation data\n",
      "#PARAMS = 1250178\n",
      "BDE\n",
      "BindingEnergy\n",
      "DimeNet++\n",
      "--done---\n",
      "79794 training data, 23252 test data and 11052 validation data\n",
      "#PARAMS = 3994124\n",
      "Drugs\n",
      "ip\n",
      "ChytorchDiscrete\n",
      "--done---\n",
      "378811 training data, 108258 test data and 54294 validation data\n",
      "#PARAMS = 1689952\n",
      "ea\n",
      "ChytorchDiscrete\n",
      "--done---\n",
      "378590 training data, 108507 test data and 54266 validation data\n",
      "#PARAMS = 1689952\n",
      "chi\n",
      "ChytorchDiscrete\n",
      "--done---\n",
      "378743 training data, 108810 test data and 53810 validation data\n",
      "#PARAMS = 1689952\n",
      "Drugs\n",
      "ip\n",
      "ChIRo\n",
      "--done---\n",
      "378643 training data, 108778 test data and 53942 validation data\n",
      "#PARAMS = 917123\n",
      "ea\n",
      "ChIRo\n",
      "--done---\n",
      "378872 training data, 108193 test data and 54298 validation data\n",
      "#PARAMS = 917123\n",
      "chi\n",
      "ChIRo\n",
      "--done---\n",
      "379283 training data, 107898 test data and 54182 validation data\n",
      "#PARAMS = 917123\n",
      "Drugs\n",
      "ip\n",
      "SchNet\n",
      "--done---\n",
      "378526 training data, 108708 test data and 54129 validation data\n",
      "#PARAMS = 227246\n",
      "ea\n",
      "SchNet\n",
      "--done---\n",
      "378444 training data, 108021 test data and 54898 validation data\n",
      "#PARAMS = 227246\n",
      "chi\n",
      "SchNet\n",
      "--done---\n",
      "378768 training data, 108441 test data and 54154 validation data\n",
      "#PARAMS = 227246\n",
      "Drugs\n",
      "ip\n",
      "GemNet\n",
      "--done---\n",
      "378472 training data, 108906 test data and 53985 validation data\n",
      "#PARAMS = 1964736\n",
      "ea\n",
      "GemNet\n",
      "--done---\n",
      "379028 training data, 107665 test data and 54670 validation data\n",
      "#PARAMS = 1964736\n",
      "chi\n",
      "GemNet\n",
      "--done---\n",
      "379351 training data, 107493 test data and 54519 validation data\n",
      "#PARAMS = 1964736\n",
      "Drugs\n",
      "ip\n",
      "PaiNN\n",
      "--done---\n",
      "378294 training data, 108636 test data and 54433 validation data\n",
      "#PARAMS = 1322240\n",
      "ea\n",
      "PaiNN\n",
      "--done---\n",
      "378176 training data, 108974 test data and 54213 validation data\n",
      "#PARAMS = 1322240\n",
      "chi\n",
      "PaiNN\n",
      "--done---\n",
      "379057 training data, 108058 test data and 54248 validation data\n",
      "#PARAMS = 1322240\n",
      "Drugs\n",
      "ip\n",
      "ClofNet\n",
      "--done---\n",
      "379471 training data, 107526 test data and 54366 validation data\n",
      "#PARAMS = 617153\n",
      "ea\n",
      "ClofNet\n",
      "--done---\n",
      "377851 training data, 108809 test data and 54703 validation data\n",
      "#PARAMS = 617153\n",
      "chi\n",
      "ClofNet\n",
      "--done---\n",
      "379960 training data, 107206 test data and 54197 validation data\n",
      "#PARAMS = 617153\n",
      "Drugs\n",
      "ip\n",
      "DimeNet++\n",
      "--done---\n",
      "379789 training data, 107604 test data and 53970 validation data\n",
      "#PARAMS = 1989126\n",
      "ea\n",
      "DimeNet++\n",
      "--done---\n",
      "378620 training data, 108621 test data and 54122 validation data\n",
      "#PARAMS = 1989126\n",
      "chi\n",
      "DimeNet++\n",
      "--done---\n",
      "378958 training data, 107856 test data and 54549 validation data\n",
      "#PARAMS = 1989126\n",
      "Kraken\n",
      "sterimol_B5\n",
      "ChytorchDiscrete\n",
      "--done---\n",
      "11221 training data, 3000 test data and 1528 validation data\n",
      "#PARAMS = 1694560\n",
      "sterimol_L\n",
      "ChytorchDiscrete\n",
      "--done---\n",
      "11112 training data, 2981 test data and 1656 validation data\n",
      "#PARAMS = 1694560\n",
      "sterimol_burB5\n",
      "ChytorchDiscrete\n",
      "--done---\n",
      "11173 training data, 3042 test data and 1534 validation data\n",
      "#PARAMS = 1694560\n",
      "sterimol_burL\n",
      "ChytorchDiscrete\n",
      "--done---\n",
      "10963 training data, 3228 test data and 1558 validation data\n",
      "#PARAMS = 1694560\n",
      "Kraken\n",
      "sterimol_B5\n",
      "ChIRo\n",
      "--done---\n",
      "10889 training data, 3158 test data and 1702 validation data\n",
      "#PARAMS = 917123\n",
      "sterimol_L\n",
      "ChIRo\n",
      "--done---\n",
      "10839 training data, 3289 test data and 1621 validation data\n",
      "#PARAMS = 917123\n",
      "sterimol_burB5\n",
      "ChIRo\n",
      "--done---\n",
      "11161 training data, 3037 test data and 1551 validation data\n",
      "#PARAMS = 917123\n",
      "sterimol_burL\n",
      "ChIRo\n",
      "--done---\n",
      "11029 training data, 3166 test data and 1554 validation data\n",
      "#PARAMS = 917123\n",
      "Kraken\n",
      "sterimol_B5\n",
      "SchNet\n",
      "--done---\n",
      "10783 training data, 3243 test data and 1723 validation data\n",
      "#PARAMS = 231854\n",
      "sterimol_L\n",
      "SchNet\n",
      "--done---\n",
      "10949 training data, 3257 test data and 1543 validation data\n",
      "#PARAMS = 231854\n",
      "sterimol_burB5\n",
      "SchNet\n",
      "--done---\n",
      "10861 training data, 3259 test data and 1629 validation data\n",
      "#PARAMS = 231854\n",
      "sterimol_burL\n",
      "SchNet\n",
      "--done---\n",
      "11483 training data, 2852 test data and 1414 validation data\n",
      "#PARAMS = 231854\n",
      "Kraken\n",
      "sterimol_B5\n",
      "GemNet\n",
      "--done---\n",
      "11086 training data, 3131 test data and 1532 validation data\n",
      "#PARAMS = 1969344\n",
      "sterimol_L\n",
      "GemNet\n",
      "--done---\n",
      "11201 training data, 3063 test data and 1485 validation data\n",
      "#PARAMS = 1969344\n",
      "sterimol_burB5\n",
      "GemNet\n",
      "--done---\n",
      "11042 training data, 3217 test data and 1490 validation data\n",
      "#PARAMS = 1969344\n",
      "sterimol_burL\n",
      "GemNet\n",
      "--done---\n",
      "10936 training data, 3344 test data and 1469 validation data\n",
      "#PARAMS = 1969344\n",
      "Kraken\n",
      "sterimol_B5\n",
      "PaiNN\n",
      "--done---\n",
      "10856 training data, 3196 test data and 1697 validation data\n",
      "#PARAMS = 1326848\n",
      "sterimol_L\n",
      "PaiNN\n",
      "--done---\n",
      "10977 training data, 3143 test data and 1629 validation data\n",
      "#PARAMS = 1326848\n",
      "sterimol_burB5\n",
      "PaiNN\n",
      "--done---\n",
      "10976 training data, 3052 test data and 1721 validation data\n",
      "#PARAMS = 1326848\n",
      "sterimol_burL\n",
      "PaiNN\n",
      "--done---\n",
      "10956 training data, 3111 test data and 1682 validation data\n",
      "#PARAMS = 1326848\n",
      "Kraken\n",
      "sterimol_B5\n",
      "ClofNet\n",
      "--done---\n",
      "11098 training data, 3129 test data and 1522 validation data\n",
      "#PARAMS = 621761\n",
      "sterimol_L\n",
      "ClofNet\n",
      "--done---\n",
      "11026 training data, 3164 test data and 1559 validation data\n",
      "#PARAMS = 621761\n",
      "sterimol_burB5\n",
      "ClofNet\n",
      "--done---\n",
      "11111 training data, 2995 test data and 1643 validation data\n",
      "#PARAMS = 621761\n",
      "sterimol_burL\n",
      "ClofNet\n",
      "--done---\n",
      "10939 training data, 3338 test data and 1472 validation data\n",
      "#PARAMS = 621761\n",
      "Kraken\n",
      "sterimol_B5\n",
      "DimeNet++\n",
      "--done---\n",
      "11056 training data, 3114 test data and 1579 validation data\n",
      "#PARAMS = 1993734\n",
      "sterimol_L\n",
      "DimeNet++\n",
      "--done---\n",
      "11134 training data, 3058 test data and 1557 validation data\n",
      "#PARAMS = 1993734\n",
      "sterimol_burB5\n",
      "DimeNet++\n",
      "--done---\n",
      "11092 training data, 3075 test data and 1582 validation data\n",
      "#PARAMS = 1993734\n",
      "sterimol_burL\n",
      "DimeNet++\n",
      "--done---\n",
      "10980 training data, 3189 test data and 1580 validation data\n",
      "#PARAMS = 1993734\n",
      "tmQMg\n",
      "tzvp_dipole_moment\n",
      "ChytorchDiscrete\n",
      "--done---\n",
      "39593 training data, 11313 test data and 5656 validation data\n",
      "#PARAMS = 1698016\n",
      "tmQMg\n",
      "tzvp_dipole_moment\n",
      "ChIRo\n",
      "--done---\n",
      "39593 training data, 11313 test data and 5656 validation data\n",
      "#PARAMS = 917123\n",
      "tmQMg\n",
      "tzvp_dipole_moment\n",
      "SchNet\n",
      "--done---\n",
      "39593 training data, 11313 test data and 5656 validation data\n",
      "#PARAMS = 235310\n",
      "tmQMg\n",
      "tzvp_dipole_moment\n",
      "GemNet\n",
      "--done---\n",
      "39593 training data, 11313 test data and 5656 validation data\n",
      "#PARAMS = 1972800\n",
      "tmQMg\n",
      "tzvp_dipole_moment\n",
      "PaiNN\n",
      "--done---\n",
      "39593 training data, 11313 test data and 5656 validation data\n",
      "#PARAMS = 1330304\n",
      "tmQMg\n",
      "tzvp_dipole_moment\n",
      "ClofNet\n",
      "--done---\n",
      "39593 training data, 11313 test data and 5656 validation data\n",
      "#PARAMS = 625217\n",
      "tmQMg\n",
      "tzvp_dipole_moment\n",
      "DimeNet++\n",
      "--done---\n",
      "39593 training data, 11313 test data and 5656 validation data\n",
      "#PARAMS = 1997190\n"
     ]
    }
   ],
   "source": [
    "for dataname,targets in zip(['BDE','Drugs','Kraken','tmQMg'],\n",
    "                           [['BindingEnergy'],['ip', 'ea', 'chi'],['sterimol_B5', 'sterimol_L', 'sterimol_burB5', 'sterimol_burL'],['tzvp_dipole_moment']]):\n",
    "    for modeltype in ['ChytorchDiscrete', 'ChIRo','SchNet','GemNet','PaiNN','ClofNet','DimeNet++']:#,'GIN','GIN-VN','GPS','ChemProp']: #,'Chytorch'\n",
    "        print(dataname)\n",
    "        for target in targets:\n",
    "            print(target)\n",
    "            print(modeltype)\n",
    "            config = Config\n",
    "            config.dataset = dataname\n",
    "            config.target = target\n",
    "            config.device = 'cuda:0'\n",
    "            \n",
    "            \n",
    "            config.model4d.model=modeltype\n",
    "            config.model4d.augmentation = True\n",
    "            \n",
    "            config_dict = config_to_dict(config)\n",
    "    \n",
    "            subkeys = [\"dataset\",\n",
    "                       \"max_num_conformers\",\n",
    "                       \"target\",\n",
    "                       \"train_ratio\",\n",
    "                       \"valid_ratio\",\n",
    "                       \"seed\",\n",
    "                       \"model3d\", #.augmentation\"\n",
    "                       \"batch_size\"]\n",
    "            config_dict_datamodule = {}\n",
    "            for k,v in config_dict.items():\n",
    "                if k in subkeys:\n",
    "                    if k==\"model4d\":\n",
    "                        config_dict_datamodule[f'{k}_augmentation']=config_dict[k].augmentation\n",
    "                    else:\n",
    "                        config_dict_datamodule[k]=v\n",
    "            data = DataModule(config_dict_datamodule, multitask = False)\n",
    "            data.prepare_data()\n",
    "            data.split_compute()\n",
    "            model = ModelLM(max_atomic_num=data.max_atomic_num, \n",
    "                            whole_dataset = data.dataset, \n",
    "                            unique_variables=data.unique_variables, \n",
    "                            multitask = False, **config_dict)\n",
    "            print(f'#PARAMS = {sum(p.numel() for p in model.parameters() if p.requires_grad)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0c7d3e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataname = 'Drugs'  #['BDE','Drugs','Kraken','tmQMg],\n",
    "target = 'ea' #[['BindingEnergy'],['ip', 'ea', 'chi'],['sterimol_B5', 'sterimol_L', 'sterimol_burB5', 'sterimol_burL']]'tzvp_dipole_moment'\n",
    "modeltype = 'SchNet'  #['SchNet','GemNet','PaiNN','ClofNet','LEFTNet','DimeNet++','ChytorchDiscrete', 'ChIRo']  #oom: 'GemNet'\n",
    "\n",
    "writer = SummaryWriter(log_dir='/mnt/code/logs/')\n",
    "#loader = ConfigLoader(model=Config, config='params/params_1d.json')\n",
    "#config = loader()\n",
    "\n",
    "config = Config\n",
    "config.dataset = dataname\n",
    "config.target = target\n",
    "config.device = 'cuda:0'\n",
    "\n",
    "\n",
    "#config.hidden_dim =128\n",
    "#config.dropout = 0.5\n",
    "\n",
    "#config.train_ratio = 0.7\n",
    "#config.valid_ratio = 0.1\n",
    "config.batch_size = 64 #256\n",
    "#config.patience = 200\n",
    "\n",
    "#config.learning_rate = 0.00001\n",
    "#config.weight_decay = 1e-4\n",
    "#config.scheduler = None\n",
    "#config.reduce_lr_on_plateau = ReduceLROnPlateau()\n",
    "\n",
    "#config.num_epochs = 2000\n",
    "#config.cosine_annealing_lr = CosineAnnealingLR()\n",
    "#config.linear_warmup_cosine_annealing_lr = LinearWarmupCosineAnnealingLR()\n",
    "\n",
    "\n",
    "#not used in 1d\n",
    "\n",
    "#config.one_cycle_lr = OneCycleLR()\n",
    "#config.seed = 123\n",
    "#activation = 'relu'\n",
    "\n",
    "######3DMODEL\n",
    "config.model3d.model=modeltype\n",
    "config.model3d.augmentation = True\n",
    "\n",
    "#config.model3d.schnet = SchNet()\n",
    "#config.model3d.dimenet = DimeNet()\n",
    "#config.model3d.dimenetplusplus = DimeNetPlusPlus()\n",
    "#config.model3d.gemnet = GemNet()\n",
    "#config.model3d.painn = PaiNN()\n",
    "#config.model3d.clofnet = ClofNet()\n",
    "#config.model3d.leftnet= LEFTNet()\n",
    "#config.model3d.chytorch_discrete = ChytorchDiscrete()\n",
    "#config.model3d.chytorch_conformer = ChytorchConformer()\n",
    "#config.model3d.chytorch_rotary = ChytorchRotary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "048b769b-7030-444e-b6e0-edc2ec536b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_dict = config_to_dict(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d61b8117-6368-49f0-bab2-1b12bea10ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "subkeys = [\"dataset\",\n",
    "           \"max_num_conformers\",\n",
    "           \"target\",\n",
    "           \"train_ratio\",\n",
    "           \"valid_ratio\",\n",
    "           \"seed\",\n",
    "           \"model4d\", #.augmentation\"\n",
    "           \"batch_size\"]\n",
    "config_dict_datamodule = {}\n",
    "for k,v in config_dict.items():\n",
    "    if k in subkeys:\n",
    "        if k==\"model4d\":\n",
    "            config_dict_datamodule[f'{k}_augmentation']=config_dict[k].augmentation\n",
    "        else:\n",
    "            config_dict_datamodule[k]=v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d24cfce3-1969-43a4-809d-36d81a4c8508",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 64,\n",
       " 'dataset': 'Drugs',\n",
       " 'max_num_conformers': 20,\n",
       " 'model4d_augmentation': True,\n",
       " 'seed': 123,\n",
       " 'target': 'ea',\n",
       " 'train_ratio': 0.7,\n",
       " 'valid_ratio': 0.1}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_dict_datamodule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4cb75383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--done---\n",
      "378766 training data, 108580 test data and 54017 validation data\n"
     ]
    }
   ],
   "source": [
    "data = DataModule(config_dict_datamodule, multitask = False)\n",
    "data.prepare_data()\n",
    "data.split_compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0d562cc6-949d-4df2-9785-efb3c86329a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[16545178, 9], edge_index=[2, 35655306], edge_attr=[35655306, 3], pos=[16545178, 3], name=[541363], id=[541363], smiles=[541363], y=[541363, 4], molecule_idx=[541363])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dataset.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5435ff99-f4f4-482b-a442-cfbb608bf555",
   "metadata": {},
   "outputs": [],
   "source": [
    "#z.shape, hgs.shape, pos.shape, bat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3edb7560",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModelLM(max_atomic_num=data.max_atomic_num, \n",
    "                whole_dataset = data.dataset, \n",
    "                unique_variables=data.unique_variables, \n",
    "                multitask = False, **config_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9e82ee2b-5cfe-4480-95d4-d6e3a5cd3f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model._log_hyperparams = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "69530b9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#PARAMS = 1989126\n"
     ]
    }
   ],
   "source": [
    "print(f'#PARAMS = {sum(p.numel() for p in model.parameters() if p.requires_grad)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ffad2ebc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelLM(\n",
       "  (net): Model4D(\n",
       "    (graph_encoders): ModuleList(\n",
       "      (0): DimeNetPlusPlus(\n",
       "        (rbf): BesselBasisLayer(\n",
       "          (envelope): Envelope()\n",
       "        )\n",
       "        (sbf): SphericalBasisLayer(\n",
       "          (envelope): Envelope()\n",
       "        )\n",
       "        (emb): EmbeddingBlock(\n",
       "          (emb): Embedding(17, 128)\n",
       "          (lin_rbf): Linear(in_features=6, out_features=128, bias=True)\n",
       "          (lin): Linear(in_features=384, out_features=128, bias=True)\n",
       "        )\n",
       "        (output_blocks): ModuleList(\n",
       "          (0-4): 5 x OutputPPBlock(\n",
       "            (lin_rbf): Linear(in_features=6, out_features=128, bias=False)\n",
       "            (lin_up): Linear(in_features=128, out_features=256, bias=False)\n",
       "            (lins): ModuleList(\n",
       "              (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (lin): Linear(in_features=256, out_features=128, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (interaction_blocks): ModuleList(\n",
       "          (0-3): 4 x InteractionPPBlock(\n",
       "            (lin_rbf1): Linear(in_features=6, out_features=8, bias=False)\n",
       "            (lin_rbf2): Linear(in_features=8, out_features=128, bias=False)\n",
       "            (lin_sbf1): Linear(in_features=42, out_features=8, bias=False)\n",
       "            (lin_sbf2): Linear(in_features=8, out_features=64, bias=False)\n",
       "            (lin_kj): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (lin_ji): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (lin_down): Linear(in_features=128, out_features=64, bias=False)\n",
       "            (lin_up): Linear(in_features=64, out_features=128, bias=False)\n",
       "            (layers_before_skip): ModuleList(\n",
       "              (0): ResidualLayer(\n",
       "                (lin1): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (lin2): Linear(in_features=128, out_features=128, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (lin): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layers_after_skip): ModuleList(\n",
       "              (0): ResidualLayer(\n",
       "                (lin1): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (lin2): Linear(in_features=128, out_features=128, bias=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (set_encoders): ModuleList(\n",
       "      (0): SelfAttentionPooling(\n",
       "        (attention): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (phi): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (3): ReLU()\n",
       "        )\n",
       "        (rho): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (3): ReLU()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (linear): Symbolic(\n",
       "      (head): Linear(in_features=128, out_features=1, bias=False)\n",
       "      (scaler): Linear(in_features=128, out_features=1, bias=False)\n",
       "    )\n",
       "  )\n",
       "  (loss_fn): MSELoss()\n",
       ")"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1a20c54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_name = f\"tmp_4D_singletask_{dataname}_{target}_{modeltype}_multitask_v0\"\n",
    "\n",
    "dir_load_model = None\n",
    "log_dir_folder = '/mnt/code/logs/'\n",
    "log_dir_folder = os.path.join(log_dir_folder, dir_name)\n",
    "if os.path.exists(log_dir_folder):\n",
    "    if os.path.exists(os.path.join(log_dir_folder, \"last.ckpt\")):\n",
    "        dir_load_model = os.path.join(log_dir_folder, \"last.ckpt\")\n",
    "    csv_path = os.path.join(log_dir_folder, \"metrics.csv\")\n",
    "    while os.path.exists(csv_path):\n",
    "        csv_path = csv_path + '.bak'\n",
    "    if os.path.exists(os.path.join(log_dir_folder, \"metrics.csv\")):\n",
    "        os.rename(os.path.join(log_dir_folder, \"metrics.csv\"), csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "50c750fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_to_monitor = \"val_epoch_loss\"\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=log_dir_folder,\n",
    "    monitor=metric_to_monitor,\n",
    "    mode = 'min',\n",
    "    save_top_k=1,\n",
    "    save_last=True,\n",
    "    every_n_epochs=5,\n",
    "    save_weights_only=True,\n",
    "    verbose=True,\n",
    "    filename=\"best-model-{epoch}-{val_epoch_loss:.4f}\",\n",
    ")\n",
    "\n",
    "\n",
    "early_stopping = early_stop_callback = EarlyStopping(\n",
    "        monitor=metric_to_monitor,  # The metric you want to monitor\n",
    "        patience=config.patience,  # Number of epochs with no improvement after which training will be stopped\n",
    "        verbose=True,\n",
    "        mode='min'  # Minimizing the validation loss\n",
    "    )\n",
    "\n",
    "tb_logger = TensorBoardLogger(log_dir_folder, name=\"tensorbord\")#, version=\"\", default_hp_metric=False)\n",
    "csv_logger = CSVLogger(log_dir_folder, name=\"\", version=\"\")\n",
    "\n",
    "model_params = dict(\n",
    "    devices=1, #args['ngpus'],\n",
    "    accelerator='gpu', #args['accelerator'],\n",
    "    default_root_dir=log_dir_folder, #args['log_dir'],\n",
    "    logger=[tb_logger, csv_logger],\n",
    "    enable_progress_bar=True)\n",
    "\n",
    "\n",
    "model_params.update(dict(\n",
    "    max_epochs=config.num_epochs,#1000,\n",
    "    callbacks=[checkpoint_callback, early_stopping],\n",
    "    #enable_checkpointing=False,\n",
    "    gradient_clip_val=10,#args['clip_norm'],\n",
    "    #precision=\"16-mixed\",\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "843bb182",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision('medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "787f4d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Missing logger folder: /mnt/code/logs/tmp_4D_singletask_Drugs_ea_SchNet_multitask_v0/tensorbord\n",
      "/usr/local/lib/python3.10/dist-packages/lightning_fabric/loggers/csv_logs.py:268: Experiment logs directory /mnt/code/logs/tmp_4D_singletask_Drugs_ea_SchNet_multitask_v0/ exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!\n",
      "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py:653: Checkpoint directory /mnt/code/logs/tmp_4D_singletask_Drugs_ea_SchNet_multitask_v0 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type    | Params\n",
      "------------------------------------\n",
      "0 | net     | Model4D | 2.0 M \n",
      "1 | loss_fn | MSELoss | 0     \n",
      "------------------------------------\n",
      "2.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.0 M     Total params\n",
      "7.957     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5be8b19deeaa45e08e83ac8c631a7fb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(**model_params)\n",
    "trainer.fit(model, datamodule=data, ckpt_path=dir_load_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "531274fc-c611-4e67-afb5-7ac09c7aec6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['energy', 'ip', 'ea', 'chi']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dataset.descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e66010-c81f-4506-9371-18e972ab3956",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcca3d98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
