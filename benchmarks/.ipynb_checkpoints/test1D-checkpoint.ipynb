{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a92808ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue May 14 08:58:29 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A10G                    On  | 00000000:00:1D.0 Off |                    0 |\n",
      "|  0%   28C    P8              30W / 300W |      0MiB / 23028MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee14c6fb-4b15-441a-a679-aa212f5eec51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in links: https://data.pyg.org/whl/torch-2.3.0+cu121.html\n",
      "Requirement already satisfied: torch-geometric in /home/ubuntu/.local/lib/python3.10/site-packages (2.5.3)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.4.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.26.4)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.13.0)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2024.3.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.31.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (4.66.4)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.8)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.9.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.4)\n",
      "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.9.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.4.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (4.0.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (23.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (6.0.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2.2.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2024.2.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (3.5.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (1.4.2)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in links: https://data.pyg.org/whl/torch-2.3.0+cu121.html\n",
      "Requirement already satisfied: torch-scatter in /home/ubuntu/.local/lib/python3.10/site-packages (2.1.2+pt23cu121)\n",
      "Requirement already satisfied: torch-sparse in /home/ubuntu/.local/lib/python3.10/site-packages (0.6.18+pt23cu121)\n",
      "Requirement already satisfied: torch-cluster in /home/ubuntu/.local/lib/python3.10/site-packages (1.6.3+pt23cu121)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-sparse) (1.13.0)\n",
      "Requirement already satisfied: numpy<2.3,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from scipy->torch-sparse) (1.26.4)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /home/ubuntu/.local/lib/python3.10/site-packages (4.40.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.14.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers) (0.23.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers) (2024.5.10)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch-geometric -f https://data.pyg.org/whl/torch-2.3.0+cu121.html\n",
    "!pip install torch-scatter torch-sparse torch-cluster -f https://data.pyg.org/whl/torch-2.3.0+cu121.html\n",
    "!pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45edc818",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from dataclasses import asdict\n",
    "from transformers import RobertaTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "#from pl_bolts.optimizers import LinearWarmupCosineAnnealingLR\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingLR\n",
    "\n",
    "from config import Config\n",
    "from data.ee import EE\n",
    "from data.bde import BDE\n",
    "from data.drugs import Drugs\n",
    "from data.kraken import Kraken\n",
    "from happy_config import ConfigLoader\n",
    "from utils.early_stopping import EarlyStopping, generate_checkpoint_filename\n",
    "\n",
    "from models.model_1d import LSTM, Transformer\n",
    "from models.models_1d.utils import construct_fingerprint, construct_smiles, concatenate_smiles\n",
    "#from train_1d import *\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9b09137",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from pytorch_lightning.loggers import CSVLogger, TensorBoardLogger\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import LightningModule, Trainer, LightningDataModule\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CyclicLR, CosineAnnealingLR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e757818c",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################\n",
    "import os\n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = gpus\n",
    "torch.cuda.empty_cache()\n",
    "#print(os.environ['CUDA_VISIBLE_DEVICES'])\n",
    "\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# If you are using a GPU, you should also set the seed for CUDA operations\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "#np.random.seed(seed)\n",
    "################################################## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee749ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Molecules(Dataset):\n",
    "    def __init__(self, smiles_ids, attention_masks, labels, fingerprint=None, input_type='smiles'):\n",
    "        self.smiles_ids = smiles_ids\n",
    "        self.attention_masks = attention_masks\n",
    "        self.labels = labels\n",
    "        self.fingerprint = fingerprint\n",
    "        self.input_type = input_type\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.labels.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.input_type == 'SMILES':\n",
    "            smiles = self.smiles_ids[index]\n",
    "            attention_mask = self.attention_masks[index]\n",
    "            y = self.labels[index]\n",
    "            return smiles, attention_mask, y.clone()\n",
    "        else:\n",
    "            fingerprint = self.fingerprint[index]\n",
    "            y = self.labels[index]\n",
    "            return torch.tensor(fingerprint, dtype=torch.long), y.clone()\n",
    "\n",
    "def r2_score_torch(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute the R-squared score.\n",
    "\n",
    "    Parameters:\n",
    "        y_true (torch.Tensor): The true target values.\n",
    "        y_pred (torch.Tensor): The predicted target values.\n",
    "\n",
    "    Returns:\n",
    "        float: The R-squared score.\n",
    "    \"\"\"\n",
    "    # Calculate the mean of the true target values\n",
    "    y_mean = torch.mean(y_true)\n",
    "    # Calculate the total sum of squares (TSS)\n",
    "    tss = torch.sum((y_true - y_mean) ** 2)\n",
    "    # Calculate the residual sum of squares (RSS)\n",
    "    rss = torch.sum((y_true - y_pred) ** 2)\n",
    "    # Calculate R-squared score\n",
    "    r2 = 1 - rss / tss\n",
    "\n",
    "    return r2.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08ff1411",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataModule(LightningDataModule):\n",
    "    def __init__(self, hparams, dataset=None):\n",
    "        super().__init__()\n",
    "        self.hparams.update(hparams.__dict__) if hasattr(hparams, \"__dict__\") else self.hparams.update(hparams)\n",
    "        self._saved_dataloaders = dict()\n",
    "        self.dataset = dataset\n",
    "        \n",
    "        if self.dataset is None:\n",
    "            variable_name = None\n",
    "            unique_variables = 1\n",
    "            \n",
    "            if self.hparams.dataset == 'Drugs':\n",
    "                dataset = Drugs('/mnt/data/MARCEL/datasets/Drugs', max_num_conformers=self.hparams.max_num_conformers).shuffle()\n",
    "            elif self.hparams.dataset == 'Kraken':\n",
    "                dataset = Kraken('/mnt/data/MARCEL/datasets/Kraken', max_num_conformers=self.hparams.max_num_conformers).shuffle()\n",
    "            elif self.hparams.dataset == 'BDE':\n",
    "                dataset = BDE('/mnt/data/MARCEL/datasets/BDE').shuffle()\n",
    "                variable_name = 'is_ligand'\n",
    "                unique_variables = 2\n",
    "            elif self.hparams.dataset == 'EE':\n",
    "                dataset = EE('/mnt/data/MARCEL/datasets/EE', max_num_conformers=self.hparams.max_num_conformers).shuffle()\n",
    "                variable_name = 'config_id'\n",
    "                unique_variables = 2\n",
    "            \n",
    "            #autoscaling\n",
    "            target_id = dataset.descriptors.index(self.hparams.target)\n",
    "            labels = dataset.y[:, target_id]\n",
    "            mean = labels.mean(dim=0).item()\n",
    "            std = labels.std(dim=0).item()\n",
    "            labels = (labels - mean) / std\n",
    "            \n",
    "            if variable_name is not None:\n",
    "                smiles = concatenate_smiles(dataset, variable_name)\n",
    "            else:\n",
    "                smiles = construct_smiles(dataset)\n",
    "            fingerprint = construct_fingerprint(smiles) if self.hparams.model1d_input_type == 'Fingerprint' else None\n",
    "\n",
    "            tokenizer = RobertaTokenizer.from_pretrained('seyonec/PubChem10M_SMILES_BPE_450k')\n",
    "            dicts = tokenizer(smiles, return_tensors='pt', padding='longest')\n",
    "            smiles_ids, attention_masks = dicts['input_ids'], dicts['attention_mask']\n",
    "            vocab_size = tokenizer.vocab_size if self.hparams.model1d_input_type == 'SMILES' else fingerprint.shape[1]\n",
    "\n",
    "            dataset = Molecules(smiles_ids, attention_masks, labels, fingerprint, input_type=self.hparams.model1d_input_type)\n",
    "\n",
    "            self.dataset = dataset\n",
    "            self.vocab_size=vocab_size\n",
    "            self.tokenizer=tokenizer\n",
    "            self.smiles_ids=smiles_ids\n",
    "            #modelnet = model.to(device)\n",
    "\n",
    "            print('--done---')\n",
    "\n",
    "    def split_compute(self):\n",
    "        train_ratio = self.hparams.train_ratio\n",
    "        valid_ratio = self.hparams.valid_ratio\n",
    "        test_ratio = 1 - train_ratio - valid_ratio\n",
    "\n",
    "        train_len = int(train_ratio * len(self.dataset))\n",
    "        valid_len = int(valid_ratio * len(self.dataset))\n",
    "        test_len = len(self.dataset) - train_len - valid_len\n",
    "\n",
    "        self.train_dataset, self.valid_dataset, self.test_dataset = random_split(self.dataset, lengths=[train_len, valid_len, test_len])\n",
    "        print(f'{len(self.train_dataset)} training data, {len(self.test_dataset)} test data and {len(self.valid_dataset)} validation data')\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return self._get_dataloader(self.train_dataset, \"train\")\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self._get_dataloader(self.valid_dataset, \"val\")\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return self._get_dataloader(self.test_dataset, \"test\")\n",
    "    \n",
    "    def _get_dataloader(self, dataset, stage, store_dataloader=True):\n",
    "        store_dataloader = store_dataloader\n",
    "        \n",
    "        if stage in self._saved_dataloaders and store_dataloader:\n",
    "            return self._saved_dataloaders[stage]\n",
    "\n",
    "        if stage == \"train\":\n",
    "            dl = DataLoader(dataset=dataset, batch_size=self.hparams.batch_size, shuffle=True, num_workers = 20)                                  \n",
    "        else:\n",
    "            dl = DataLoader(dataset=dataset, batch_size=self.hparams.batch_size, shuffle=False, num_workers = 20) \n",
    "\n",
    "        if store_dataloader:\n",
    "            self._saved_dataloaders[stage] = dl\n",
    "        return dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5796c8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def compute_pnorm(model: nn.Module) -> float:\n",
    "    \"\"\"\n",
    "    Computes the norm of the parameters of a model.\n",
    "    :param model: A PyTorch model.\n",
    "    :return: The norm of the parameters of the model.\n",
    "    \"\"\"\n",
    "    return math.sqrt(sum([p.norm().item() ** 2 for p in model.parameters() if p.requires_grad]))\n",
    "\n",
    "\n",
    "def compute_gnorm(model: nn.Module) -> float:\n",
    "    \"\"\"\n",
    "    Computes the norm of the gradients of a model.\n",
    "    :param model: A PyTorch model.\n",
    "    :return: The norm of the gradients of the model.\n",
    "    \"\"\"\n",
    "    return math.sqrt(sum([p.grad.norm().item() ** 2 for p in model.parameters() if p.grad is not None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc1085bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model1D(LightningModule):\n",
    "    def __init__(self, vocab_size=None, tokenizer=None, smiles_ids=None, **hparams):\n",
    "        super().__init__()\n",
    "        #self.hparams.update(hparams.__dict__) if hasattr(hparams, \"__dict__\") else self.hparams.update(hparams)\n",
    "        \n",
    "        if hparams.get('model1d').model == 'LSTM':\n",
    "            self.net = LSTM(\n",
    "                vocab_size, hparams.get('hidden_dim'), hparams.get('hidden_dim'), 1,\n",
    "                hparams.get('model1d').num_layers, hparams.get('dropout'), padding_idx=tokenizer.pad_token_id)\n",
    "        elif hparams.get('model1d').model == 'Transformer':\n",
    "            self.net = Transformer(\n",
    "                vocab_size, hparams.get('model1d').embedding_dim, smiles_ids.shape[1],\n",
    "                hparams.get('model1d').num_heads, hparams.get('hidden_dim'), 1,\n",
    "                hparams.get('model1d').num_layers, hparams.get('dropout'), padding_idx=tokenizer.pad_token_id)\n",
    "                \n",
    "        self.loss_fn = nn.MSELoss() #LOGITS #GroupedScaledMAELoss(torch.ones(4, dtype=torch.long))\n",
    "        self.device_= torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.input_type = hparams.get('model1d').input_type\n",
    "        self.lr = hparams.get('learning_rate')\n",
    "        self.wd = hparams.get('weight_decay')\n",
    "\n",
    "        self._reset_losses_dict()\n",
    "        self._reset_inference_results()\n",
    "        self.save_hyperparameters(ignore=[\"cosine_annealing_lr\",\"linear_warmup_cosine_annealing_lr\",\n",
    "                                          \"model1d\",\"model2d\",\"model3d\",\"model4d\",\"modelfprf\",\"one_cycle_lr\",\n",
    "                                          \"reduce_lr_on_plateau\",\"whole_dataset\",\"device\",\"scheduler\"])\n",
    "\n",
    "    def forward(self, batch):\n",
    "        if self.input_type == 'SMILES':\n",
    "            input_ids, attention_mask, y = batch\n",
    "            if isinstance(self.net, Transformer):\n",
    "                out = self.net(input_ids, attention_mask)\n",
    "            else:\n",
    "                out = self.net(input_ids)\n",
    "        else:\n",
    "            fingerprints, y = batch\n",
    "            out = self.net(fingerprints)\n",
    "        return out, y\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        o = AdamW(self.parameters(), lr=self.lr, weight_decay=self.wd)\n",
    "        s = CyclicLR(o, self.lr, 2e-4, 1000, mode='triangular', cycle_momentum=False)\n",
    "        # instantiate the WeakMethod in the lr scheduler object into the custom scale function attribute\n",
    "        #s._scale_fn_custom = s._scale_fn_ref()\n",
    "        # remove the reference so there are no more WeakMethod references in the object\n",
    "        #s._scale_fn_ref = None\n",
    "        return [o], [{'scheduler': s, 'interval': 'step', 'name': 'lr_scheduler'}]\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        pnorm = compute_pnorm(self.net)\n",
    "        gnorm = compute_gnorm(self.net)\n",
    "        self.log(f'(training) pnorm', pnorm)\n",
    "        self.log(f'(training) gnorm', gnorm)\n",
    "        return self.step(batch, \"train\")\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self.step(batch, \"val\")\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        return self.step(batch, \"test\")\n",
    "        \n",
    "    def step(self, batch, stage):\n",
    "        start = time()\n",
    "\n",
    "        with torch.set_grad_enabled(stage == \"train\"):\n",
    "            pred, targets = self(batch)\n",
    "            loss = self.loss_fn(pred.squeeze(), targets)\n",
    "            \n",
    "            if stage == \"test\":\n",
    "                self.inference_results['y_pred'].append(pred.squeeze())\n",
    "                self.inference_results['y_true'].append(targets.squeeze())\n",
    "                return None\n",
    "\n",
    "            r2=r2_score_torch(targets.cpu(),pred.squeeze().cpu().detach())\n",
    "\n",
    "            self.logging_info[f'{stage}_loss'].append(loss.item())\n",
    "            self.logging_info[f'{stage}_r2'].append(r2)\n",
    "            self.logging_info[f'{stage}_time'].append(time()-start)\n",
    "            \n",
    "            if stage == 'train':\n",
    "                self.log(\"lr\", self.trainer.optimizers[0].param_groups[0][\"lr\"], on_step=True, on_epoch=False, prog_bar=True, logger=True, sync_dist=True)\n",
    "                self.log(f'{stage}_step_loss', loss.item(), on_step=True, on_epoch=False, prog_bar=True, logger=True, sync_dist=True)\n",
    "\n",
    "            return loss\n",
    "        \n",
    "    def on_validation_epoch_end(self):\n",
    "        if not self.trainer.sanity_checking:                    \n",
    "            result_dict = {\n",
    "                \"epoch\": float(self.current_epoch),\n",
    "                \"train_epoch_loss\": torch.tensor(self.logging_info[\"train_loss\"]).mean().item(),\n",
    "                \"train_epoch_r2\": torch.tensor(self.logging_info[\"train_r2\"]).mean().item(),\n",
    "                \"val_epoch_loss\": torch.tensor(self.logging_info[\"val_loss\"]).mean().item(),\n",
    "                \"val_epoch_r2\": torch.tensor(self.logging_info[\"val_r2\"]).mean().item(),\n",
    "                \"train_epoch_time\": sum(self.logging_info[\"train_time\"]),\n",
    "                \"val_epoch_time\": sum(self.logging_info[\"val_time\"]),\n",
    "                }\n",
    "            self.log_dict(result_dict, logger=True, sync_dist=True)\n",
    "            \n",
    "        self._reset_losses_dict()\n",
    "    \n",
    "    def on_test_epoch_end(self) -> None:\n",
    "        for key in self.inference_results.keys():\n",
    "            self.inference_results[key] = torch.cat(self.inference_results[key], dim=0)\n",
    "    \n",
    "    def _reset_losses_dict(self):\n",
    "        self.logging_info = {\n",
    "            \"train_loss\": [],\n",
    "            \"train_r2\": [], \n",
    "            \"train_mse\": [], \n",
    "            \"val_loss\": [],\n",
    "            \"val_r2\": [],\n",
    "            \"train_sample_size\": [], \n",
    "            \"val_sample_size\": [],\n",
    "            \"train_time\": [],\n",
    "            \"val_time\": [],\n",
    "        }\n",
    "        \n",
    "    def _reset_inference_results(self):\n",
    "        self.inference_results = {'y_pred': [],\n",
    "                                  'y_true': []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c7d3e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataname = 'BDE'\n",
    "target = 'BindingEnergy'\n",
    "input_type = 'SMILES'\n",
    "modeltype = 'LSTM'\n",
    "\n",
    "writer = SummaryWriter(log_dir='/mnt/code/logs/')\n",
    "#loader = ConfigLoader(model=Config, config='params/params_1d.json')\n",
    "#config = loader()\n",
    "\n",
    "config = Config\n",
    "config.dataset = dataname\n",
    "config.target = target\n",
    "config.device = 'cuda:0'\n",
    "\n",
    "#config.max_num_conformers = 20\n",
    "config.model1d.input_type = input_type\n",
    "config.model1d.model = modeltype\n",
    "#config.model1d.num_layers = 4\n",
    "\n",
    "#Specific for Transformer\n",
    "#config.model1d.embedding_dim = 128\n",
    "#config.model1d.num_heads = 4\n",
    "\n",
    "#config.hidden_dim =128\n",
    "#config.dropout = 0.5\n",
    "\n",
    "#config.train_ratio = 0.7\n",
    "#config.valid_ratio = 0.1\n",
    "#config.batch_size = 256\n",
    "#config.patience = 200\n",
    "\n",
    "#config.learning_rate = 0.00001\n",
    "#config.weight_decay = 1e-4\n",
    "#config.scheduler = None\n",
    "#config.reduce_lr_on_plateau = ReduceLROnPlateau()\n",
    "\n",
    "#config.num_epochs = 2000\n",
    "#config.cosine_annealing_lr = CosineAnnealingLR()\n",
    "#config.linear_warmup_cosine_annealing_lr = LinearWarmupCosineAnnealingLR()\n",
    "\n",
    "\n",
    "#not used in 1d\n",
    "\n",
    "#config.one_cycle_lr = OneCycleLR()\n",
    "#config.seed = 123\n",
    "#activation = 'relu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b408513-9516-4a38-a222-59c16c941123",
   "metadata": {},
   "outputs": [],
   "source": [
    "def config_to_dict(config_class):\n",
    "    config_dict = {}\n",
    "    for attr_name in dir(config_class):\n",
    "        if not attr_name.startswith(\"__\") and not callable(getattr(config_class, attr_name)):\n",
    "            config_dict[attr_name] = getattr(config_class, attr_name)\n",
    "    return config_dict\n",
    "config_dict = config_to_dict(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61f36ba4-f7a9-4c63-a175-eaef194f2529",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activation': 'relu',\n",
       " 'batch_size': 256,\n",
       " 'cosine_annealing_lr': CosineAnnealingLR(eta_min=1e-06),\n",
       " 'dataset': 'BDE',\n",
       " 'device': 'cuda:0',\n",
       " 'dropout': 0.5,\n",
       " 'hidden_dim': 128,\n",
       " 'learning_rate': 0.001,\n",
       " 'linear_warmup_cosine_annealing_lr': LinearWarmupCosineAnnealingLR(warmup_steps=200, max_epochs=2000),\n",
       " 'max_num_conformers': 20,\n",
       " 'max_num_molecules': None,\n",
       " 'model1d': Model1D(model='LSTM', input_type='SMILES', embedding_dim=128, num_layers=4, num_heads=4),\n",
       " 'model2d': Model2D(model='GIN', gin=GIN(num_layers=6, virtual_node=False), gps=GPS(num_layers=6, walk_length=20, num_heads=4), chemprop=ChemProp(num_layers=6)),\n",
       " 'model3d': Model3D(model='PaiNN', augmentation=True, schnet=SchNet(hidden_dim=128, num_filters=5, num_interactions=6, num_gaussians=50, cutoff=10, readout='mean', dipole=False), dimenet=DimeNet(hidden_channels=128, out_channels=128, num_blocks=6, num_bilinear=8, num_spherical=7, num_radial=6), dimenetplusplus=DimeNetPlusPlus(hidden_channels=128, out_channels=128, num_blocks=4, int_emb_size=64, basis_emb_size=8, out_emb_channels=256, num_spherical=7, num_radial=6), gemnet=GemNet(num_spherical=7, num_radial=6, num_blocks=4, emb_size_atom=128, emb_size_edge=128, emb_size_trip=64, emb_size_rbf=16, emb_size_cbf=16, emb_size_bil_trip=64, num_before_skip=1, num_after_skip=1, num_concat=1, num_atoms=1, num_atom=2, bond_feat_dim=0), painn=PaiNN(hidden_dim=128, num_interactions=6, num_rbf=64, cutoff=12.0, readout='add', shared_interactions=False, shared_filters=False), clofnet=ClofNet(cutoff=6.5, num_layers=6, hidden_channels=128, num_radial=32), leftnet=LEFTNet(cutoff=6.5, num_layers=6, hidden_channels=128, num_radial=32), chiro=ChIRo(), chytorch_discrete=ChytorchDiscrete(max_distance=83, d_model=128, nhead=16, num_layers=8, dim_feedforward=512, shared_weights=False, dropout=0.1, norm_first=True, post_norm=True, zero_bias=True), chytorch_conformer=ChytorchConformer(nkernel=128, shared_layers=True, d_model=128, nhead=16, num_layers=8, dim_feedforward=512, dropout=0.1, norm_first=True, post_norm=True), chytorch_rotary=ChytorchRotary(d_model=128, nhead=16, num_layers=8, dim_feedforward=512, shared_weights=False, dropout=0.1, norm_first=False, post_norm=False)),\n",
       " 'model4d': Model4D(graph_encoder='SchNet', set_encoder='Attention', schnet=SchNet(hidden_dim=128, num_filters=5, num_interactions=6, num_gaussians=50, cutoff=10, readout='mean', dipole=False), dimenet=DimeNet(hidden_channels=128, out_channels=128, num_blocks=6, num_bilinear=8, num_spherical=7, num_radial=6), dimenetplusplus=DimeNetPlusPlus(hidden_channels=128, out_channels=128, num_blocks=4, int_emb_size=64, basis_emb_size=8, out_emb_channels=256, num_spherical=7, num_radial=6), gemnet=GemNet(num_spherical=7, num_radial=6, num_blocks=4, emb_size_atom=128, emb_size_edge=128, emb_size_trip=64, emb_size_rbf=16, emb_size_cbf=16, emb_size_bil_trip=64, num_before_skip=1, num_after_skip=1, num_concat=1, num_atoms=1, num_atom=2, bond_feat_dim=0), painn=PaiNN(hidden_dim=128, num_interactions=6, num_rbf=64, cutoff=12.0, readout='add', shared_interactions=False, shared_filters=False), clofnet=ClofNet(cutoff=6.5, num_layers=6, hidden_channels=128, num_radial=32), transformer=TransformerPooling(num_heads=8, num_layers=2)),\n",
       " 'modelfprf': ModelFPRF(n_jobs=8, n_estimators=500, min_samples_leaf=2, min_samples_split=10, min_impurity_decrease=0, warm_start=True),\n",
       " 'num_epochs': 2000,\n",
       " 'one_cycle_lr': OneCycleLR(max_lr=0.001, steps_per_epoch=100),\n",
       " 'patience': 200,\n",
       " 'reduce_lr_on_plateau': ReduceLROnPlateau(mode='min', factor=0.8, patience=20),\n",
       " 'scheduler': None,\n",
       " 'seed': 123,\n",
       " 'target': 'BindingEnergy',\n",
       " 'train_ratio': 0.7,\n",
       " 'valid_ratio': 0.1,\n",
       " 'weight_decay': 0.0001}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e37820aa-f5ac-4d0e-93be-00812d5dc478",
   "metadata": {},
   "outputs": [],
   "source": [
    "subkeys = [\"dataset\",\n",
    "           \"max_num_conformers\",\n",
    "           \"target\",\n",
    "           \"train_ratio\",\n",
    "           \"valid_ratio\",\n",
    "           \"seed\",\n",
    "           \"model1d\", #.augmentation\"\n",
    "           \"batch_size\"]\n",
    "config_dict_datamodule = {}\n",
    "for k,v in config_dict.items():\n",
    "    if k in subkeys:\n",
    "        if k==\"model1d\":\n",
    "            config_dict_datamodule[f'{k}_input_type']=config_dict[k].input_type\n",
    "        else:\n",
    "            config_dict_datamodule[k]=v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4cb75383",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The given 'InMemoryDataset' only references a subset of examples of the full dataset, but 'data' will contain information of the full dataset. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--done---\n",
      "4140 training data, 1184 test data and 591 validation data\n"
     ]
    }
   ],
   "source": [
    "data = DataModule(config_dict_datamodule)\n",
    "data.prepare_data()\n",
    "data.split_compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b7969c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def config_to_dict(config_class):\n",
    "    config_dict = {}\n",
    "    for attr_name in dir(config_class):\n",
    "        if not attr_name.startswith(\"__\") and not callable(getattr(config_class, attr_name)):\n",
    "            if attr_name=='model1d':\n",
    "                attr_val = getattr(config_class, attr_name)\n",
    "                for subattr, subvalue in zip(['model', 'input_type', 'embedding_dim', 'num_layers', 'num_heads'],\n",
    "                                             [attr_val.model, attr_val.input_type, attr_val.embedding_dim, \n",
    "                                              attr_val.num_layers, attr_val.num_heads]):\n",
    "                    config_dict[f'{attr_name}_{subattr}'] = subvalue\n",
    "            elif attr_name in ['model2d','model3d','model4d','modelfprf',\n",
    "                               'linear_warmup_cosine_annealing_lr','cosine_annealing_lr',\n",
    "                               'reduce_lr_on_plateau','one_cycle_lr']:\n",
    "                pass\n",
    "            else:\n",
    "                config_dict[attr_name] = getattr(config_class, attr_name)\n",
    "    return config_dict\n",
    "#config_dict = config_to_dict(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e27d8c6e-419a-4301-8523-93ba2ac90290",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activation': 'relu',\n",
       " 'batch_size': 256,\n",
       " 'cosine_annealing_lr': CosineAnnealingLR(eta_min=1e-06),\n",
       " 'dataset': 'BDE',\n",
       " 'device': 'cuda:0',\n",
       " 'dropout': 0.5,\n",
       " 'hidden_dim': 128,\n",
       " 'learning_rate': 0.001,\n",
       " 'linear_warmup_cosine_annealing_lr': LinearWarmupCosineAnnealingLR(warmup_steps=200, max_epochs=2000),\n",
       " 'max_num_conformers': 20,\n",
       " 'max_num_molecules': None,\n",
       " 'model1d': Model1D(model='LSTM', input_type='SMILES', embedding_dim=128, num_layers=4, num_heads=4),\n",
       " 'model2d': Model2D(model='GIN', gin=GIN(num_layers=6, virtual_node=False), gps=GPS(num_layers=6, walk_length=20, num_heads=4), chemprop=ChemProp(num_layers=6)),\n",
       " 'model3d': Model3D(model='PaiNN', augmentation=True, schnet=SchNet(hidden_dim=128, num_filters=5, num_interactions=6, num_gaussians=50, cutoff=10, readout='mean', dipole=False), dimenet=DimeNet(hidden_channels=128, out_channels=128, num_blocks=6, num_bilinear=8, num_spherical=7, num_radial=6), dimenetplusplus=DimeNetPlusPlus(hidden_channels=128, out_channels=128, num_blocks=4, int_emb_size=64, basis_emb_size=8, out_emb_channels=256, num_spherical=7, num_radial=6), gemnet=GemNet(num_spherical=7, num_radial=6, num_blocks=4, emb_size_atom=128, emb_size_edge=128, emb_size_trip=64, emb_size_rbf=16, emb_size_cbf=16, emb_size_bil_trip=64, num_before_skip=1, num_after_skip=1, num_concat=1, num_atoms=1, num_atom=2, bond_feat_dim=0), painn=PaiNN(hidden_dim=128, num_interactions=6, num_rbf=64, cutoff=12.0, readout='add', shared_interactions=False, shared_filters=False), clofnet=ClofNet(cutoff=6.5, num_layers=6, hidden_channels=128, num_radial=32), leftnet=LEFTNet(cutoff=6.5, num_layers=6, hidden_channels=128, num_radial=32), chiro=ChIRo(), chytorch_discrete=ChytorchDiscrete(max_distance=83, d_model=128, nhead=16, num_layers=8, dim_feedforward=512, shared_weights=False, dropout=0.1, norm_first=True, post_norm=True, zero_bias=True), chytorch_conformer=ChytorchConformer(nkernel=128, shared_layers=True, d_model=128, nhead=16, num_layers=8, dim_feedforward=512, dropout=0.1, norm_first=True, post_norm=True), chytorch_rotary=ChytorchRotary(d_model=128, nhead=16, num_layers=8, dim_feedforward=512, shared_weights=False, dropout=0.1, norm_first=False, post_norm=False)),\n",
       " 'model4d': Model4D(graph_encoder='SchNet', set_encoder='Attention', schnet=SchNet(hidden_dim=128, num_filters=5, num_interactions=6, num_gaussians=50, cutoff=10, readout='mean', dipole=False), dimenet=DimeNet(hidden_channels=128, out_channels=128, num_blocks=6, num_bilinear=8, num_spherical=7, num_radial=6), dimenetplusplus=DimeNetPlusPlus(hidden_channels=128, out_channels=128, num_blocks=4, int_emb_size=64, basis_emb_size=8, out_emb_channels=256, num_spherical=7, num_radial=6), gemnet=GemNet(num_spherical=7, num_radial=6, num_blocks=4, emb_size_atom=128, emb_size_edge=128, emb_size_trip=64, emb_size_rbf=16, emb_size_cbf=16, emb_size_bil_trip=64, num_before_skip=1, num_after_skip=1, num_concat=1, num_atoms=1, num_atom=2, bond_feat_dim=0), painn=PaiNN(hidden_dim=128, num_interactions=6, num_rbf=64, cutoff=12.0, readout='add', shared_interactions=False, shared_filters=False), clofnet=ClofNet(cutoff=6.5, num_layers=6, hidden_channels=128, num_radial=32), transformer=TransformerPooling(num_heads=8, num_layers=2)),\n",
       " 'modelfprf': ModelFPRF(n_jobs=8, n_estimators=500, min_samples_leaf=2, min_samples_split=10, min_impurity_decrease=0, warm_start=True),\n",
       " 'num_epochs': 2000,\n",
       " 'one_cycle_lr': OneCycleLR(max_lr=0.001, steps_per_epoch=100),\n",
       " 'patience': 200,\n",
       " 'reduce_lr_on_plateau': ReduceLROnPlateau(mode='min', factor=0.8, patience=20),\n",
       " 'scheduler': None,\n",
       " 'seed': 123,\n",
       " 'target': 'BindingEnergy',\n",
       " 'train_ratio': 0.7,\n",
       " 'valid_ratio': 0.1,\n",
       " 'weight_decay': 0.0001}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3edb7560",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model1D(vocab_size=data.vocab_size, tokenizer=data.tokenizer, smiles_ids=data.smiles_ids, **config_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "69530b9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#PARAMS = 1542785\n"
     ]
    }
   ],
   "source": [
    "print(f'#PARAMS = {sum(p.numel() for p in model.parameters() if p.requires_grad)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ffad2ebc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model1D(\n",
       "  (net): LSTM(\n",
       "    (embedding): Embedding(7924, 128, padding_idx=1)\n",
       "    (lstm): LSTM(128, 128, num_layers=4, batch_first=True, dropout=0.5)\n",
       "    (fc): Linear(in_features=128, out_features=1, bias=True)\n",
       "  )\n",
       "  (loss_fn): MSELoss()\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1a20c54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_name = f\"tmp_2\"\n",
    "\n",
    "dir_load_model = None\n",
    "log_dir_folder = '/mnt/code/logs/'\n",
    "log_dir_folder = os.path.join(log_dir_folder, dir_name)\n",
    "if os.path.exists(log_dir_folder):\n",
    "    if os.path.exists(os.path.join(log_dir_folder, \"last.ckpt\")):\n",
    "        dir_load_model = os.path.join(log_dir_folder, \"last.ckpt\")\n",
    "    csv_path = os.path.join(log_dir_folder, \"metrics.csv\")\n",
    "    while os.path.exists(csv_path):\n",
    "        csv_path = csv_path + '.bak'\n",
    "    if os.path.exists(os.path.join(log_dir_folder, \"metrics.csv\")):\n",
    "        os.rename(os.path.join(log_dir_folder, \"metrics.csv\"), csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "50c750fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_to_monitor = \"val_epoch_loss\"\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=log_dir_folder,\n",
    "    monitor=metric_to_monitor,\n",
    "    mode = 'min',\n",
    "    save_top_k=1,\n",
    "    save_last=True,\n",
    "    every_n_epochs=5,\n",
    "    filename=\"best-model-{epoch}-{val_epoch_loss:.4f}\",\n",
    ")\n",
    "\n",
    "early_stopping = early_stop_callback = EarlyStopping(\n",
    "        monitor=metric_to_monitor,  # The metric you want to monitor\n",
    "        patience=config.patience,  # Number of epochs with no improvement after which training will be stopped\n",
    "        verbose=True,\n",
    "        mode='min'  # Minimizing the validation loss\n",
    "    )\n",
    "\n",
    "tb_logger = TensorBoardLogger(log_dir_folder, name=\"tensorbord\")#, version=\"\", default_hp_metric=False)\n",
    "csv_logger = CSVLogger(log_dir_folder, name=\"\", version=\"\")\n",
    "\n",
    "model_params = dict(\n",
    "    devices=1, #args['ngpus'],\n",
    "    accelerator='gpu', #args['accelerator'],\n",
    "    default_root_dir=log_dir_folder, #args['log_dir'],\n",
    "    logger=[tb_logger, csv_logger],\n",
    "    enable_progress_bar=True)\n",
    "\n",
    "\n",
    "model_params.update(dict(\n",
    "    max_epochs=config.num_epochs,#1000,\n",
    "    callbacks=[checkpoint_callback, early_stopping],\n",
    "    gradient_clip_val=10,#args['clip_norm'],\n",
    "    #precision=\"16-mixed\",\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "843bb182",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision('medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "787f4d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Missing logger folder: /mnt/code/logs/tmp_2/tensorbord\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type    | Params\n",
      "------------------------------------\n",
      "0 | net     | LSTM    | 1.5 M \n",
      "1 | loss_fn | MSELoss | 0     \n",
      "------------------------------------\n",
      "1.5 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.5 M     Total params\n",
      "6.171     Total estimated model params size (MB)\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/lightning_fabric/loggers/csv_logs.py:183: UserWarning: Experiment logs directory /mnt/code/logs/tmp_2/ exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1609: PossibleUserWarning: The number of training batches (17) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "851409323ce145e9a38a67f61a75f5fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_epoch_loss improved. New best score: 7.141\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_epoch_loss improved by 6.140 >= min_delta = 0.0. New best score: 1.001\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:48: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:38\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 38\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:650\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    644\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_set_ckpt_path(\n\u001b[1;32m    645\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    646\u001b[0m     ckpt_path,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    647\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    648\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    649\u001b[0m )\n\u001b[0;32m--> 650\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    652\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1112\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   1110\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39mresume_end()\n\u001b[0;32m-> 1112\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1114\u001b[0m log\u001b[38;5;241m.\u001b[39mdetail(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1191\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_predict()\n\u001b[0;32m-> 1191\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1214\u001b[0m, in \u001b[0;36mTrainer._run_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1213\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1214\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pytorch_lightning/loops/loop.py:199\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 199\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:267\u001b[0m, in \u001b[0;36mFitLoop.advance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_training_epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 267\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pytorch_lightning/loops/loop.py:199\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 199\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py:241\u001b[0m, in \u001b[0;36mTrainingEpochLoop.advance\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;66;03m# -----------------------------------------\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;66;03m# SAVE METRICS TO LOGGERS AND PROGRESS_BAR\u001b[39;00m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;66;03m# -----------------------------------------\u001b[39;00m\n\u001b[0;32m--> 241\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_logger_connector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_train_step_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:157\u001b[0m, in \u001b[0;36mLoggerConnector.update_train_step_metrics\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshould_update_logs \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mfast_dev_run:\n\u001b[0;32m--> 157\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetrics\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlog\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:109\u001b[0m, in \u001b[0;36mLoggerConnector.log_metrics\u001b[0;34m(self, metrics, step)\u001b[0m\n\u001b[1;32m    108\u001b[0m logger\u001b[38;5;241m.\u001b[39mlog_metrics(metrics\u001b[38;5;241m=\u001b[39mscalar_metrics, step\u001b[38;5;241m=\u001b[39mstep)\n\u001b[0;32m--> 109\u001b[0m \u001b[43mlogger\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning_utilities/core/rank_zero.py:42\u001b[0m, in \u001b[0;36mrank_zero_only.<locals>.wrapped_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rank \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m default\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/lightning_fabric/loggers/csv_logs.py:140\u001b[0m, in \u001b[0;36mCSVLogger.save\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39msave()\n\u001b[0;32m--> 140\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexperiment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pytorch_lightning/loggers/csv_logs.py:62\u001b[0m, in \u001b[0;36mExperimentWriter.save\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     61\u001b[0m hparams_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_dir, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mNAME_HPARAMS_FILE)\n\u001b[0;32m---> 62\u001b[0m \u001b[43msave_hparams_to_yaml\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhparams_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39msave()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pytorch_lightning/core/saving.py:416\u001b[0m, in \u001b[0;36msave_hparams_to_yaml\u001b[0;34m(config_yaml, hparams, use_omegaconf)\u001b[0m\n\u001b[1;32m    415\u001b[0m     v \u001b[38;5;241m=\u001b[39m v\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, Enum) \u001b[38;5;28;01melse\u001b[39;00m v\n\u001b[0;32m--> 416\u001b[0m     \u001b[43myaml\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/yaml/__init__.py:253\u001b[0m, in \u001b[0;36mdump\u001b[0;34m(data, stream, Dumper, **kwds)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;124;03mSerialize a Python object into a YAML stream.\u001b[39;00m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;124;03mIf stream is None, return the produced string instead.\u001b[39;00m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 253\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdump_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDumper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDumper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/yaml/__init__.py:241\u001b[0m, in \u001b[0;36mdump_all\u001b[0;34m(documents, stream, Dumper, default_style, default_flow_style, canonical, indent, width, allow_unicode, line_break, encoding, explicit_start, explicit_end, version, tags, sort_keys)\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m documents:\n\u001b[0;32m--> 241\u001b[0m     \u001b[43mdumper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepresent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    242\u001b[0m dumper\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/yaml/representer.py:28\u001b[0m, in \u001b[0;36mBaseRepresenter.represent\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     27\u001b[0m node \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrepresent_data(data)\n\u001b[0;32m---> 28\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mserialize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrepresented_objects \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/yaml/serializer.py:54\u001b[0m, in \u001b[0;36mSerializer.serialize\u001b[0;34m(self, node)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39manchor_node(node)\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mserialize_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39memit(DocumentEndEvent(explicit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_explicit_end))\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/yaml/serializer.py:98\u001b[0m, in \u001b[0;36mSerializer.serialize_node\u001b[0;34m(self, node, parent, index)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m node\u001b[38;5;241m.\u001b[39mvalue:\n\u001b[0;32m---> 98\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mserialize_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m     index \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/yaml/serializer.py:100\u001b[0m, in \u001b[0;36mSerializer.serialize_node\u001b[0;34m(self, node, parent, index)\u001b[0m\n\u001b[1;32m     99\u001b[0m         index \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 100\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43memit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mSequenceEndEvent\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(node, MappingNode):\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/yaml/emitter.py:115\u001b[0m, in \u001b[0;36mEmitter.emit\u001b[0;34m(self, event)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevents\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 115\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/yaml/emitter.py:374\u001b[0m, in \u001b[0;36mEmitter.expect_first_block_sequence_item\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexpect_first_block_sequence_item\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 374\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpect_block_sequence_item\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfirst\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/yaml/emitter.py:384\u001b[0m, in \u001b[0;36mEmitter.expect_block_sequence_item\u001b[0;34m(self, first)\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstates\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexpect_block_sequence_item)\n\u001b[0;32m--> 384\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpect_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequence\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/yaml/emitter.py:242\u001b[0m, in \u001b[0;36mEmitter.expect_node\u001b[0;34m(self, root, sequence, mapping, simple_key)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_anchor(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m&\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 242\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_tag\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent, ScalarEvent):\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/yaml/emitter.py:473\u001b[0m, in \u001b[0;36mEmitter.process_tag\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    472\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstyle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 473\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstyle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoose_scalar_style\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcanonical \u001b[38;5;129;01mor\u001b[39;00m tag \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    475\u001b[0m     ((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstyle \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent\u001b[38;5;241m.\u001b[39mimplicit[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    476\u001b[0m             \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstyle \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent\u001b[38;5;241m.\u001b[39mimplicit[\u001b[38;5;241m1\u001b[39m]))):\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/yaml/emitter.py:496\u001b[0m, in \u001b[0;36mEmitter.choose_scalar_style\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39manalysis \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 496\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39manalysis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43manalyze_scalar\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent\u001b[38;5;241m.\u001b[39mstyle \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcanonical:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/yaml/emitter.py:672\u001b[0m, in \u001b[0;36mEmitter.analyze_scalar\u001b[0;34m(self, scalar)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;66;03m# Check for indicators.\u001b[39;00m\n\u001b[0;32m--> 672\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    673\u001b[0m     \u001b[38;5;66;03m# Leading indicators are special characters.\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m#,[]\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m&*!|>\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m@`\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mTrainer(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_params)\u001b[38;5;66;03m#, profiler=profiler)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdir_load_model\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:608\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    606\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_unwrap_optimized(model)\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m_lightning_module \u001b[38;5;241m=\u001b[39m model\n\u001b[0;32m--> 608\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:54\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     52\u001b[0m         trainer\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_exception\u001b[39m\u001b[38;5;124m\"\u001b[39m, exception)\n\u001b[1;32m     53\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m logger \u001b[38;5;129;01min\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mloggers:\n\u001b[0;32m---> 54\u001b[0m             \u001b[43mlogger\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinalize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfailed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:\n\u001b[1;32m     56\u001b[0m     trainer\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mINTERRUPTED\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning_utilities/core/rank_zero.py:42\u001b[0m, in \u001b[0;36mrank_zero_only.<locals>.wrapped_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `rank_zero_only.rank` needs to be set before use\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rank \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m default\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/lightning_fabric/loggers/csv_logs.py:148\u001b[0m, in \u001b[0;36mCSVLogger.finalize\u001b[0;34m(self, status)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_experiment \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;66;03m# When using multiprocessing, finalize() should be a no-op on the main process, as no experiment has been\u001b[39;00m\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;66;03m# initialized there\u001b[39;00m\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 148\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning_utilities/core/rank_zero.py:42\u001b[0m, in \u001b[0;36mrank_zero_only.<locals>.wrapped_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `rank_zero_only.rank` needs to be set before use\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rank \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m default\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/lightning_fabric/loggers/csv_logs.py:140\u001b[0m, in \u001b[0;36mCSVLogger.save\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;129m@rank_zero_only\u001b[39m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39msave()\n\u001b[0;32m--> 140\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexperiment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pytorch_lightning/loggers/csv_logs.py:62\u001b[0m, in \u001b[0;36mExperimentWriter.save\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Save recorded hparams and metrics into files.\"\"\"\u001b[39;00m\n\u001b[1;32m     61\u001b[0m hparams_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_dir, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mNAME_HPARAMS_FILE)\n\u001b[0;32m---> 62\u001b[0m \u001b[43msave_hparams_to_yaml\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhparams_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39msave()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pytorch_lightning/core/saving.py:416\u001b[0m, in \u001b[0;36msave_hparams_to_yaml\u001b[0;34m(config_yaml, hparams, use_omegaconf)\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    415\u001b[0m     v \u001b[38;5;241m=\u001b[39m v\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, Enum) \u001b[38;5;28;01melse\u001b[39;00m v\n\u001b[0;32m--> 416\u001b[0m     \u001b[43myaml\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    418\u001b[0m     warn(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSkipping \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m parameter because it is not possible to safely dump to YAML.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/yaml/__init__.py:253\u001b[0m, in \u001b[0;36mdump\u001b[0;34m(data, stream, Dumper, **kwds)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdump\u001b[39m(data, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, Dumper\u001b[38;5;241m=\u001b[39mDumper, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[1;32m    249\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;124;03m    Serialize a Python object into a YAML stream.\u001b[39;00m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;124;03m    If stream is None, return the produced string instead.\u001b[39;00m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 253\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdump_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDumper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDumper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/yaml/__init__.py:241\u001b[0m, in \u001b[0;36mdump_all\u001b[0;34m(documents, stream, Dumper, default_style, default_flow_style, canonical, indent, width, allow_unicode, line_break, encoding, explicit_start, explicit_end, version, tags, sort_keys)\u001b[0m\n\u001b[1;32m    239\u001b[0m     dumper\u001b[38;5;241m.\u001b[39mopen()\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m documents:\n\u001b[0;32m--> 241\u001b[0m         \u001b[43mdumper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepresent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    242\u001b[0m     dumper\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/yaml/representer.py:28\u001b[0m, in \u001b[0;36mBaseRepresenter.represent\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrepresent\u001b[39m(\u001b[38;5;28mself\u001b[39m, data):\n\u001b[1;32m     27\u001b[0m     node \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrepresent_data(data)\n\u001b[0;32m---> 28\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mserialize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrepresented_objects \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobject_keeper \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/yaml/serializer.py:54\u001b[0m, in \u001b[0;36mSerializer.serialize\u001b[0;34m(self, node)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39memit(DocumentStartEvent(explicit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_explicit_start,\n\u001b[1;32m     52\u001b[0m     version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_version, tags\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_tags))\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39manchor_node(node)\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mserialize_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39memit(DocumentEndEvent(explicit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_explicit_end))\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mserialized_nodes \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/yaml/serializer.py:98\u001b[0m, in \u001b[0;36mSerializer.serialize_node\u001b[0;34m(self, node, parent, index)\u001b[0m\n\u001b[1;32m     96\u001b[0m index \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m node\u001b[38;5;241m.\u001b[39mvalue:\n\u001b[0;32m---> 98\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mserialize_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m     index \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39memit(SequenceEndEvent())\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/yaml/serializer.py:100\u001b[0m, in \u001b[0;36mSerializer.serialize_node\u001b[0;34m(self, node, parent, index)\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mserialize_node(item, node, index)\n\u001b[1;32m     99\u001b[0m         index \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 100\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43memit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mSequenceEndEvent\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(node, MappingNode):\n\u001b[1;32m    102\u001b[0m     implicit \u001b[38;5;241m=\u001b[39m (node\u001b[38;5;241m.\u001b[39mtag\n\u001b[1;32m    103\u001b[0m                 \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresolve(MappingNode, node\u001b[38;5;241m.\u001b[39mvalue, \u001b[38;5;28;01mTrue\u001b[39;00m))\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/yaml/emitter.py:115\u001b[0m, in \u001b[0;36mEmitter.emit\u001b[0;34m(self, event)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneed_more_events():\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevents\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 115\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/yaml/emitter.py:374\u001b[0m, in \u001b[0;36mEmitter.expect_first_block_sequence_item\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexpect_first_block_sequence_item\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 374\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpect_block_sequence_item\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfirst\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/yaml/emitter.py:384\u001b[0m, in \u001b[0;36mEmitter.expect_block_sequence_item\u001b[0;34m(self, first)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrite_indicator(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m, indention\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstates\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexpect_block_sequence_item)\n\u001b[0;32m--> 384\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpect_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequence\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/yaml/emitter.py:242\u001b[0m, in \u001b[0;36mEmitter.expect_node\u001b[0;34m(self, root, sequence, mapping, simple_key)\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent, (ScalarEvent, CollectionStartEvent)):\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_anchor(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m&\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 242\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_tag\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent, ScalarEvent):\n\u001b[1;32m    244\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexpect_scalar()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/yaml/emitter.py:473\u001b[0m, in \u001b[0;36mEmitter.process_tag\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    471\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent, ScalarEvent):\n\u001b[1;32m    472\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstyle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 473\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstyle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoose_scalar_style\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    474\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcanonical \u001b[38;5;129;01mor\u001b[39;00m tag \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    475\u001b[0m         ((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstyle \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent\u001b[38;5;241m.\u001b[39mimplicit[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    476\u001b[0m                 \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstyle \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent\u001b[38;5;241m.\u001b[39mimplicit[\u001b[38;5;241m1\u001b[39m]))):\n\u001b[1;32m    477\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepared_tag \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/yaml/emitter.py:496\u001b[0m, in \u001b[0;36mEmitter.choose_scalar_style\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mchoose_scalar_style\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    495\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39manalysis \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 496\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39manalysis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43manalyze_scalar\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    497\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent\u001b[38;5;241m.\u001b[39mstyle \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcanonical:\n\u001b[1;32m    498\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/yaml/emitter.py:735\u001b[0m, in \u001b[0;36mEmitter.analyze_scalar\u001b[0;34m(self, scalar)\u001b[0m\n\u001b[1;32m    733\u001b[0m     index \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    734\u001b[0m     preceded_by_whitespace \u001b[38;5;241m=\u001b[39m (ch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\0\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\x85\u001b[39;00m\u001b[38;5;130;01m\\u2028\u001b[39;00m\u001b[38;5;130;01m\\u2029\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 735\u001b[0m     followed_by_whitespace \u001b[38;5;241m=\u001b[39m (index\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mscalar\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m    736\u001b[0m             scalar[index\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\0\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\x85\u001b[39;00m\u001b[38;5;130;01m\\u2028\u001b[39;00m\u001b[38;5;130;01m\\u2029\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    738\u001b[0m \u001b[38;5;66;03m# Let's decide what styles are allowed.\u001b[39;00m\n\u001b[1;32m    739\u001b[0m allow_flow_plain \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(**model_params)#, profiler=profiler)\n",
    "trainer.fit(model, datamodule=data, ckpt_path=dir_load_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9bff1c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.utilities.parsing import is_picklable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ba90d5ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.360118865966797"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(model.logging_info[\"train_loss\"]).mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "90de3575",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.66825008392334"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(model.logging_info[\"train_loss\"]).mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5fa189",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e9f7f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d090a7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = generate_checkpoint_filename()\n",
    "early_stopping = EarlyStopping(patience=config.patience, path=checkpoint_path)\n",
    "print(f'Checkpoint path: {checkpoint_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bb2800",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74ed650",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)\n",
    "\n",
    "if config.scheduler == 'ReduceLROnPlateau':\n",
    "    scheduler = ReduceLROnPlateau(\n",
    "        optimizer, verbose=True, **asdict(config.reduce_lr_on_plateau))\n",
    "elif config.scheduler == 'CosineAnnealingLR':\n",
    "    scheduler = CosineAnnealingLR(\n",
    "        optimizer, T_max=config.num_epochs, verbose=True, **asdict(config.cosine_annealing_lr))\n",
    "elif config.scheduler == 'LinearWarmupCosineAnnealingLR':\n",
    "    scheduler = LinearWarmupCosineAnnealingLR(\n",
    "        optimizer, **asdict(config.linear_warmup_cosine_annealing_lr))\n",
    "else:\n",
    "    scheduler = None\n",
    "\n",
    "best_val_error = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848e9551",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(config.num_epochs):\n",
    "    loss = train(train_loader)\n",
    "    if scheduler is not None:\n",
    "        scheduler.step(loss)\n",
    "    valid_error = eval(valid_loader)\n",
    "\n",
    "    early_stopping(valid_error, model)\n",
    "    if early_stopping.counter == 0:\n",
    "        test_error = eval(test_loader)\n",
    "    if early_stopping.early_stop:\n",
    "        print('Early stopping...')\n",
    "        break\n",
    "\n",
    "    writer.add_scalar(f'Loss_{config.model1d.model}/{config.model1d.input_type}'\n",
    "                      f'/{config.dataset}/{config.target}/train', loss, epoch)\n",
    "    writer.add_scalar(f'Loss_{config.model1d.model}/{config.model1d.input_type}'\n",
    "                      f'/{config.dataset}/{config.target}/valid', valid_error, epoch)\n",
    "    writer.add_scalar(f'Loss_{config.model1d.model}/{config.model1d.input_type}'\n",
    "                      f'/{config.dataset}/{config.target}/test', test_error, epoch)\n",
    "    print(f'Progress: {epoch}/{config.num_epochs}/{loss:.5f}/{valid_error:.5f}/{test_error:.5f}')\n",
    "    res_dict[dataname][algo][d]['loss'].append(loss)\n",
    "    res_dict[dataname][algo][d]['valid_error'].append(valid_error)\n",
    "    res_dict[dataname][algo][d]['test_error'].append(test_error)\n",
    "\n",
    "model.load_state_dict(torch.load(checkpoint_path))\n",
    "test_error = eval(test_loader)\n",
    "test_r2 = eval_r2(test_loader)\n",
    "print(f'Best validation error: {-early_stopping.best_score:.7f}')\n",
    "print(f'Test error: {test_error:.7f}')\n",
    "print(f'Test r2: {test_r2:.7f}')\n",
    "\n",
    "res_dict[dataname][algo][d]['Test error'] = test_error\n",
    "res_dict[dataname][algo][d]['Test r2'] = test_r2\n",
    "res_dict[dataname][algo][d]['checkpoint']=checkpoint_path\n",
    "\n",
    "\n",
    "#os.remove(checkpoint_path)\n",
    "writer.close()\n",
    "\n",
    "import pickle\n",
    "with open(\"res_dict_1d.pkl\", \"wb\") as f:\n",
    "pickle.dump(res_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcca3d98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
