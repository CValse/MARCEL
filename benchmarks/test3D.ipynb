{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a92808ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri May 10 13:54:50 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A10G                    On  | 00000000:00:16.0 Off |                    0 |\n",
      "|  0%   23C    P8              15W / 300W |      0MiB / 23028MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7119098",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export CUDA_HOME=/usr/local/cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1050ea3b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://data.pyg.org/whl/torch-2.0.1+cu122.html\n",
      "Requirement already satisfied: torch in /opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages (1.8.2)\n",
      "Collecting torch\n",
      "  Downloading torch-2.3.0-cp38-cp38-manylinux1_x86_64.whl.metadata (26 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages (from torch) (3.13.1)\n",
      "Collecting typing-extensions>=4.8.0 (from torch)\n",
      "  Downloading typing_extensions-4.11.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting sympy (from torch)\n",
      "  Downloading sympy-1.12-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: networkx in /opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages (from torch) (2023.6.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
      "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n",
      "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==2.3.0 (from torch)\n",
      "  Downloading triton-2.3.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages (from jinja2->torch) (2.1.1)\n",
      "Collecting mpmath>=0.19 (from sympy->torch)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Downloading torch-2.3.0-cp38-cp38-manylinux1_x86_64.whl (779.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m779.1/779.1 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading triton-2.3.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.0/168.0 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading typing_extensions-4.11.0-py3-none-any.whl (34 kB)\n",
      "Downloading sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m67.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m54.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25h\u001b[33mDEPRECATION: pytorch-lightning 1.5.6 has a non-standard dependency specifier torch>=1.7.*. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pytorch-lightning or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: mpmath, typing-extensions, triton, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.6.3\n",
      "    Uninstalling typing_extensions-4.6.3:\n",
      "      Successfully uninstalled typing_extensions-4.6.3\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.8.2\n",
      "    Uninstalling torch-1.8.2:\n",
      "      Successfully uninstalled torch-1.8.2\n",
      "Successfully installed mpmath-1.3.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 sympy-1.12 torch-2.3.0 triton-2.3.0 typing-extensions-4.11.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://data.pyg.org/whl/torch-2.0.1+cu122.html\n",
      "Requirement already satisfied: torch-geometric in /opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages (2.5.3)\n",
      "Requirement already satisfied: tqdm in /opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages (from torch-geometric) (4.66.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages (from torch-geometric) (1.23.3)\n",
      "Requirement already satisfied: scipy in /opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages (from torch-geometric) (1.10.1)\n",
      "Requirement already satisfied: fsspec in /opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages (from torch-geometric) (2023.6.0)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages (from torch-geometric) (3.1.2)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages (from torch-geometric) (3.8.1)\n",
      "Requirement already satisfied: requests in /opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages (from torch-geometric) (2.28.1)\n",
      "Requirement already satisfied: pyparsing in /opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages (from torch-geometric) (3.0.9)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages (from torch-geometric) (1.0.2)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages (from torch-geometric) (5.9.8)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages (from aiohttp->torch-geometric) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages (from aiohttp->torch-geometric) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages (from aiohttp->torch-geometric) (6.0.5)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages (from aiohttp->torch-geometric) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages (from aiohttp->torch-geometric) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages (from aiohttp->torch-geometric) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages (from aiohttp->torch-geometric) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages (from jinja2->torch-geometric) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages (from requests->torch-geometric) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages (from requests->torch-geometric) (1.26.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages (from requests->torch-geometric) (2024.2.2)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages (from scikit-learn->torch-geometric) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages (from scikit-learn->torch-geometric) (2.2.0)\n",
      "\u001b[33mDEPRECATION: pytorch-lightning 1.5.6 has a non-standard dependency specifier torch>=1.7.*. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pytorch-lightning or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in links: https://data.pyg.org/whl/torch-2.1.0+cu122.html\n",
      "Collecting torch-scatter\n",
      "  Using cached torch_scatter-2.1.2.tar.gz (108 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting torch-sparse\n",
      "  Using cached torch_sparse-0.6.18.tar.gz (209 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting torch-cluster\n",
      "  Using cached torch_cluster-1.6.3.tar.gz (54 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: scipy in /opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages (from torch-sparse) (1.10.1)\n",
      "Requirement already satisfied: numpy<1.27.0,>=1.19.5 in /opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages (from scipy->torch-sparse) (1.23.3)\n",
      "Building wheels for collected packages: torch-scatter, torch-sparse, torch-cluster\n",
      "  Building wheel for torch-scatter (setup.py) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[30 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m running bdist_wheel\n",
      "  \u001b[31m   \u001b[0m running build\n",
      "  \u001b[31m   \u001b[0m running build_py\n",
      "  \u001b[31m   \u001b[0m creating build\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-3.8\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-3.8/torch_scatter\n",
      "  \u001b[31m   \u001b[0m copying torch_scatter/__init__.py -> build/lib.linux-x86_64-3.8/torch_scatter\n",
      "  \u001b[31m   \u001b[0m copying torch_scatter/testing.py -> build/lib.linux-x86_64-3.8/torch_scatter\n",
      "  \u001b[31m   \u001b[0m copying torch_scatter/placeholder.py -> build/lib.linux-x86_64-3.8/torch_scatter\n",
      "  \u001b[31m   \u001b[0m copying torch_scatter/segment_coo.py -> build/lib.linux-x86_64-3.8/torch_scatter\n",
      "  \u001b[31m   \u001b[0m copying torch_scatter/utils.py -> build/lib.linux-x86_64-3.8/torch_scatter\n",
      "  \u001b[31m   \u001b[0m copying torch_scatter/segment_csr.py -> build/lib.linux-x86_64-3.8/torch_scatter\n",
      "  \u001b[31m   \u001b[0m copying torch_scatter/scatter.py -> build/lib.linux-x86_64-3.8/torch_scatter\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-3.8/torch_scatter/composite\n",
      "  \u001b[31m   \u001b[0m copying torch_scatter/composite/__init__.py -> build/lib.linux-x86_64-3.8/torch_scatter/composite\n",
      "  \u001b[31m   \u001b[0m copying torch_scatter/composite/std.py -> build/lib.linux-x86_64-3.8/torch_scatter/composite\n",
      "  \u001b[31m   \u001b[0m copying torch_scatter/composite/softmax.py -> build/lib.linux-x86_64-3.8/torch_scatter/composite\n",
      "  \u001b[31m   \u001b[0m copying torch_scatter/composite/logsumexp.py -> build/lib.linux-x86_64-3.8/torch_scatter/composite\n",
      "  \u001b[31m   \u001b[0m running egg_info\n",
      "  \u001b[31m   \u001b[0m writing torch_scatter.egg-info/PKG-INFO\n",
      "  \u001b[31m   \u001b[0m writing dependency_links to torch_scatter.egg-info/dependency_links.txt\n",
      "  \u001b[31m   \u001b[0m writing requirements to torch_scatter.egg-info/requires.txt\n",
      "  \u001b[31m   \u001b[0m writing top-level names to torch_scatter.egg-info/top_level.txt\n",
      "  \u001b[31m   \u001b[0m reading manifest file 'torch_scatter.egg-info/SOURCES.txt'\n",
      "  \u001b[31m   \u001b[0m reading manifest template 'MANIFEST.in'\n",
      "  \u001b[31m   \u001b[0m warning: no previously-included files matching '*' found under directory 'test'\n",
      "  \u001b[31m   \u001b[0m adding license file 'LICENSE'\n",
      "  \u001b[31m   \u001b[0m writing manifest file 'torch_scatter.egg-info/SOURCES.txt'\n",
      "  \u001b[31m   \u001b[0m running build_ext\n",
      "  \u001b[31m   \u001b[0m error: [Errno 2] No such file or directory: '/usr/local/cuda/bin/nvcc'\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[31m  ERROR: Failed building wheel for torch-scatter\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[?25h  Running setup.py clean for torch-scatter\n",
      "  Building wheel for torch-sparse (setup.py) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[53 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m running bdist_wheel\n",
      "  \u001b[31m   \u001b[0m running build\n",
      "  \u001b[31m   \u001b[0m running build_py\n",
      "  \u001b[31m   \u001b[0m creating build\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-3.8\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-3.8/torch_sparse\n",
      "  \u001b[31m   \u001b[0m copying torch_sparse/spadd.py -> build/lib.linux-x86_64-3.8/torch_sparse\n",
      "  \u001b[31m   \u001b[0m copying torch_sparse/spmm.py -> build/lib.linux-x86_64-3.8/torch_sparse\n",
      "  \u001b[31m   \u001b[0m copying torch_sparse/add.py -> build/lib.linux-x86_64-3.8/torch_sparse\n",
      "  \u001b[31m   \u001b[0m copying torch_sparse/__init__.py -> build/lib.linux-x86_64-3.8/torch_sparse\n",
      "  \u001b[31m   \u001b[0m copying torch_sparse/coalesce.py -> build/lib.linux-x86_64-3.8/torch_sparse\n",
      "  \u001b[31m   \u001b[0m copying torch_sparse/saint.py -> build/lib.linux-x86_64-3.8/torch_sparse\n",
      "  \u001b[31m   \u001b[0m copying torch_sparse/eye.py -> build/lib.linux-x86_64-3.8/torch_sparse\n",
      "  \u001b[31m   \u001b[0m copying torch_sparse/reduce.py -> build/lib.linux-x86_64-3.8/torch_sparse\n",
      "  \u001b[31m   \u001b[0m copying torch_sparse/testing.py -> build/lib.linux-x86_64-3.8/torch_sparse\n",
      "  \u001b[31m   \u001b[0m copying torch_sparse/spspmm.py -> build/lib.linux-x86_64-3.8/torch_sparse\n",
      "  \u001b[31m   \u001b[0m copying torch_sparse/masked_select.py -> build/lib.linux-x86_64-3.8/torch_sparse\n",
      "  \u001b[31m   \u001b[0m copying torch_sparse/mul.py -> build/lib.linux-x86_64-3.8/torch_sparse\n",
      "  \u001b[31m   \u001b[0m copying torch_sparse/permute.py -> build/lib.linux-x86_64-3.8/torch_sparse\n",
      "  \u001b[31m   \u001b[0m copying torch_sparse/transpose.py -> build/lib.linux-x86_64-3.8/torch_sparse\n",
      "  \u001b[31m   \u001b[0m copying torch_sparse/convert.py -> build/lib.linux-x86_64-3.8/torch_sparse\n",
      "  \u001b[31m   \u001b[0m copying torch_sparse/storage.py -> build/lib.linux-x86_64-3.8/torch_sparse\n",
      "  \u001b[31m   \u001b[0m copying torch_sparse/sample.py -> build/lib.linux-x86_64-3.8/torch_sparse\n",
      "  \u001b[31m   \u001b[0m copying torch_sparse/utils.py -> build/lib.linux-x86_64-3.8/torch_sparse\n",
      "  \u001b[31m   \u001b[0m copying torch_sparse/index_select.py -> build/lib.linux-x86_64-3.8/torch_sparse\n",
      "  \u001b[31m   \u001b[0m copying torch_sparse/select.py -> build/lib.linux-x86_64-3.8/torch_sparse\n",
      "  \u001b[31m   \u001b[0m copying torch_sparse/cat.py -> build/lib.linux-x86_64-3.8/torch_sparse\n",
      "  \u001b[31m   \u001b[0m copying torch_sparse/typing.py -> build/lib.linux-x86_64-3.8/torch_sparse\n",
      "  \u001b[31m   \u001b[0m copying torch_sparse/tensor.py -> build/lib.linux-x86_64-3.8/torch_sparse\n",
      "  \u001b[31m   \u001b[0m copying torch_sparse/matmul.py -> build/lib.linux-x86_64-3.8/torch_sparse\n",
      "  \u001b[31m   \u001b[0m copying torch_sparse/bandwidth.py -> build/lib.linux-x86_64-3.8/torch_sparse\n",
      "  \u001b[31m   \u001b[0m copying torch_sparse/metis.py -> build/lib.linux-x86_64-3.8/torch_sparse\n",
      "  \u001b[31m   \u001b[0m copying torch_sparse/narrow.py -> build/lib.linux-x86_64-3.8/torch_sparse\n",
      "  \u001b[31m   \u001b[0m copying torch_sparse/rw.py -> build/lib.linux-x86_64-3.8/torch_sparse\n",
      "  \u001b[31m   \u001b[0m copying torch_sparse/diag.py -> build/lib.linux-x86_64-3.8/torch_sparse\n",
      "  \u001b[31m   \u001b[0m running egg_info\n",
      "  \u001b[31m   \u001b[0m writing torch_sparse.egg-info/PKG-INFO\n",
      "  \u001b[31m   \u001b[0m writing dependency_links to torch_sparse.egg-info/dependency_links.txt\n",
      "  \u001b[31m   \u001b[0m writing requirements to torch_sparse.egg-info/requires.txt\n",
      "  \u001b[31m   \u001b[0m writing top-level names to torch_sparse.egg-info/top_level.txt\n",
      "  \u001b[31m   \u001b[0m reading manifest file 'torch_sparse.egg-info/SOURCES.txt'\n",
      "  \u001b[31m   \u001b[0m reading manifest template 'MANIFEST.in'\n",
      "  \u001b[31m   \u001b[0m warning: no previously-included files matching '*' found under directory 'third_party/parallel-hashmap/css'\n",
      "  \u001b[31m   \u001b[0m warning: no previously-included files matching '*' found under directory 'third_party/parallel-hashmap/html'\n",
      "  \u001b[31m   \u001b[0m warning: no previously-included files matching '*' found under directory 'third_party/parallel-hashmap/tests'\n",
      "  \u001b[31m   \u001b[0m warning: no previously-included files matching '*' found under directory 'third_party/parallel-hashmap/examples'\n",
      "  \u001b[31m   \u001b[0m warning: no previously-included files matching '*' found under directory 'third_party/parallel-hashmap/benchmark'\n",
      "  \u001b[31m   \u001b[0m warning: no previously-included files matching '*' found under directory 'test'\n",
      "  \u001b[31m   \u001b[0m warning: no previously-included files matching '*' found under directory 'benchmark'\n",
      "  \u001b[31m   \u001b[0m adding license file 'LICENSE'\n",
      "  \u001b[31m   \u001b[0m writing manifest file 'torch_sparse.egg-info/SOURCES.txt'\n",
      "  \u001b[31m   \u001b[0m running build_ext\n",
      "  \u001b[31m   \u001b[0m error: [Errno 2] No such file or directory: '/usr/local/cuda/bin/nvcc'\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[31m  ERROR: Failed building wheel for torch-sparse\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[?25h  Running setup.py clean for torch-sparse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Building wheel for torch-cluster (setup.py) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[29 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m running bdist_wheel\n",
      "  \u001b[31m   \u001b[0m running build\n",
      "  \u001b[31m   \u001b[0m running build_py\n",
      "  \u001b[31m   \u001b[0m creating build\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-3.8\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-3.8/torch_cluster\n",
      "  \u001b[31m   \u001b[0m copying torch_cluster/graclus.py -> build/lib.linux-x86_64-3.8/torch_cluster\n",
      "  \u001b[31m   \u001b[0m copying torch_cluster/__init__.py -> build/lib.linux-x86_64-3.8/torch_cluster\n",
      "  \u001b[31m   \u001b[0m copying torch_cluster/knn.py -> build/lib.linux-x86_64-3.8/torch_cluster\n",
      "  \u001b[31m   \u001b[0m copying torch_cluster/testing.py -> build/lib.linux-x86_64-3.8/torch_cluster\n",
      "  \u001b[31m   \u001b[0m copying torch_cluster/nearest.py -> build/lib.linux-x86_64-3.8/torch_cluster\n",
      "  \u001b[31m   \u001b[0m copying torch_cluster/fps.py -> build/lib.linux-x86_64-3.8/torch_cluster\n",
      "  \u001b[31m   \u001b[0m copying torch_cluster/typing.py -> build/lib.linux-x86_64-3.8/torch_cluster\n",
      "  \u001b[31m   \u001b[0m copying torch_cluster/radius.py -> build/lib.linux-x86_64-3.8/torch_cluster\n",
      "  \u001b[31m   \u001b[0m copying torch_cluster/sampler.py -> build/lib.linux-x86_64-3.8/torch_cluster\n",
      "  \u001b[31m   \u001b[0m copying torch_cluster/grid.py -> build/lib.linux-x86_64-3.8/torch_cluster\n",
      "  \u001b[31m   \u001b[0m copying torch_cluster/rw.py -> build/lib.linux-x86_64-3.8/torch_cluster\n",
      "  \u001b[31m   \u001b[0m running egg_info\n",
      "  \u001b[31m   \u001b[0m writing torch_cluster.egg-info/PKG-INFO\n",
      "  \u001b[31m   \u001b[0m writing dependency_links to torch_cluster.egg-info/dependency_links.txt\n",
      "  \u001b[31m   \u001b[0m writing requirements to torch_cluster.egg-info/requires.txt\n",
      "  \u001b[31m   \u001b[0m writing top-level names to torch_cluster.egg-info/top_level.txt\n",
      "  \u001b[31m   \u001b[0m reading manifest file 'torch_cluster.egg-info/SOURCES.txt'\n",
      "  \u001b[31m   \u001b[0m reading manifest template 'MANIFEST.in'\n",
      "  \u001b[31m   \u001b[0m warning: no previously-included files matching '*' found under directory 'test'\n",
      "  \u001b[31m   \u001b[0m adding license file 'LICENSE'\n",
      "  \u001b[31m   \u001b[0m writing manifest file 'torch_cluster.egg-info/SOURCES.txt'\n",
      "  \u001b[31m   \u001b[0m running build_ext\n",
      "  \u001b[31m   \u001b[0m error: [Errno 2] No such file or directory: '/usr/local/cuda/bin/nvcc'\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[31m  ERROR: Failed building wheel for torch-cluster\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[?25h  Running setup.py clean for torch-cluster\n",
      "Failed to build torch-scatter torch-sparse torch-cluster\n",
      "\u001b[31mERROR: Could not build wheels for torch-scatter, torch-sparse, torch-cluster, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade torch -f https://data.pyg.org/whl/torch-2.0.1+cu122.html\n",
    "!pip install --upgrade torch-geometric -f https://data.pyg.org/whl/torch-2.0.1+cu122.html\n",
    "!pip install --upgrade torch-scatter torch-sparse torch-cluster -f https://data.pyg.org/whl/torch-2.1.0+cu122.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad21b96d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://data.pyg.org/whl/torch-2.1.0+cu122.html\n",
      "Collecting torch-scatter\n",
      "  Using cached torch_scatter-2.1.2.tar.gz (108 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting torch-sparse\n",
      "  Using cached torch_sparse-0.6.18.tar.gz (209 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting torch-cluster\n",
      "  Using cached torch_cluster-1.6.3.tar.gz (54 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: scipy in /opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages (from torch-sparse) (1.10.1)\n",
      "Requirement already satisfied: numpy<1.27.0,>=1.19.5 in /opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages (from scipy->torch-sparse) (1.23.3)\n",
      "Building wheels for collected packages: torch-scatter, torch-sparse, torch-cluster\n",
      "  Building wheel for torch-scatter (setup.py) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[42 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m running bdist_wheel\n",
      "  \u001b[31m   \u001b[0m running build\n",
      "  \u001b[31m   \u001b[0m running build_py\n",
      "  \u001b[31m   \u001b[0m creating build\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-3.8\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-3.8/torch_scatter\n",
      "  \u001b[31m   \u001b[0m copying torch_scatter/__init__.py -> build/lib.linux-x86_64-3.8/torch_scatter\n",
      "  \u001b[31m   \u001b[0m copying torch_scatter/testing.py -> build/lib.linux-x86_64-3.8/torch_scatter\n",
      "  \u001b[31m   \u001b[0m copying torch_scatter/placeholder.py -> build/lib.linux-x86_64-3.8/torch_scatter\n",
      "  \u001b[31m   \u001b[0m copying torch_scatter/segment_coo.py -> build/lib.linux-x86_64-3.8/torch_scatter\n",
      "  \u001b[31m   \u001b[0m copying torch_scatter/utils.py -> build/lib.linux-x86_64-3.8/torch_scatter\n",
      "  \u001b[31m   \u001b[0m copying torch_scatter/segment_csr.py -> build/lib.linux-x86_64-3.8/torch_scatter\n",
      "  \u001b[31m   \u001b[0m copying torch_scatter/scatter.py -> build/lib.linux-x86_64-3.8/torch_scatter\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-3.8/torch_scatter/composite\n",
      "  \u001b[31m   \u001b[0m copying torch_scatter/composite/__init__.py -> build/lib.linux-x86_64-3.8/torch_scatter/composite\n",
      "  \u001b[31m   \u001b[0m copying torch_scatter/composite/std.py -> build/lib.linux-x86_64-3.8/torch_scatter/composite\n",
      "  \u001b[31m   \u001b[0m copying torch_scatter/composite/softmax.py -> build/lib.linux-x86_64-3.8/torch_scatter/composite\n",
      "  \u001b[31m   \u001b[0m copying torch_scatter/composite/logsumexp.py -> build/lib.linux-x86_64-3.8/torch_scatter/composite\n",
      "  \u001b[31m   \u001b[0m running egg_info\n",
      "  \u001b[31m   \u001b[0m writing torch_scatter.egg-info/PKG-INFO\n",
      "  \u001b[31m   \u001b[0m writing dependency_links to torch_scatter.egg-info/dependency_links.txt\n",
      "  \u001b[31m   \u001b[0m writing requirements to torch_scatter.egg-info/requires.txt\n",
      "  \u001b[31m   \u001b[0m writing top-level names to torch_scatter.egg-info/top_level.txt\n",
      "  \u001b[31m   \u001b[0m reading manifest file 'torch_scatter.egg-info/SOURCES.txt'\n",
      "  \u001b[31m   \u001b[0m reading manifest template 'MANIFEST.in'\n",
      "  \u001b[31m   \u001b[0m warning: no previously-included files matching '*' found under directory 'test'\n",
      "  \u001b[31m   \u001b[0m adding license file 'LICENSE'\n",
      "  \u001b[31m   \u001b[0m writing manifest file 'torch_scatter.egg-info/SOURCES.txt'\n",
      "  \u001b[31m   \u001b[0m running build_ext\n",
      "  \u001b[31m   \u001b[0m building 'torch_scatter._segment_csr_cpu' extension\n",
      "  \u001b[31m   \u001b[0m creating build/temp.linux-x86_64-3.8\n",
      "  \u001b[31m   \u001b[0m creating build/temp.linux-x86_64-3.8/csrc\n",
      "  \u001b[31m   \u001b[0m creating build/temp.linux-x86_64-3.8/csrc/cpu\n",
      "  \u001b[31m   \u001b[0m gcc -pthread -B /opt/conda/envs/chem38_c20240222/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -DWITH_PYTHON -Icsrc -I/opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages/torch/include -I/opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages/torch/include/TH -I/opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages/torch/include/THC -I/opt/conda/envs/chem38_c20240222/include/python3.8 -c csrc/segment_csr.cpp -o build/temp.linux-x86_64-3.8/csrc/segment_csr.o -O3 -Wno-sign-compare -DAT_PARALLEL_OPENMP -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=_segment_csr_cpu -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
      "  \u001b[31m   \u001b[0m cc1plus: warning: command line option ‘-Wstrict-prototypes’ is valid for C/ObjC but not for C++\n",
      "  \u001b[31m   \u001b[0m gcc -pthread -B /opt/conda/envs/chem38_c20240222/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -DWITH_PYTHON -Icsrc -I/opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages/torch/include -I/opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages/torch/include/TH -I/opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages/torch/include/THC -I/opt/conda/envs/chem38_c20240222/include/python3.8 -c csrc/cpu/segment_csr_cpu.cpp -o build/temp.linux-x86_64-3.8/csrc/cpu/segment_csr_cpu.o -O3 -Wno-sign-compare -DAT_PARALLEL_OPENMP -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=_segment_csr_cpu -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
      "  \u001b[31m   \u001b[0m cc1plus: warning: command line option ‘-Wstrict-prototypes’ is valid for C/ObjC but not for C++\n",
      "  \u001b[31m   \u001b[0m csrc/cpu/segment_csr_cpu.cpp:6:10: fatal error: ATen/OpMathType.h: No such file or directory\n",
      "  \u001b[31m   \u001b[0m  #include <ATen/OpMathType.h>\n",
      "  \u001b[31m   \u001b[0m           ^~~~~~~~~~~~~~~~~~~\n",
      "  \u001b[31m   \u001b[0m compilation terminated.\n",
      "  \u001b[31m   \u001b[0m error: command 'gcc' failed with exit status 1\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[31m  ERROR: Failed building wheel for torch-scatter\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[?25h  Running setup.py clean for torch-scatter\n",
      "  Building wheel for torch-sparse (setup.py) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[221 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m running bdist_wheel\n",
      "  \u001b[31m   \u001b[0m running build\n",
      "  \u001b[31m   \u001b[0m running build_py\n",
      "  \u001b[31m   \u001b[0m creating build\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-3.8\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-3.8/torch_sparse\n",
      "  \u001b[31m   \u001b[0m copying torch_sparse/spadd.py -> build/lib.linux-x86_64-3.8/torch_sparse\n",
      "  \u001b[31m   \u001b[0m copying torch_sparse/spmm.py -> build/lib.linux-x86_64-3.8/torch_sparse\n",
      "  \u001b[31m   \u001b[0m copying torch_sparse/add.py -> build/lib.linux-x86_64-3.8/torch_sparse\n",
      "  \u001b[31m   \u001b[0m copying torch_sparse/__init__.py -> build/lib.linux-x86_64-3.8/torch_sparse\n",
      "  \u001b[31m   \u001b[0m copying torch_sparse/coalesce.py -> build/lib.linux-x86_64-3.8/torch_sparse\n",
      "  \u001b[31m   \u001b[0m copying torch_sparse/saint.py -> build/lib.linux-x86_64-3.8/torch_sparse\n",
      "  \u001b[31m   \u001b[0m copying torch_sparse/eye.py -> build/lib.linux-x86_64-3.8/torch_sparse\n",
      "  \u001b[31m   \u001b[0m copying torch_sparse/reduce.py -> build/lib.linux-x86_64-3.8/torch_sparse\n",
      "  \u001b[31m   \u001b[0m copying torch_sparse/testing.py -> build/lib.linux-x86_64-3.8/torch_sparse\n",
      "  \u001b[31m   \u001b[0m copying torch_sparse/spspmm.py -> build/lib.linux-x86_64-3.8/torch_sparse\n",
      "  \u001b[31m   \u001b[0m copying torch_sparse/masked_select.py -> build/lib.linux-x86_64-3.8/torch_sparse\n",
      "  \u001b[31m   \u001b[0m copying torch_sparse/mul.py -> build/lib.linux-x86_64-3.8/torch_sparse\n",
      "  \u001b[31m   \u001b[0m copying torch_sparse/permute.py -> build/lib.linux-x86_64-3.8/torch_sparse\n",
      "  \u001b[31m   \u001b[0m copying torch_sparse/transpose.py -> build/lib.linux-x86_64-3.8/torch_sparse\n",
      "  \u001b[31m   \u001b[0m copying torch_sparse/convert.py -> build/lib.linux-x86_64-3.8/torch_sparse\n",
      "  \u001b[31m   \u001b[0m copying torch_sparse/storage.py -> build/lib.linux-x86_64-3.8/torch_sparse\n",
      "  \u001b[31m   \u001b[0m copying torch_sparse/sample.py -> build/lib.linux-x86_64-3.8/torch_sparse\n",
      "  \u001b[31m   \u001b[0m copying torch_sparse/utils.py -> build/lib.linux-x86_64-3.8/torch_sparse\n",
      "  \u001b[31m   \u001b[0m copying torch_sparse/index_select.py -> build/lib.linux-x86_64-3.8/torch_sparse\n",
      "  \u001b[31m   \u001b[0m copying torch_sparse/select.py -> build/lib.linux-x86_64-3.8/torch_sparse\n",
      "  \u001b[31m   \u001b[0m copying torch_sparse/cat.py -> build/lib.linux-x86_64-3.8/torch_sparse\n",
      "  \u001b[31m   \u001b[0m copying torch_sparse/typing.py -> build/lib.linux-x86_64-3.8/torch_sparse\n",
      "  \u001b[31m   \u001b[0m copying torch_sparse/tensor.py -> build/lib.linux-x86_64-3.8/torch_sparse\n",
      "  \u001b[31m   \u001b[0m copying torch_sparse/matmul.py -> build/lib.linux-x86_64-3.8/torch_sparse\n",
      "  \u001b[31m   \u001b[0m copying torch_sparse/bandwidth.py -> build/lib.linux-x86_64-3.8/torch_sparse\n",
      "  \u001b[31m   \u001b[0m copying torch_sparse/metis.py -> build/lib.linux-x86_64-3.8/torch_sparse\n",
      "  \u001b[31m   \u001b[0m copying torch_sparse/narrow.py -> build/lib.linux-x86_64-3.8/torch_sparse\n",
      "  \u001b[31m   \u001b[0m copying torch_sparse/rw.py -> build/lib.linux-x86_64-3.8/torch_sparse\n",
      "  \u001b[31m   \u001b[0m copying torch_sparse/diag.py -> build/lib.linux-x86_64-3.8/torch_sparse\n",
      "  \u001b[31m   \u001b[0m running egg_info\n",
      "  \u001b[31m   \u001b[0m writing torch_sparse.egg-info/PKG-INFO\n",
      "  \u001b[31m   \u001b[0m writing dependency_links to torch_sparse.egg-info/dependency_links.txt\n",
      "  \u001b[31m   \u001b[0m writing requirements to torch_sparse.egg-info/requires.txt\n",
      "  \u001b[31m   \u001b[0m writing top-level names to torch_sparse.egg-info/top_level.txt\n",
      "  \u001b[31m   \u001b[0m reading manifest file 'torch_sparse.egg-info/SOURCES.txt'\n",
      "  \u001b[31m   \u001b[0m reading manifest template 'MANIFEST.in'\n",
      "  \u001b[31m   \u001b[0m warning: no previously-included files matching '*' found under directory 'third_party/parallel-hashmap/css'\n",
      "  \u001b[31m   \u001b[0m warning: no previously-included files matching '*' found under directory 'third_party/parallel-hashmap/html'\n",
      "  \u001b[31m   \u001b[0m warning: no previously-included files matching '*' found under directory 'third_party/parallel-hashmap/tests'\n",
      "  \u001b[31m   \u001b[0m warning: no previously-included files matching '*' found under directory 'third_party/parallel-hashmap/examples'\n",
      "  \u001b[31m   \u001b[0m warning: no previously-included files matching '*' found under directory 'third_party/parallel-hashmap/benchmark'\n",
      "  \u001b[31m   \u001b[0m warning: no previously-included files matching '*' found under directory 'test'\n",
      "  \u001b[31m   \u001b[0m warning: no previously-included files matching '*' found under directory 'benchmark'\n",
      "  \u001b[31m   \u001b[0m adding license file 'LICENSE'\n",
      "  \u001b[31m   \u001b[0m writing manifest file 'torch_sparse.egg-info/SOURCES.txt'\n",
      "  \u001b[31m   \u001b[0m running build_ext\n",
      "  \u001b[31m   \u001b[0m building 'torch_sparse._rw_cpu' extension\n",
      "  \u001b[31m   \u001b[0m creating /tmp/pip-install-9htt1bck/torch-sparse_15b9e3bfc6d44c7f8e82891b7c13b6b5/build/temp.linux-x86_64-3.8\n",
      "  \u001b[31m   \u001b[0m creating /tmp/pip-install-9htt1bck/torch-sparse_15b9e3bfc6d44c7f8e82891b7c13b6b5/build/temp.linux-x86_64-3.8/csrc\n",
      "  \u001b[31m   \u001b[0m creating /tmp/pip-install-9htt1bck/torch-sparse_15b9e3bfc6d44c7f8e82891b7c13b6b5/build/temp.linux-x86_64-3.8/csrc/cpu\n",
      "  \u001b[31m   \u001b[0m /opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages/torch/utils/cpp_extension.py:283: UserWarning:\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m                                !! WARNING !!\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "  \u001b[31m   \u001b[0m Your compiler (c++) is not compatible with the compiler Pytorch was\n",
      "  \u001b[31m   \u001b[0m built with for this platform, which is g++ on linux. Please\n",
      "  \u001b[31m   \u001b[0m use g++ to to compile your extension. Alternatively, you may\n",
      "  \u001b[31m   \u001b[0m compile PyTorch from source using c++, and then you can also use\n",
      "  \u001b[31m   \u001b[0m c++ to compile your extension.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m See https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md for help\n",
      "  \u001b[31m   \u001b[0m with compiling PyTorch from source.\n",
      "  \u001b[31m   \u001b[0m !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m                               !! WARNING !!\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m   warnings.warn(WRONG_COMPILER_WARNING.format(\n",
      "  \u001b[31m   \u001b[0m Emitting ninja build file /tmp/pip-install-9htt1bck/torch-sparse_15b9e3bfc6d44c7f8e82891b7c13b6b5/build/temp.linux-x86_64-3.8/build.ninja...\n",
      "  \u001b[31m   \u001b[0m Compiling objects...\n",
      "  \u001b[31m   \u001b[0m Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "  \u001b[31m   \u001b[0m [1/2] c++ -MMD -MF /tmp/pip-install-9htt1bck/torch-sparse_15b9e3bfc6d44c7f8e82891b7c13b6b5/build/temp.linux-x86_64-3.8/csrc/cpu/rw_cpu.o.d -pthread -B /opt/conda/envs/chem38_c20240222/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -DWITH_PYTHON -Icsrc -I/tmp/pip-install-9htt1bck/torch-sparse_15b9e3bfc6d44c7f8e82891b7c13b6b5/third_party/parallel-hashmap -I/opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages/torch/include -I/opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages/torch/include/TH -I/opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages/torch/include/THC -I/opt/conda/envs/chem38_c20240222/include/python3.8 -c -c /tmp/pip-install-9htt1bck/torch-sparse_15b9e3bfc6d44c7f8e82891b7c13b6b5/csrc/cpu/rw_cpu.cpp -o /tmp/pip-install-9htt1bck/torch-sparse_15b9e3bfc6d44c7f8e82891b7c13b6b5/build/temp.linux-x86_64-3.8/csrc/cpu/rw_cpu.o -O3 -Wno-sign-compare -DAT_PARALLEL_OPENMP -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=_rw_cpu -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
      "  \u001b[31m   \u001b[0m cc1plus: warning: command line option ‘-Wstrict-prototypes’ is valid for C/ObjC but not for C++\n",
      "  \u001b[31m   \u001b[0m In file included from /tmp/pip-install-9htt1bck/torch-sparse_15b9e3bfc6d44c7f8e82891b7c13b6b5/csrc/cpu/rw_cpu.cpp:3:\n",
      "  \u001b[31m   \u001b[0m /tmp/pip-install-9htt1bck/torch-sparse_15b9e3bfc6d44c7f8e82891b7c13b6b5/csrc/cpu/utils.h:8: warning: \"CHECK_LT\" redefined\n",
      "  \u001b[31m   \u001b[0m  #define CHECK_LT(low, high) AT_ASSERTM(low < high, \"low must be smaller than high\")\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m In file included from /opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages/torch/include/c10/util/Logging.h:28,\n",
      "  \u001b[31m   \u001b[0m                  from /opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages/torch/include/c10/core/TensorImpl.h:21,\n",
      "  \u001b[31m   \u001b[0m                  from /opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages/torch/include/ATen/core/TensorBody.h:13,\n",
      "  \u001b[31m   \u001b[0m                  from /opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages/torch/include/ATen/Tensor.h:3,\n",
      "  \u001b[31m   \u001b[0m                  from /opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages/torch/include/ATen/Context.h:4,\n",
      "  \u001b[31m   \u001b[0m                  from /opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages/torch/include/ATen/ATen.h:9,\n",
      "  \u001b[31m   \u001b[0m                  from /opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
      "  \u001b[31m   \u001b[0m                  from /opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
      "  \u001b[31m   \u001b[0m                  from /opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
      "  \u001b[31m   \u001b[0m                  from /opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
      "  \u001b[31m   \u001b[0m                  from /opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
      "  \u001b[31m   \u001b[0m                  from /opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
      "  \u001b[31m   \u001b[0m                  from /opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
      "  \u001b[31m   \u001b[0m                  from /opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/torch.h:3,\n",
      "  \u001b[31m   \u001b[0m                  from /tmp/pip-install-9htt1bck/torch-sparse_15b9e3bfc6d44c7f8e82891b7c13b6b5/csrc/cpu/../extensions.h:2,\n",
      "  \u001b[31m   \u001b[0m                  from /tmp/pip-install-9htt1bck/torch-sparse_15b9e3bfc6d44c7f8e82891b7c13b6b5/csrc/cpu/rw_cpu.h:3,\n",
      "  \u001b[31m   \u001b[0m                  from /tmp/pip-install-9htt1bck/torch-sparse_15b9e3bfc6d44c7f8e82891b7c13b6b5/csrc/cpu/rw_cpu.cpp:1:\n",
      "  \u001b[31m   \u001b[0m /opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages/torch/include/c10/util/logging_is_not_google_glog.h:143: note: this is the location of the previous definition\n",
      "  \u001b[31m   \u001b[0m  #define CHECK_LT(val1, val2) CHECK_OP(val1, val2, <)\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m [2/2] c++ -MMD -MF /tmp/pip-install-9htt1bck/torch-sparse_15b9e3bfc6d44c7f8e82891b7c13b6b5/build/temp.linux-x86_64-3.8/csrc/rw.o.d -pthread -B /opt/conda/envs/chem38_c20240222/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -DWITH_PYTHON -Icsrc -I/tmp/pip-install-9htt1bck/torch-sparse_15b9e3bfc6d44c7f8e82891b7c13b6b5/third_party/parallel-hashmap -I/opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages/torch/include -I/opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages/torch/include/TH -I/opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages/torch/include/THC -I/opt/conda/envs/chem38_c20240222/include/python3.8 -c -c /tmp/pip-install-9htt1bck/torch-sparse_15b9e3bfc6d44c7f8e82891b7c13b6b5/csrc/rw.cpp -o /tmp/pip-install-9htt1bck/torch-sparse_15b9e3bfc6d44c7f8e82891b7c13b6b5/build/temp.linux-x86_64-3.8/csrc/rw.o -O3 -Wno-sign-compare -DAT_PARALLEL_OPENMP -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=_rw_cpu -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
      "  \u001b[31m   \u001b[0m cc1plus: warning: command line option ‘-Wstrict-prototypes’ is valid for C/ObjC but not for C++\n",
      "  \u001b[31m   \u001b[0m g++ -pthread -shared -B /opt/conda/envs/chem38_c20240222/compiler_compat -L/opt/conda/envs/chem38_c20240222/lib -Wl,-rpath=/opt/conda/envs/chem38_c20240222/lib -Wl,--no-as-needed -Wl,--sysroot=/ /tmp/pip-install-9htt1bck/torch-sparse_15b9e3bfc6d44c7f8e82891b7c13b6b5/build/temp.linux-x86_64-3.8/csrc/rw.o /tmp/pip-install-9htt1bck/torch-sparse_15b9e3bfc6d44c7f8e82891b7c13b6b5/build/temp.linux-x86_64-3.8/csrc/cpu/rw_cpu.o -L/opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages/torch/lib -lc10 -ltorch -ltorch_cpu -ltorch_python -o build/lib.linux-x86_64-3.8/torch_sparse/_rw_cpu.so -s\n",
      "  \u001b[31m   \u001b[0m building 'torch_sparse._rw_cuda' extension\n",
      "  \u001b[31m   \u001b[0m creating /tmp/pip-install-9htt1bck/torch-sparse_15b9e3bfc6d44c7f8e82891b7c13b6b5/build/temp.linux-x86_64-3.8/csrc/cuda\n",
      "  \u001b[31m   \u001b[0m /opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages/torch/utils/cpp_extension.py:283: UserWarning:\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m                                !! WARNING !!\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "  \u001b[31m   \u001b[0m Your compiler (c++) is not compatible with the compiler Pytorch was\n",
      "  \u001b[31m   \u001b[0m built with for this platform, which is g++ on linux. Please\n",
      "  \u001b[31m   \u001b[0m use g++ to to compile your extension. Alternatively, you may\n",
      "  \u001b[31m   \u001b[0m compile PyTorch from source using c++, and then you can also use\n",
      "  \u001b[31m   \u001b[0m c++ to compile your extension.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m See https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md for help\n",
      "  \u001b[31m   \u001b[0m with compiling PyTorch from source.\n",
      "  \u001b[31m   \u001b[0m !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m                               !! WARNING !!\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m   warnings.warn(WRONG_COMPILER_WARNING.format(\n",
      "  \u001b[31m   \u001b[0m Emitting ninja build file /tmp/pip-install-9htt1bck/torch-sparse_15b9e3bfc6d44c7f8e82891b7c13b6b5/build/temp.linux-x86_64-3.8/build.ninja...\n",
      "  \u001b[31m   \u001b[0m Compiling objects...\n",
      "  \u001b[31m   \u001b[0m Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "  \u001b[31m   \u001b[0m [1/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output /tmp/pip-install-9htt1bck/torch-sparse_15b9e3bfc6d44c7f8e82891b7c13b6b5/build/temp.linux-x86_64-3.8/csrc/cuda/rw_cuda.o.d -DWITH_PYTHON -DWITH_CUDA -Icsrc -I/tmp/pip-install-9htt1bck/torch-sparse_15b9e3bfc6d44c7f8e82891b7c13b6b5/third_party/parallel-hashmap -I/opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages/torch/include -I/opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages/torch/include/TH -I/opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/envs/chem38_c20240222/include/python3.8 -c -c /tmp/pip-install-9htt1bck/torch-sparse_15b9e3bfc6d44c7f8e82891b7c13b6b5/csrc/cuda/rw_cuda.cu -o /tmp/pip-install-9htt1bck/torch-sparse_15b9e3bfc6d44c7f8e82891b7c13b6b5/build/temp.linux-x86_64-3.8/csrc/cuda/rw_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -O3 --expt-relaxed-constexpr -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=_rw_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_86,code=sm_86 -std=c++14\n",
      "  \u001b[31m   \u001b[0m \u001b[31mFAILED: \u001b[0m/tmp/pip-install-9htt1bck/torch-sparse_15b9e3bfc6d44c7f8e82891b7c13b6b5/build/temp.linux-x86_64-3.8/csrc/cuda/rw_cuda.o\n",
      "  \u001b[31m   \u001b[0m /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output /tmp/pip-install-9htt1bck/torch-sparse_15b9e3bfc6d44c7f8e82891b7c13b6b5/build/temp.linux-x86_64-3.8/csrc/cuda/rw_cuda.o.d -DWITH_PYTHON -DWITH_CUDA -Icsrc -I/tmp/pip-install-9htt1bck/torch-sparse_15b9e3bfc6d44c7f8e82891b7c13b6b5/third_party/parallel-hashmap -I/opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages/torch/include -I/opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages/torch/include/TH -I/opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/envs/chem38_c20240222/include/python3.8 -c -c /tmp/pip-install-9htt1bck/torch-sparse_15b9e3bfc6d44c7f8e82891b7c13b6b5/csrc/cuda/rw_cuda.cu -o /tmp/pip-install-9htt1bck/torch-sparse_15b9e3bfc6d44c7f8e82891b7c13b6b5/build/temp.linux-x86_64-3.8/csrc/cuda/rw_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -O3 --expt-relaxed-constexpr -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=_rw_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_86,code=sm_86 -std=c++14\n",
      "  \u001b[31m   \u001b[0m /bin/sh: /usr/local/cuda/bin/nvcc: No such file or directory\n",
      "  \u001b[31m   \u001b[0m [2/3] c++ -MMD -MF /tmp/pip-install-9htt1bck/torch-sparse_15b9e3bfc6d44c7f8e82891b7c13b6b5/build/temp.linux-x86_64-3.8/csrc/cpu/rw_cpu.o.d -pthread -B /opt/conda/envs/chem38_c20240222/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -DWITH_PYTHON -DWITH_CUDA -Icsrc -I/tmp/pip-install-9htt1bck/torch-sparse_15b9e3bfc6d44c7f8e82891b7c13b6b5/third_party/parallel-hashmap -I/opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages/torch/include -I/opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages/torch/include/TH -I/opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/envs/chem38_c20240222/include/python3.8 -c -c /tmp/pip-install-9htt1bck/torch-sparse_15b9e3bfc6d44c7f8e82891b7c13b6b5/csrc/cpu/rw_cpu.cpp -o /tmp/pip-install-9htt1bck/torch-sparse_15b9e3bfc6d44c7f8e82891b7c13b6b5/build/temp.linux-x86_64-3.8/csrc/cpu/rw_cpu.o -O3 -Wno-sign-compare -DAT_PARALLEL_OPENMP -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=_rw_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
      "  \u001b[31m   \u001b[0m cc1plus: warning: command line option ‘-Wstrict-prototypes’ is valid for C/ObjC but not for C++\n",
      "  \u001b[31m   \u001b[0m In file included from /tmp/pip-install-9htt1bck/torch-sparse_15b9e3bfc6d44c7f8e82891b7c13b6b5/csrc/cpu/rw_cpu.cpp:3:\n",
      "  \u001b[31m   \u001b[0m /tmp/pip-install-9htt1bck/torch-sparse_15b9e3bfc6d44c7f8e82891b7c13b6b5/csrc/cpu/utils.h:8: warning: \"CHECK_LT\" redefined\n",
      "  \u001b[31m   \u001b[0m  #define CHECK_LT(low, high) AT_ASSERTM(low < high, \"low must be smaller than high\")\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m In file included from /opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages/torch/include/c10/util/Logging.h:28,\n",
      "  \u001b[31m   \u001b[0m                  from /opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages/torch/include/c10/core/TensorImpl.h:21,\n",
      "  \u001b[31m   \u001b[0m                  from /opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages/torch/include/ATen/core/TensorBody.h:13,\n",
      "  \u001b[31m   \u001b[0m                  from /opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages/torch/include/ATen/Tensor.h:3,\n",
      "  \u001b[31m   \u001b[0m                  from /opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages/torch/include/ATen/Context.h:4,\n",
      "  \u001b[31m   \u001b[0m                  from /opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages/torch/include/ATen/ATen.h:9,\n",
      "  \u001b[31m   \u001b[0m                  from /opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
      "  \u001b[31m   \u001b[0m                  from /opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
      "  \u001b[31m   \u001b[0m                  from /opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
      "  \u001b[31m   \u001b[0m                  from /opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
      "  \u001b[31m   \u001b[0m                  from /opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
      "  \u001b[31m   \u001b[0m                  from /opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
      "  \u001b[31m   \u001b[0m                  from /opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
      "  \u001b[31m   \u001b[0m                  from /opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/torch.h:3,\n",
      "  \u001b[31m   \u001b[0m                  from /tmp/pip-install-9htt1bck/torch-sparse_15b9e3bfc6d44c7f8e82891b7c13b6b5/csrc/cpu/../extensions.h:2,\n",
      "  \u001b[31m   \u001b[0m                  from /tmp/pip-install-9htt1bck/torch-sparse_15b9e3bfc6d44c7f8e82891b7c13b6b5/csrc/cpu/rw_cpu.h:3,\n",
      "  \u001b[31m   \u001b[0m                  from /tmp/pip-install-9htt1bck/torch-sparse_15b9e3bfc6d44c7f8e82891b7c13b6b5/csrc/cpu/rw_cpu.cpp:1:\n",
      "  \u001b[31m   \u001b[0m /opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages/torch/include/c10/util/logging_is_not_google_glog.h:143: note: this is the location of the previous definition\n",
      "  \u001b[31m   \u001b[0m  #define CHECK_LT(val1, val2) CHECK_OP(val1, val2, <)\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m [3/3] c++ -MMD -MF /tmp/pip-install-9htt1bck/torch-sparse_15b9e3bfc6d44c7f8e82891b7c13b6b5/build/temp.linux-x86_64-3.8/csrc/rw.o.d -pthread -B /opt/conda/envs/chem38_c20240222/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -DWITH_PYTHON -DWITH_CUDA -Icsrc -I/tmp/pip-install-9htt1bck/torch-sparse_15b9e3bfc6d44c7f8e82891b7c13b6b5/third_party/parallel-hashmap -I/opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages/torch/include -I/opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages/torch/include/TH -I/opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/envs/chem38_c20240222/include/python3.8 -c -c /tmp/pip-install-9htt1bck/torch-sparse_15b9e3bfc6d44c7f8e82891b7c13b6b5/csrc/rw.cpp -o /tmp/pip-install-9htt1bck/torch-sparse_15b9e3bfc6d44c7f8e82891b7c13b6b5/build/temp.linux-x86_64-3.8/csrc/rw.o -O3 -Wno-sign-compare -DAT_PARALLEL_OPENMP -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=_rw_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
      "  \u001b[31m   \u001b[0m cc1plus: warning: command line option ‘-Wstrict-prototypes’ is valid for C/ObjC but not for C++\n",
      "  \u001b[31m   \u001b[0m ninja: build stopped: subcommand failed.\n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages/torch/utils/cpp_extension.py\", line 1667, in _run_ninja_build\n",
      "  \u001b[31m   \u001b[0m     subprocess.run(\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/conda/envs/chem38_c20240222/lib/python3.8/subprocess.py\", line 516, in run\n",
      "  \u001b[31m   \u001b[0m     raise CalledProcessError(retcode, process.args,\n",
      "  \u001b[31m   \u001b[0m subprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m The above exception was the direct cause of the following exception:\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \"<string>\", line 2, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"<pip-setuptools-caller>\", line 34, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-install-9htt1bck/torch-sparse_15b9e3bfc6d44c7f8e82891b7c13b6b5/setup.py\", line 147, in <module>\n",
      "  \u001b[31m   \u001b[0m     setup(\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages/setuptools/__init__.py\", line 153, in setup\n",
      "  \u001b[31m   \u001b[0m     return distutils.core.setup(**attrs)\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/conda/envs/chem38_c20240222/lib/python3.8/distutils/core.py\", line 148, in setup\n",
      "  \u001b[31m   \u001b[0m     dist.run_commands()\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/conda/envs/chem38_c20240222/lib/python3.8/distutils/dist.py\", line 966, in run_commands\n",
      "  \u001b[31m   \u001b[0m     self.run_command(cmd)\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/conda/envs/chem38_c20240222/lib/python3.8/distutils/dist.py\", line 985, in run_command\n",
      "  \u001b[31m   \u001b[0m     cmd_obj.run()\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages/wheel/bdist_wheel.py\", line 364, in run\n",
      "  \u001b[31m   \u001b[0m     self.run_command(\"build\")\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/conda/envs/chem38_c20240222/lib/python3.8/distutils/cmd.py\", line 313, in run_command\n",
      "  \u001b[31m   \u001b[0m     self.distribution.run_command(command)\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/conda/envs/chem38_c20240222/lib/python3.8/distutils/dist.py\", line 985, in run_command\n",
      "  \u001b[31m   \u001b[0m     cmd_obj.run()\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/conda/envs/chem38_c20240222/lib/python3.8/distutils/command/build.py\", line 135, in run\n",
      "  \u001b[31m   \u001b[0m     self.run_command(cmd_name)\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/conda/envs/chem38_c20240222/lib/python3.8/distutils/cmd.py\", line 313, in run_command\n",
      "  \u001b[31m   \u001b[0m     self.distribution.run_command(command)\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/conda/envs/chem38_c20240222/lib/python3.8/distutils/dist.py\", line 985, in run_command\n",
      "  \u001b[31m   \u001b[0m     cmd_obj.run()\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages/setuptools/command/build_ext.py\", line 79, in run\n",
      "  \u001b[31m   \u001b[0m     _build_ext.run(self)\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages/Cython/Distutils/old_build_ext.py\", line 186, in run\n",
      "  \u001b[31m   \u001b[0m     _build_ext.build_ext.run(self)\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/conda/envs/chem38_c20240222/lib/python3.8/distutils/command/build_ext.py\", line 340, in run\n",
      "  \u001b[31m   \u001b[0m     self.build_extensions()\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages/torch/utils/cpp_extension.py\", line 708, in build_extensions\n",
      "  \u001b[31m   \u001b[0m     build_ext.build_extensions(self)\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages/Cython/Distutils/old_build_ext.py\", line 195, in build_extensions\n",
      "  \u001b[31m   \u001b[0m     _build_ext.build_ext.build_extensions(self)\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/conda/envs/chem38_c20240222/lib/python3.8/distutils/command/build_ext.py\", line 449, in build_extensions\n",
      "  \u001b[31m   \u001b[0m     self._build_extensions_serial()\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/conda/envs/chem38_c20240222/lib/python3.8/distutils/command/build_ext.py\", line 474, in _build_extensions_serial\n",
      "  \u001b[31m   \u001b[0m     self.build_extension(ext)\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages/setuptools/command/build_ext.py\", line 202, in build_extension\n",
      "  \u001b[31m   \u001b[0m     _build_ext.build_extension(self, ext)\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/conda/envs/chem38_c20240222/lib/python3.8/distutils/command/build_ext.py\", line 528, in build_extension\n",
      "  \u001b[31m   \u001b[0m     objects = self.compiler.compile(sources,\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages/torch/utils/cpp_extension.py\", line 529, in unix_wrap_ninja_compile\n",
      "  \u001b[31m   \u001b[0m     _write_ninja_file_and_compile_objects(\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages/torch/utils/cpp_extension.py\", line 1354, in _write_ninja_file_and_compile_objects\n",
      "  \u001b[31m   \u001b[0m     _run_ninja_build(\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages/torch/utils/cpp_extension.py\", line 1683, in _run_ninja_build\n",
      "  \u001b[31m   \u001b[0m     raise RuntimeError(message) from e\n",
      "  \u001b[31m   \u001b[0m RuntimeError: Error compiling objects for extension\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[31m  ERROR: Failed building wheel for torch-sparse\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[?25h  Running setup.py clean for torch-sparse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Building wheel for torch-cluster (setup.py) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[46 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m running bdist_wheel\n",
      "  \u001b[31m   \u001b[0m running build\n",
      "  \u001b[31m   \u001b[0m running build_py\n",
      "  \u001b[31m   \u001b[0m creating build\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-3.8\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-3.8/torch_cluster\n",
      "  \u001b[31m   \u001b[0m copying torch_cluster/graclus.py -> build/lib.linux-x86_64-3.8/torch_cluster\n",
      "  \u001b[31m   \u001b[0m copying torch_cluster/__init__.py -> build/lib.linux-x86_64-3.8/torch_cluster\n",
      "  \u001b[31m   \u001b[0m copying torch_cluster/knn.py -> build/lib.linux-x86_64-3.8/torch_cluster\n",
      "  \u001b[31m   \u001b[0m copying torch_cluster/testing.py -> build/lib.linux-x86_64-3.8/torch_cluster\n",
      "  \u001b[31m   \u001b[0m copying torch_cluster/nearest.py -> build/lib.linux-x86_64-3.8/torch_cluster\n",
      "  \u001b[31m   \u001b[0m copying torch_cluster/fps.py -> build/lib.linux-x86_64-3.8/torch_cluster\n",
      "  \u001b[31m   \u001b[0m copying torch_cluster/typing.py -> build/lib.linux-x86_64-3.8/torch_cluster\n",
      "  \u001b[31m   \u001b[0m copying torch_cluster/radius.py -> build/lib.linux-x86_64-3.8/torch_cluster\n",
      "  \u001b[31m   \u001b[0m copying torch_cluster/sampler.py -> build/lib.linux-x86_64-3.8/torch_cluster\n",
      "  \u001b[31m   \u001b[0m copying torch_cluster/grid.py -> build/lib.linux-x86_64-3.8/torch_cluster\n",
      "  \u001b[31m   \u001b[0m copying torch_cluster/rw.py -> build/lib.linux-x86_64-3.8/torch_cluster\n",
      "  \u001b[31m   \u001b[0m running egg_info\n",
      "  \u001b[31m   \u001b[0m writing torch_cluster.egg-info/PKG-INFO\n",
      "  \u001b[31m   \u001b[0m writing dependency_links to torch_cluster.egg-info/dependency_links.txt\n",
      "  \u001b[31m   \u001b[0m writing requirements to torch_cluster.egg-info/requires.txt\n",
      "  \u001b[31m   \u001b[0m writing top-level names to torch_cluster.egg-info/top_level.txt\n",
      "  \u001b[31m   \u001b[0m reading manifest file 'torch_cluster.egg-info/SOURCES.txt'\n",
      "  \u001b[31m   \u001b[0m reading manifest template 'MANIFEST.in'\n",
      "  \u001b[31m   \u001b[0m warning: no previously-included files matching '*' found under directory 'test'\n",
      "  \u001b[31m   \u001b[0m adding license file 'LICENSE'\n",
      "  \u001b[31m   \u001b[0m writing manifest file 'torch_cluster.egg-info/SOURCES.txt'\n",
      "  \u001b[31m   \u001b[0m running build_ext\n",
      "  \u001b[31m   \u001b[0m building 'torch_cluster._knn_cpu' extension\n",
      "  \u001b[31m   \u001b[0m creating build/temp.linux-x86_64-3.8\n",
      "  \u001b[31m   \u001b[0m creating build/temp.linux-x86_64-3.8/csrc\n",
      "  \u001b[31m   \u001b[0m creating build/temp.linux-x86_64-3.8/csrc/cpu\n",
      "  \u001b[31m   \u001b[0m gcc -pthread -B /opt/conda/envs/chem38_c20240222/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -DWITH_PYTHON -Icsrc -I/opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages/torch/include -I/opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages/torch/include/TH -I/opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages/torch/include/THC -I/opt/conda/envs/chem38_c20240222/include/python3.8 -c csrc/knn.cpp -o build/temp.linux-x86_64-3.8/csrc/knn.o -O2 -Wno-sign-compare -DAT_PARALLEL_OPENMP -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=_knn_cpu -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
      "  \u001b[31m   \u001b[0m cc1plus: warning: command line option ‘-Wstrict-prototypes’ is valid for C/ObjC but not for C++\n",
      "  \u001b[31m   \u001b[0m gcc -pthread -B /opt/conda/envs/chem38_c20240222/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -DWITH_PYTHON -Icsrc -I/opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages/torch/include -I/opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages/torch/include/TH -I/opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages/torch/include/THC -I/opt/conda/envs/chem38_c20240222/include/python3.8 -c csrc/cpu/knn_cpu.cpp -o build/temp.linux-x86_64-3.8/csrc/cpu/knn_cpu.o -O2 -Wno-sign-compare -DAT_PARALLEL_OPENMP -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=_knn_cpu -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
      "  \u001b[31m   \u001b[0m cc1plus: warning: command line option ‘-Wstrict-prototypes’ is valid for C/ObjC but not for C++\n",
      "  \u001b[31m   \u001b[0m g++ -pthread -shared -B /opt/conda/envs/chem38_c20240222/compiler_compat -L/opt/conda/envs/chem38_c20240222/lib -Wl,-rpath=/opt/conda/envs/chem38_c20240222/lib -Wl,--no-as-needed -Wl,--sysroot=/ build/temp.linux-x86_64-3.8/csrc/knn.o build/temp.linux-x86_64-3.8/csrc/cpu/knn_cpu.o -L/opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages/torch/lib -lc10 -ltorch -ltorch_cpu -ltorch_python -o build/lib.linux-x86_64-3.8/torch_cluster/_knn_cpu.so -s\n",
      "  \u001b[31m   \u001b[0m building 'torch_cluster._knn_cuda' extension\n",
      "  \u001b[31m   \u001b[0m creating build/temp.linux-x86_64-3.8/csrc/cuda\n",
      "  \u001b[31m   \u001b[0m gcc -pthread -B /opt/conda/envs/chem38_c20240222/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -DWITH_PYTHON -DWITH_CUDA -Icsrc -I/opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages/torch/include -I/opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages/torch/include/TH -I/opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/envs/chem38_c20240222/include/python3.8 -c csrc/knn.cpp -o build/temp.linux-x86_64-3.8/csrc/knn.o -O2 -Wno-sign-compare -DAT_PARALLEL_OPENMP -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=_knn_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
      "  \u001b[31m   \u001b[0m cc1plus: warning: command line option ‘-Wstrict-prototypes’ is valid for C/ObjC but not for C++\n",
      "  \u001b[31m   \u001b[0m gcc -pthread -B /opt/conda/envs/chem38_c20240222/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -DWITH_PYTHON -DWITH_CUDA -Icsrc -I/opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages/torch/include -I/opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages/torch/include/TH -I/opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/envs/chem38_c20240222/include/python3.8 -c csrc/cpu/knn_cpu.cpp -o build/temp.linux-x86_64-3.8/csrc/cpu/knn_cpu.o -O2 -Wno-sign-compare -DAT_PARALLEL_OPENMP -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=_knn_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
      "  \u001b[31m   \u001b[0m cc1plus: warning: command line option ‘-Wstrict-prototypes’ is valid for C/ObjC but not for C++\n",
      "  \u001b[31m   \u001b[0m /usr/local/cuda/bin/nvcc -DWITH_PYTHON -DWITH_CUDA -Icsrc -I/opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages/torch/include -I/opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages/torch/include/TH -I/opt/conda/envs/chem38_c20240222/lib/python3.8/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/envs/chem38_c20240222/include/python3.8 -c csrc/cuda/knn_cuda.cu -o build/temp.linux-x86_64-3.8/csrc/cuda/knn_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O2 --expt-relaxed-constexpr -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=_knn_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_86,code=sm_86 -std=c++14\n",
      "  \u001b[31m   \u001b[0m unable to execute '/usr/local/cuda/bin/nvcc': No such file or directory\n",
      "  \u001b[31m   \u001b[0m error: command '/usr/local/cuda/bin/nvcc' failed with exit status 1\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[31m  ERROR: Failed building wheel for torch-cluster\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[?25h  Running setup.py clean for torch-cluster\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to build torch-scatter torch-sparse torch-cluster\r\n",
      "\u001b[31mERROR: Could not build wheels for torch-scatter, torch-sparse, torch-cluster, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c377884",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch-lightning-bolts\n",
      "  Downloading pytorch_lightning_bolts-0.3.2.post1-py3-none-any.whl (252 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m252.7/252.7 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pytorch-lightning>=1.1.1 in /opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages (from pytorch-lightning-bolts) (1.5.6)\n",
      "Requirement already satisfied: torch>=1.6 in /opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages (from pytorch-lightning-bolts) (1.8.2)\n",
      "Requirement already satisfied: torchmetrics>=0.2.0 in /opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages (from pytorch-lightning-bolts) (0.7.3)\n",
      "Requirement already satisfied: numpy>=1.17.2 in /opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages (from pytorch-lightning>=1.1.1->pytorch-lightning-bolts) (1.23.3)\n",
      "Requirement already satisfied: future>=0.17.1 in /opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages (from pytorch-lightning>=1.1.1->pytorch-lightning-bolts) (0.18.2)\n",
      "Requirement already satisfied: tqdm>=4.41.0 in /opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages (from pytorch-lightning>=1.1.1->pytorch-lightning-bolts) (4.63.0)\n",
      "Requirement already satisfied: PyYAML>=5.1 in /opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages (from pytorch-lightning>=1.1.1->pytorch-lightning-bolts) (5.4.1)\n",
      "Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages (from pytorch-lightning>=1.1.1->pytorch-lightning-bolts) (2023.6.0)\n",
      "Requirement already satisfied: tensorboard>=2.2.0 in /opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages (from pytorch-lightning>=1.1.1->pytorch-lightning-bolts) (2.2.0)\n",
      "Requirement already satisfied: pyDeprecate==0.3.1 in /opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages (from pytorch-lightning>=1.1.1->pytorch-lightning-bolts) (0.3.1)\n",
      "Requirement already satisfied: packaging>=17.0 in /opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages (from pytorch-lightning>=1.1.1->pytorch-lightning-bolts) (23.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages (from pytorch-lightning>=1.1.1->pytorch-lightning-bolts) (4.6.3)\n",
      "Requirement already satisfied: requests in /opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning>=1.1.1->pytorch-lightning-bolts) (2.28.1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning>=1.1.1->pytorch-lightning-bolts) (3.8.1)\n",
      "Requirement already satisfied: absl-py>=0.4 in /opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.1.1->pytorch-lightning-bolts) (1.4.0)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.1.1->pytorch-lightning-bolts) (1.43.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.1.1->pytorch-lightning-bolts) (1.35.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.1.1->pytorch-lightning-bolts) (0.4.6)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.1.1->pytorch-lightning-bolts) (3.4.3)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.1.1->pytorch-lightning-bolts) (3.20.3)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.1.1->pytorch-lightning-bolts) (59.5.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.1.1->pytorch-lightning-bolts) (1.16.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.1.1->pytorch-lightning-bolts) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.1.1->pytorch-lightning-bolts) (2.2.2)\n",
      "Requirement already satisfied: wheel>=0.26 in /opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.1.1->pytorch-lightning-bolts) (0.38.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning>=1.1.1->pytorch-lightning-bolts) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning>=1.1.1->pytorch-lightning-bolts) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning>=1.1.1->pytorch-lightning-bolts) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning>=1.1.1->pytorch-lightning-bolts) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning>=1.1.1->pytorch-lightning-bolts) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning>=1.1.1->pytorch-lightning-bolts) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning>=1.1.1->pytorch-lightning-bolts) (1.3.1)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning>=1.1.1->pytorch-lightning-bolts) (4.2.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning>=1.1.1->pytorch-lightning-bolts) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning>=1.1.1->pytorch-lightning-bolts) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning>=1.1.1->pytorch-lightning-bolts) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning>=1.1.1->pytorch-lightning-bolts) (6.0.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning>=1.1.1->pytorch-lightning-bolts) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning>=1.1.1->pytorch-lightning-bolts) (1.26.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning>=1.1.1->pytorch-lightning-bolts) (2023.5.7)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages (from werkzeug>=0.11.15->tensorboard>=2.2.0->pytorch-lightning>=1.1.1->pytorch-lightning-bolts) (2.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning>=1.1.1->pytorch-lightning-bolts) (3.11.0)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning>=1.1.1->pytorch-lightning-bolts) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning>=1.1.1->pytorch-lightning-bolts) (3.2.2)\n",
      "Installing collected packages: pytorch-lightning-bolts\n",
      "\u001b[31mERROR: Could not install packages due to an OSError: [Errno 13] Permission denied: '/opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages/tests/callbacks'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\u001b[0m\u001b[31m\n",
      "\u001b[0mRequirement already satisfied: transformers in /opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages (4.6.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages (from transformers) (3.12.2)\n",
      "Requirement already satisfied: huggingface-hub==0.0.8 in /opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages (from transformers) (0.0.8)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages (from transformers) (1.23.3)\n",
      "Requirement already satisfied: packaging in /opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages (from transformers) (2023.6.3)\n",
      "Requirement already satisfied: requests in /opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: sacremoses in /opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages (from transformers) (0.0.53)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages (from transformers) (0.10.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages (from transformers) (4.63.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages (from requests->transformers) (1.26.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages (from requests->transformers) (2023.5.7)\n",
      "Requirement already satisfied: six in /opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages (from sacremoses->transformers) (1.16.0)\n",
      "Requirement already satisfied: click in /opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages (from sacremoses->transformers) (8.0.4)\n",
      "Requirement already satisfied: joblib in /opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages (from sacremoses->transformers) (1.2.0)\n",
      "Collecting lightning-bolts\n",
      "  Downloading lightning_bolts-0.7.0-py3-none-any.whl (300 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.8/300.8 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in /opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages (from lightning-bolts) (1.23.3)\n",
      "Collecting pytorch-lightning<2.0.0,>1.7.0 (from lightning-bolts)\n",
      "  Downloading pytorch_lightning-1.9.5-py3-none-any.whl (829 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m829.5/829.5 kB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torchmetrics in /opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages (from lightning-bolts) (0.7.3)\n",
      "Collecting lightning-utilities>0.3.1 (from lightning-bolts)\n",
      "  Downloading lightning_utilities-0.11.2-py3-none-any.whl (26 kB)\n",
      "Collecting torchvision>=0.10.0 (from lightning-bolts)\n",
      "  Downloading torchvision-0.18.0-cp38-cp38-manylinux1_x86_64.whl (7.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m69.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting tensorboard>=2.9.1 (from lightning-bolts)\n",
      "  Downloading tensorboard-2.14.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m59.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=17.1 in /opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages (from lightning-utilities>0.3.1->lightning-bolts) (23.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages (from lightning-utilities>0.3.1->lightning-bolts) (59.5.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages (from lightning-utilities>0.3.1->lightning-bolts) (4.6.3)\n",
      "Collecting torch>=1.10.0 (from pytorch-lightning<2.0.0,>1.7.0->lightning-bolts)\n",
      "  Downloading torch-2.3.0-cp38-cp38-manylinux1_x86_64.whl (779.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m779.1/779.1 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.57.0 in /opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages (from pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (4.63.0)\n",
      "Requirement already satisfied: PyYAML>=5.4 in /opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages (from pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (5.4.1)\n",
      "Requirement already satisfied: fsspec[http]>2021.06.0 in /opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages (from pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (2023.6.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages (from tensorboard>=2.9.1->lightning-bolts) (1.4.0)\n",
      "Collecting grpcio>=1.48.2 (from tensorboard>=2.9.1->lightning-bolts)\n",
      "  Downloading grpcio-1.63.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages (from tensorboard>=2.9.1->lightning-bolts) (1.35.0)\n",
      "Collecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard>=2.9.1->lightning-bolts)\n",
      "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages (from tensorboard>=2.9.1->lightning-bolts) (3.4.3)\n",
      "Requirement already satisfied: protobuf>=3.19.6 in /opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages (from tensorboard>=2.9.1->lightning-bolts) (3.20.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages (from tensorboard>=2.9.1->lightning-bolts) (2.28.1)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard>=2.9.1->lightning-bolts)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages (from tensorboard>=2.9.1->lightning-bolts) (2.2.2)\n",
      "Requirement already satisfied: wheel>=0.26 in /opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages (from tensorboard>=2.9.1->lightning-bolts) (0.38.4)\n",
      "Requirement already satisfied: pyDeprecate==0.3.* in /opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages (from torchmetrics->lightning-bolts) (0.3.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages (from torchvision>=0.10.0->lightning-bolts) (9.3.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: filelock in /opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages (from torch>=1.10.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (3.12.2)\n",
      "Collecting typing-extensions (from lightning-utilities>0.3.1->lightning-bolts)\n",
      "  Downloading typing_extensions-4.11.0-py3-none-any.whl (34 kB)\n",
      "Collecting sympy (from torch>=1.10.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts)\n",
      "  Downloading sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: networkx in /opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages (from torch>=1.10.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages (from torch>=1.10.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (3.1.2)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.10.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.10.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.10.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.10.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts)\n",
      "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.10.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts)\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.10.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts)\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.10.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts)\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.10.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts)\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.10.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts)\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nccl-cu12==2.20.5 (from torch>=1.10.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts)\n",
      "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.10.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts)\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting triton==2.3.0 (from torch>=1.10.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts)\n",
      "  Downloading triton-2.3.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.0/168.0 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages (from fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (3.8.1)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->lightning-bolts) (4.2.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->lightning-bolts) (0.3.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->lightning-bolts) (1.16.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->lightning-bolts) (4.9)\n",
      "Collecting google-auth<3,>=1.6.3 (from tensorboard>=2.9.1->lightning-bolts)\n",
      "  Downloading google_auth-2.29.0-py2.py3-none-any.whl (189 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m189.2/189.2 kB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard>=2.9.1->lightning-bolts) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard>=2.9.1->lightning-bolts) (6.0.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard>=2.9.1->lightning-bolts) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard>=2.9.1->lightning-bolts) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard>=2.9.1->lightning-bolts) (1.26.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard>=2.9.1->lightning-bolts) (2023.5.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->lightning-bolts) (2.1.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (1.3.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.9.1->lightning-bolts) (3.11.0)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.9.1->lightning-bolts) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard>=2.9.1->lightning-bolts) (3.2.2)\n",
      "Collecting mpmath>=0.19 (from sympy->torch>=1.10.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m52.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: mpmath, typing-extensions, triton, tensorboard-data-server, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, grpcio, nvidia-cusparse-cu12, nvidia-cudnn-cu12, lightning-utilities, google-auth, nvidia-cusolver-cu12, google-auth-oauthlib, torch, tensorboard, torchvision, pytorch-lightning, lightning-bolts\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.6.3\n",
      "    Uninstalling typing_extensions-4.6.3:\n",
      "      Successfully uninstalled typing_extensions-4.6.3\n",
      "  Attempting uninstall: grpcio\n",
      "    Found existing installation: grpcio 1.43.0\n",
      "    Uninstalling grpcio-1.43.0:\n",
      "      Successfully uninstalled grpcio-1.43.0\n",
      "  Attempting uninstall: google-auth\n",
      "    Found existing installation: google-auth 1.35.0\n",
      "    Uninstalling google-auth-1.35.0:\n",
      "      Successfully uninstalled google-auth-1.35.0\n",
      "  Attempting uninstall: google-auth-oauthlib\n",
      "    Found existing installation: google-auth-oauthlib 0.4.6\n",
      "    Uninstalling google-auth-oauthlib-0.4.6:\n",
      "      Successfully uninstalled google-auth-oauthlib-0.4.6\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.8.2\n",
      "    Uninstalling torch-1.8.2:\n",
      "      Successfully uninstalled torch-1.8.2\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.2.0\n",
      "    Uninstalling tensorboard-2.2.0:\n",
      "      Successfully uninstalled tensorboard-2.2.0\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.9.2\n",
      "    Uninstalling torchvision-0.9.2:\n",
      "      Successfully uninstalled torchvision-0.9.2\n",
      "  Attempting uninstall: pytorch-lightning\n",
      "    Found existing installation: pytorch-lightning 1.5.6\n",
      "    Uninstalling pytorch-lightning-1.5.6:\n",
      "      Successfully uninstalled pytorch-lightning-1.5.6\n",
      "Successfully installed google-auth-2.29.0 google-auth-oauthlib-1.0.0 grpcio-1.63.0 lightning-bolts-0.7.0 lightning-utilities-0.11.2 mpmath-1.3.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 pytorch-lightning-1.9.5 sympy-1.12 tensorboard-2.14.0 tensorboard-data-server-0.7.2 torch-2.3.0 torchvision-0.18.0 triton-2.3.0 typing-extensions-4.11.0\n",
      "Requirement already satisfied: lightning-utilities in /opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages (0.11.2)\n",
      "Requirement already satisfied: packaging>=17.1 in /opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages (from lightning-utilities) (23.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages (from lightning-utilities) (59.5.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages (from lightning-utilities) (4.11.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pytorch-lightning\n",
    "!pip install transformers\n",
    "!pip install lightning-bolts\n",
    "!pip install lightning-utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45edc818",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch_scatter'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mearly_stopping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EarlyStopping, generate_checkpoint_filename\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_3d\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Model3D\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels_3d\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchiro\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChIRo\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels_3d\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpainn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PaiNN\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels_3d\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mschnet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SchNet\n",
      "File \u001b[0;32m/mnt/code/benchmarks/models/models_3d/chiro.py:6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch_geometric\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch_scatter\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmath\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mMLP\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch_scatter'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from dataclasses import asdict\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "# from pl_bolts.optimizers import LinearWarmupCosineAnnealingLR\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingLR\n",
    "\n",
    "from config import Config\n",
    "from data.ee import EE\n",
    "from data.bde import BDE\n",
    "from data.drugs import Drugs\n",
    "from data.kraken import Kraken\n",
    "from happy_config import ConfigLoader\n",
    "from loaders.samplers import EnsembleSampler, EnsembleMultiBatchSampler\n",
    "from loaders.multibatch import MultiBatchLoader\n",
    "from utils.early_stopping import EarlyStopping, generate_checkpoint_filename\n",
    "\n",
    "from models.model_3d import Model3D\n",
    "from models.models_3d.chiro import ChIRo\n",
    "from models.models_3d.painn import PaiNN\n",
    "from models.models_3d.schnet import SchNet\n",
    "from models.models_3d.gemnet import GemNetT\n",
    "from models.models_3d.dimenet import DimeNet, DimeNetPlusPlus\n",
    "from models.models_3d.clofnet import ClofNet\n",
    "from models.models_3d.leftnet import LEFTNet\n",
    "#from models.models_3d.chytorch_discrete import ChytorchDiscrete\n",
    "#from models.models_3d.chytorch_conformer import ChytorchConformer\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9b09137",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from pytorch_lightning.loggers import CSVLogger, TensorBoardLogger\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import LightningModule, Trainer, LightningDataModule\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CyclicLR, CosineAnnealingLR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e757818c",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################\n",
    "import os\n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = gpus\n",
    "torch.cuda.empty_cache()\n",
    "#print(os.environ['CUDA_VISIBLE_DEVICES'])\n",
    "\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# If you are using a GPU, you should also set the seed for CUDA operations\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "#np.random.seed(seed)\n",
    "################################################## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee749ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Molecules(Dataset):\n",
    "    def __init__(self, smiles_ids, attention_masks, labels, fingerprint=None, input_type='smiles'):\n",
    "        self.smiles_ids = smiles_ids\n",
    "        self.attention_masks = attention_masks\n",
    "        self.labels = labels\n",
    "        self.fingerprint = fingerprint\n",
    "        self.input_type = input_type\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.labels.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.input_type == 'SMILES':\n",
    "            smiles = self.smiles_ids[index]\n",
    "            attention_mask = self.attention_masks[index]\n",
    "            y = self.labels[index]\n",
    "            return smiles, attention_mask, y.clone()\n",
    "        else:\n",
    "            fingerprint = self.fingerprint[index]\n",
    "            y = self.labels[index]\n",
    "            return torch.tensor(fingerprint, dtype=torch.long), y.clone()\n",
    "\n",
    "def r2_score_torch(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute the R-squared score.\n",
    "\n",
    "    Parameters:\n",
    "        y_true (torch.Tensor): The true target values.\n",
    "        y_pred (torch.Tensor): The predicted target values.\n",
    "\n",
    "    Returns:\n",
    "        float: The R-squared score.\n",
    "    \"\"\"\n",
    "    # Calculate the mean of the true target values\n",
    "    y_mean = torch.mean(y_true)\n",
    "    # Calculate the total sum of squares (TSS)\n",
    "    tss = torch.sum((y_true - y_mean) ** 2)\n",
    "    # Calculate the residual sum of squares (RSS)\n",
    "    rss = torch.sum((y_true - y_pred) ** 2)\n",
    "    # Calculate R-squared score\n",
    "    r2 = 1 - rss / tss\n",
    "\n",
    "    return r2.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08ff1411",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataModule(LightningDataModule):\n",
    "    def __init__(self, hparams, dataset=None):\n",
    "        super().__init__()\n",
    "        self.hparams.update(hparams.__dict__) if hasattr(hparams, \"__dict__\") else self.hparams.update(hparams)\n",
    "        self._saved_dataloaders = dict()\n",
    "        self.dataset = dataset\n",
    "        \n",
    "        if self.dataset is None:\n",
    "            variable_name = None\n",
    "            unique_variables = 1\n",
    "            \n",
    "            if self.hparams.dataset == 'Drugs':\n",
    "                dataset = Drugs('/mnt/data/MARCEL/datasets/Drugs', max_num_conformers=self.hparams.max_num_conformers).shuffle()\n",
    "            elif self.hparams.dataset == 'Kraken':\n",
    "                dataset = Kraken('/mnt/data/MARCEL/datasets/Kraken', max_num_conformers=self.hparams.max_num_conformers).shuffle()\n",
    "            elif self.hparams.dataset == 'BDE':\n",
    "                dataset = BDE('/mnt/data/MARCEL/datasets/BDE').shuffle()\n",
    "                variable_name = 'is_ligand'\n",
    "                unique_variables = 2\n",
    "            elif self.hparams.dataset == 'EE':\n",
    "                dataset = EE('/mnt/data/MARCEL/datasets/EE', max_num_conformers=self.hparams.max_num_conformers).shuffle()\n",
    "                variable_name = 'config_id'\n",
    "                unique_variables = 2\n",
    "            \n",
    "            #autoscaling\n",
    "            target_id = dataset.descriptors.index(self.hparams.target)\n",
    "            labels = dataset.y[:, target_id]\n",
    "            mean = labels.mean(dim=0).item()\n",
    "            std = labels.std(dim=0).item()\n",
    "            labels = (labels - mean) / std\n",
    "            \n",
    "            if variable_name is not None:\n",
    "                smiles = concatenate_smiles(dataset, variable_name)\n",
    "            else:\n",
    "                smiles = construct_smiles(dataset)\n",
    "            fingerprint = construct_fingerprint(smiles) if self.hparams.model1d.input_type == 'Fingerprint' else None\n",
    "\n",
    "            tokenizer = RobertaTokenizer.from_pretrained('seyonec/PubChem10M_SMILES_BPE_450k')\n",
    "            dicts = tokenizer(smiles, return_tensors='pt', padding='longest')\n",
    "            smiles_ids, attention_masks = dicts['input_ids'], dicts['attention_mask']\n",
    "            vocab_size = tokenizer.vocab_size if self.hparams.model1d.input_type == 'SMILES' else fingerprint.shape[1]\n",
    "\n",
    "            dataset = Molecules(smiles_ids, attention_masks, labels, fingerprint, input_type=self.hparams.model1d.input_type)\n",
    "\n",
    "            self.dataset = dataset\n",
    "            self.vocab_size=vocab_size\n",
    "            self.tokenizer=tokenizer\n",
    "            self.smiles_ids=smiles_ids\n",
    "            #modelnet = model.to(device)\n",
    "\n",
    "            print('--done---')\n",
    "\n",
    "    def split_compute(self):\n",
    "        train_ratio = self.hparams.train_ratio\n",
    "        valid_ratio = self.hparams.valid_ratio\n",
    "        test_ratio = 1 - train_ratio - valid_ratio\n",
    "\n",
    "        train_len = int(train_ratio * len(self.dataset))\n",
    "        valid_len = int(valid_ratio * len(self.dataset))\n",
    "        test_len = len(self.dataset) - train_len - valid_len\n",
    "\n",
    "        self.train_dataset, self.valid_dataset, self.test_dataset = random_split(self.dataset, lengths=[train_len, valid_len, test_len])\n",
    "        print(f'{len(self.train_dataset)} training data, {len(self.test_dataset)} test data and {len(self.valid_dataset)} validation data')\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return self._get_dataloader(self.train_dataset, \"train\")\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self._get_dataloader(self.valid_dataset, \"val\")\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return self._get_dataloader(self.test_dataset, \"test\")\n",
    "    \n",
    "    def _get_dataloader(self, dataset, stage, store_dataloader=True):\n",
    "        store_dataloader = store_dataloader\n",
    "        \n",
    "        if stage in self._saved_dataloaders and store_dataloader:\n",
    "            return self._saved_dataloaders[stage]\n",
    "\n",
    "        if stage == \"train\":\n",
    "            dl = DataLoader(dataset=dataset, batch_size=self.hparams.batch_size, shuffle=True)                                  \n",
    "        else:\n",
    "            dl = DataLoader(dataset=dataset, batch_size=self.hparams.batch_size, shuffle=False) \n",
    "\n",
    "        if store_dataloader:\n",
    "            self._saved_dataloaders[stage] = dl\n",
    "        return dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5796c8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def compute_pnorm(model: nn.Module) -> float:\n",
    "    \"\"\"\n",
    "    Computes the norm of the parameters of a model.\n",
    "    :param model: A PyTorch model.\n",
    "    :return: The norm of the parameters of the model.\n",
    "    \"\"\"\n",
    "    return math.sqrt(sum([p.norm().item() ** 2 for p in model.parameters() if p.requires_grad]))\n",
    "\n",
    "\n",
    "def compute_gnorm(model: nn.Module) -> float:\n",
    "    \"\"\"\n",
    "    Computes the norm of the gradients of a model.\n",
    "    :param model: A PyTorch model.\n",
    "    :return: The norm of the gradients of the model.\n",
    "    \"\"\"\n",
    "    return math.sqrt(sum([p.grad.norm().item() ** 2 for p in model.parameters() if p.grad is not None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fc1085bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model1D(LightningModule):\n",
    "    def __init__(self, vocab_size=None, tokenizer=None, smiles_ids=None, **hparams):\n",
    "        super().__init__()\n",
    "        #self.hparams.update(hparams.__dict__) if hasattr(hparams, \"__dict__\") else self.hparams.update(hparams)\n",
    "        \n",
    "        if hparams.get('model1d_model') == 'LSTM':\n",
    "            self.net = LSTM(\n",
    "                vocab_size, hparams.get('hidden_dim'), hparams.get('hidden_dim'), 1,\n",
    "                hparams.get('model1d_num_layers'), hparams.get('dropout'), padding_idx=tokenizer.pad_token_id)\n",
    "        elif hparams.get('model1d_model') == 'Transformer':\n",
    "            self.net = Transformer(\n",
    "                vocab_size, hparams.get('model1d_embedding_dim'), smiles_ids.shape[1],\n",
    "                hparams.get('model1d_num_heads'), hparams.get('hidden_dim'), 1,\n",
    "                hparams.get('model1d_num_layers'), hparams.get('dropout'), padding_idx=tokenizer.pad_token_id)\n",
    "                \n",
    "        self.loss_fn = nn.MSELoss() #LOGITS #GroupedScaledMAELoss(torch.ones(4, dtype=torch.long))\n",
    "        self.device_= torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.input_type = hparams.get('model1d_input_type')\n",
    "        self.lr = hparams.get('learning_rate')\n",
    "        self.wd = hparams.get('weight_decay')\n",
    "\n",
    "        self._reset_losses_dict()\n",
    "        self._reset_inference_results()\n",
    "        self.save_hyperparameters(hparams)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        if self.input_type == 'SMILES':\n",
    "            input_ids, attention_mask, y = batch\n",
    "            if isinstance(self.net, Transformer):\n",
    "                out = self.net(input_ids, attention_mask)\n",
    "            else:\n",
    "                out = self.net(input_ids)\n",
    "        else:\n",
    "            fingerprints, y = batch\n",
    "            out = self.net(fingerprints)\n",
    "        return out, y\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        o = AdamW(self.parameters(), lr=self.lr, weight_decay=self.wd)\n",
    "        s = CyclicLR(o, self.lr, 2e-4, 1000, mode='triangular', cycle_momentum=False)\n",
    "        # instantiate the WeakMethod in the lr scheduler object into the custom scale function attribute\n",
    "        #s._scale_fn_custom = s._scale_fn_ref()\n",
    "        # remove the reference so there are no more WeakMethod references in the object\n",
    "        #s._scale_fn_ref = None\n",
    "        return [o], [{'scheduler': s, 'interval': 'step', 'name': 'lr_scheduler'}]\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        pnorm = compute_pnorm(self.net)\n",
    "        gnorm = compute_gnorm(self.net)\n",
    "        self.log(f'(training) pnorm', pnorm)\n",
    "        self.log(f'(training) gnorm', gnorm)\n",
    "        return self.step(batch, \"train\")\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self.step(batch, \"val\")\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        return self.step(batch, \"test\")\n",
    "        \n",
    "    def step(self, batch, stage):\n",
    "        start = time()\n",
    "\n",
    "        with torch.set_grad_enabled(stage == \"train\"):\n",
    "            pred, targets = self(batch)\n",
    "            loss = self.loss_fn(pred.squeeze(), targets)\n",
    "            \n",
    "            if stage == \"test\":\n",
    "                self.inference_results['y_pred'].append(pred.squeeze())\n",
    "                self.inference_results['y_true'].append(targets.squeeze())\n",
    "                return None\n",
    "\n",
    "            r2=r2_score_torch(targets.cpu(),pred.squeeze().cpu().detach())\n",
    "\n",
    "            self.logging_info[f'{stage}_loss'].append(loss.item())\n",
    "            self.logging_info[f'{stage}_r2'].append(r2)\n",
    "            self.logging_info[f'{stage}_time'].append(time()-start)\n",
    "            \n",
    "            if stage == 'train':\n",
    "                self.log(\"lr\", self.trainer.optimizers[0].param_groups[0][\"lr\"], on_step=True, on_epoch=False, prog_bar=True, logger=True, sync_dist=True)\n",
    "                self.log(f'{stage}_step_loss', loss.item(), on_step=True, on_epoch=False, prog_bar=True, logger=True, sync_dist=True)\n",
    "\n",
    "            return loss\n",
    "        \n",
    "    def on_validation_epoch_end(self):\n",
    "        if not self.trainer.sanity_checking:                    \n",
    "            result_dict = {\n",
    "                \"epoch\": float(self.current_epoch),\n",
    "                \"train_epoch_loss\": torch.tensor(self.logging_info[\"train_loss\"]).mean().item(),\n",
    "                \"train_epoch_r2\": torch.tensor(self.logging_info[\"train_r2\"]).mean().item(),\n",
    "                \"val_epoch_loss\": torch.tensor(self.logging_info[\"val_loss\"]).mean().item(),\n",
    "                \"val_epoch_r2\": torch.tensor(self.logging_info[\"val_r2\"]).mean().item(),\n",
    "                \"train_epoch_time\": sum(self.logging_info[\"train_time\"]),\n",
    "                \"val_epoch_time\": sum(self.logging_info[\"val_time\"]),\n",
    "                }\n",
    "            self.log_dict(result_dict, logger=True, sync_dist=True)\n",
    "            \n",
    "        self._reset_losses_dict()\n",
    "    \n",
    "    def on_test_epoch_end(self) -> None:\n",
    "        for key in self.inference_results.keys():\n",
    "            self.inference_results[key] = torch.cat(self.inference_results[key], dim=0)\n",
    "    \n",
    "    def _reset_losses_dict(self):\n",
    "        self.logging_info = {\n",
    "            \"train_loss\": [],\n",
    "            \"train_r2\": [], \n",
    "            \"train_mse\": [], \n",
    "            \"val_loss\": [],\n",
    "            \"val_r2\": [],\n",
    "            \"train_sample_size\": [], \n",
    "            \"val_sample_size\": [],\n",
    "            \"train_time\": [],\n",
    "            \"val_time\": [],\n",
    "        }\n",
    "        \n",
    "    def _reset_inference_results(self):\n",
    "        self.inference_results = {'y_pred': [],\n",
    "                                  'y_true': []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c7d3e8c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataname = 'BDE'\n",
    "target = 'BindingEnergy'\n",
    "input_type = 'SMILES'\n",
    "modeltype = 'LSTM'\n",
    "\n",
    "writer = SummaryWriter(log_dir='/mnt/code/logs/')\n",
    "#loader = ConfigLoader(model=Config, config='params/params_1d.json')\n",
    "#config = loader()\n",
    "\n",
    "config = Config\n",
    "config.dataset = dataname\n",
    "config.target = target\n",
    "config.device = 'cuda:0'\n",
    "\n",
    "#config.max_num_conformers = 20\n",
    "config.model1d.input_type = input_type\n",
    "config.model1d.model = modeltype\n",
    "#config.model1d.num_layers = 4\n",
    "\n",
    "#Specific for Transformer\n",
    "#config.model1d.embedding_dim = 128\n",
    "#config.model1d.num_heads = 4\n",
    "\n",
    "#config.hidden_dim =128\n",
    "#config.dropout = 0.5\n",
    "\n",
    "#config.train_ratio = 0.7\n",
    "#config.valid_ratio = 0.1\n",
    "#config.batch_size = 256\n",
    "#config.patience = 200\n",
    "\n",
    "#config.learning_rate = 0.00001\n",
    "#config.weight_decay = 1e-4\n",
    "#config.scheduler = None\n",
    "#config.reduce_lr_on_plateau = ReduceLROnPlateau()\n",
    "\n",
    "#config.num_epochs = 2000\n",
    "#config.cosine_annealing_lr = CosineAnnealingLR()\n",
    "#config.linear_warmup_cosine_annealing_lr = LinearWarmupCosineAnnealingLR()\n",
    "\n",
    "\n",
    "#not used in 1d\n",
    "\n",
    "#config.one_cycle_lr = OneCycleLR()\n",
    "#config.seed = 123\n",
    "#activation = 'relu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4cb75383",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/domino/.local/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n",
      "/home/domino/.local/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The given 'InMemoryDataset' only references a subset of examples of the full dataset, but 'data' will contain information of the full dataset. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n",
      "/opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0060405731201171875,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "tokenizer_config.json",
       "rate": null,
       "total": 62,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "362b32f2e7cd4c16adfde9b7a24f0732",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/62.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0036287307739257812,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "vocab.json",
       "rate": null,
       "total": 164540,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1c44a7ab804498a85b65bc738b9960f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/165k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0037529468536376953,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "merges.txt",
       "rate": null,
       "total": 101307,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63060cc411624a209ef23f6a496cddb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/101k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.003670930862426758,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "special_tokens_map.json",
       "rate": null,
       "total": 772,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b0078018e874315b4102fa6e063fefd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/772 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.003833770751953125,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "config.json",
       "rate": null,
       "total": 515,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "283d8e307d8047feaa319276a63f3a2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/515 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--done---\n",
      "4140 training data, 1184 test data and 591 validation data\n"
     ]
    }
   ],
   "source": [
    "data = DataModule(config)\n",
    "data.prepare_data()\n",
    "data.split_compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b7969c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def config_to_dict(config_class):\n",
    "    config_dict = {}\n",
    "    for attr_name in dir(config_class):\n",
    "        if not attr_name.startswith(\"__\") and not callable(getattr(config_class, attr_name)):\n",
    "            if attr_name=='model1d':\n",
    "                attr_val = getattr(config_class, attr_name)\n",
    "                for subattr, subvalue in zip(['model', 'input_type', 'embedding_dim', 'num_layers', 'num_heads'],\n",
    "                                             [attr_val.model, attr_val.input_type, attr_val.embedding_dim, \n",
    "                                              attr_val.num_layers, attr_val.num_heads]):\n",
    "                    config_dict[f'{attr_name}_{subattr}'] = subvalue\n",
    "            elif attr_name in ['model2d','model3d','model4d','modelfprf',\n",
    "                               'linear_warmup_cosine_annealing_lr','cosine_annealing_lr',\n",
    "                               'reduce_lr_on_plateau','one_cycle_lr']:\n",
    "                pass\n",
    "            else:\n",
    "                config_dict[attr_name] = getattr(config_class, attr_name)\n",
    "    return config_dict\n",
    "config_dict = config_to_dict(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4b06f2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model1D(model='LSTM', input_type='SMILES', embedding_dim=128, num_layers=4, num_heads=4)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getattr(config, 'model1d','model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "51c40b9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activation': 'relu',\n",
       " 'batch_size': 256,\n",
       " 'dataset': 'BDE',\n",
       " 'device': 'cuda:0',\n",
       " 'dropout': 0.5,\n",
       " 'hidden_dim': 128,\n",
       " 'learning_rate': 0.001,\n",
       " 'max_num_conformers': 20,\n",
       " 'max_num_molecules': None,\n",
       " 'model1d_model': 'LSTM',\n",
       " 'model1d_input_type': 'SMILES',\n",
       " 'model1d_embedding_dim': 128,\n",
       " 'model1d_num_layers': 4,\n",
       " 'model1d_num_heads': 4,\n",
       " 'num_epochs': 2000,\n",
       " 'patience': 200,\n",
       " 'scheduler': None,\n",
       " 'seed': 123,\n",
       " 'target': 'BindingEnergy',\n",
       " 'train_ratio': 0.7,\n",
       " 'valid_ratio': 0.1,\n",
       " 'weight_decay': 0.0001}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3edb7560",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model1D(vocab_size=data.vocab_size, tokenizer=data.tokenizer, smiles_ids=data.smiles_ids, **config_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "69530b9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#PARAMS = 1542785\n"
     ]
    }
   ],
   "source": [
    "print(f'#PARAMS = {sum(p.numel() for p in model.parameters() if p.requires_grad)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ffad2ebc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model1D(\n",
       "  (net): LSTM(\n",
       "    (embedding): Embedding(7924, 128, padding_idx=1)\n",
       "    (lstm): LSTM(128, 128, num_layers=4, batch_first=True, dropout=0.5)\n",
       "    (fc): Linear(in_features=128, out_features=1, bias=True)\n",
       "  )\n",
       "  (loss_fn): MSELoss()\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2e70bfc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                            Version\r\n",
      "---------------------------------- --------------------\r\n",
      "abcd-helper                        2.0.0\r\n",
      "absl-py                            1.4.0\r\n",
      "aiobotocore                        2.5.0\r\n",
      "aiohttp                            3.8.1\r\n",
      "aiohttp-retry                      2.8.3\r\n",
      "aioitertools                       0.11.0\r\n",
      "aiosignal                          1.3.1\r\n",
      "altair                             4.0.0\r\n",
      "anyio                              3.6.2\r\n",
      "appdirs                            1.4.4\r\n",
      "argon2-cffi                        21.3.0\r\n",
      "argon2-cffi-bindings               21.2.0\r\n",
      "arrow                              1.2.3\r\n",
      "arthor                             3.4.1\r\n",
      "astor                              0.8.1\r\n",
      "astroid                            2.15.6\r\n",
      "astropy                            5.1\r\n",
      "asttokens                          2.2.1\r\n",
      "async-lru                          2.0.3\r\n",
      "async-timeout                      4.0.2\r\n",
      "asyncssh                           2.8.1\r\n",
      "atpublic                           4.0\r\n",
      "attrs                              23.1.0\r\n",
      "Authlib                            1.2.0\r\n",
      "autograd                           1.5\r\n",
      "autograd-gamma                     0.5.0\r\n",
      "automl-app                         1.2.0\r\n",
      "autopep8                           2.0.2\r\n",
      "Babel                              2.12.1\r\n",
      "backcall                           0.2.0\r\n",
      "backoff                            2.2.1\r\n",
      "backports.zoneinfo                 0.2.1\r\n",
      "bcj-cffi                           0.5.1\r\n",
      "bcrypt                             4.0.1\r\n",
      "beautifulsoup4                     4.9.1\r\n",
      "bin2cmpd                           2.12\r\n",
      "bleach                             6.0.0\r\n",
      "blessed                            1.20.0\r\n",
      "blinker                            1.6.2\r\n",
      "bokeh                              2.4.3\r\n",
      "boto3                              1.26.76\r\n",
      "boto3-type-annotations             0.3.1\r\n",
      "botocore                           1.29.76\r\n",
      "Bottleneck                         1.3.5\r\n",
      "Brotli                             1.0.9\r\n",
      "brotlipy                           0.7.0\r\n",
      "bs4                                0.0.1\r\n",
      "CachedMethods                      0.1.4\r\n",
      "cachetools                         4.2.4\r\n",
      "certifi                            2023.5.7\r\n",
      "cffi                               1.15.1\r\n",
      "charset-normalizer                 2.0.4\r\n",
      "chemical-safety-tool               0.0.3\r\n",
      "chylearn                           1.4\r\n",
      "chython                            1.48.1\r\n",
      "chytorch                           1.45\r\n",
      "chytorch-rxnmap                    1.3\r\n",
      "click                              8.0.4\r\n",
      "cloudpickle                        2.2.1\r\n",
      "clustering-leader-follower         1.4\r\n",
      "cmake                              3.29.2\r\n",
      "cmpd2bin                           2.11\r\n",
      "colorama                           0.4.5\r\n",
      "comm                               0.1.3\r\n",
      "configobj                          5.0.8\r\n",
      "contourpy                          1.0.5\r\n",
      "cryptography                       3.4.8\r\n",
      "cx-Oracle                          8.0.0\r\n",
      "cycler                             0.11.0\r\n",
      "Cython                             0.29.17\r\n",
      "daal4py                            2021.5.0\r\n",
      "datanav                            1.1\r\n",
      "debugpy                            1.6.7\r\n",
      "decorator                          5.1.1\r\n",
      "defusedxml                         0.7.1\r\n",
      "detect-delimiter                   0.1.1\r\n",
      "dictdiffer                         0.9.0\r\n",
      "dill                               0.3.6\r\n",
      "diskcache                          5.6.1\r\n",
      "distro                             1.8.0\r\n",
      "dnspython                          2.3.0\r\n",
      "domino-s3-redirect                 1.0\r\n",
      "dominodatalab                      1.1.1\r\n",
      "dpath                              2.1.6\r\n",
      "dramatiq                           1.13.0\r\n",
      "dulwich                            0.21.5\r\n",
      "dvc                                2.9.3\r\n",
      "entrypoints                        0.4\r\n",
      "epam.indigo                        1.5.0\r\n",
      "et-xmlfile                         1.1.0\r\n",
      "exceptiongroup                     1.0.4\r\n",
      "executing                          1.2.0\r\n",
      "fastjsonschema                     2.17.1\r\n",
      "filelock                           3.12.2\r\n",
      "fire                               0.6.0\r\n",
      "flake8                             6.0.0\r\n",
      "Flask                              2.2.2\r\n",
      "Flask-Compress                     1.13\r\n",
      "Flask-Cors                         3.0.10\r\n",
      "Flask-OpenTracing                  1.1.0\r\n",
      "flatten-dict                       0.4.2\r\n",
      "flufl-lock                         8.0.1\r\n",
      "fonttools                          4.25.0\r\n",
      "formulaic                          0.6.4\r\n",
      "fqdn                               1.5.1\r\n",
      "frozenlist                         1.3.3\r\n",
      "fsspec                             2023.6.0\r\n",
      "ftfy                               6.1.1\r\n",
      "funcy                              2.0\r\n",
      "future                             0.18.2\r\n",
      "gcct-utils                         0.4\r\n",
      "gevent                             22.10.2\r\n",
      "gitdb                              4.0.10\r\n",
      "GitPython                          3.1.32\r\n",
      "google-auth                        2.29.0\r\n",
      "google-auth-oauthlib               1.0.0\r\n",
      "gpustat                            1.1\r\n",
      "gql                                3.4.0\r\n",
      "grandalf                           0.6\r\n",
      "graphlib-backport                  1.0.3\r\n",
      "graphql-core                       3.2.3\r\n",
      "greenlet                           2.0.1\r\n",
      "grpcio                             1.63.0\r\n",
      "h11                                0.14.0\r\n",
      "httpcore                           0.16.2\r\n",
      "httpx                              0.23.1\r\n",
      "huggingface-hub                    0.23.0\r\n",
      "hummingbird-ml                     0.4.4\r\n",
      "hvac                               1.1.1\r\n",
      "hyperopt                           0.2.7\r\n",
      "idna                               3.4\r\n",
      "importlib-metadata                 6.0.0\r\n",
      "importlib-resources                5.12.0\r\n",
      "iniconfig                          1.1.1\r\n",
      "interface-meta                     1.3.0\r\n",
      "ipykernel                          6.24.0\r\n",
      "ipython                            8.12.2\r\n",
      "ipython-genutils                   0.2.0\r\n",
      "ipywidgets                         8.0.7\r\n",
      "isaac-python-sdk                   2.0.5b26\r\n",
      "isoduration                        20.11.0\r\n",
      "isort                              5.12.0\r\n",
      "itsdangerous                       2.1.2\r\n",
      "jedi                               0.17.2\r\n",
      "jeepney                            0.8.0\r\n",
      "Jinja2                             3.1.2\r\n",
      "jmespath                           1.0.1\r\n",
      "jnj-auto-ml                        1.0.0\r\n",
      "jnj-transformers                   0.1.7+cadd\r\n",
      "joblib                             1.2.0\r\n",
      "json5                              0.9.14\r\n",
      "jsonargparse                       4.22.0\r\n",
      "jsonify                            0.5\r\n",
      "jsonpointer                        2.4\r\n",
      "jsonschema                         4.17.3\r\n",
      "jupyter                            1.0.0\r\n",
      "jupyter_client                     8.3.0\r\n",
      "jupyter-console                    6.6.3\r\n",
      "jupyter_core                       5.3.1\r\n",
      "jupyter-events                     0.6.3\r\n",
      "jupyter-lsp                        2.2.0\r\n",
      "jupyter_server                     2.7.0\r\n",
      "jupyter-server-proxy               3.2.0\r\n",
      "jupyter_server_terminals           0.4.4\r\n",
      "jupyterlab                         2.3.2\r\n",
      "jupyterlab-pygments                0.2.2\r\n",
      "jupyterlab-server                  1.2.0\r\n",
      "jupyterlab-widgets                 3.0.8\r\n",
      "kaleido                            0.2.1\r\n",
      "keyring                            21.4.0\r\n",
      "keyrings.alt                       4.1.1\r\n",
      "keyrings.cryptfile                 1.3.6\r\n",
      "kiwisolver                         1.4.4\r\n",
      "klepto                             0.2.1\r\n",
      "lazy-object-proxy                  1.6.0\r\n",
      "lazysorted                         0.1.1\r\n",
      "leader-follower                    0.6\r\n",
      "lifelines                          0.27.7\r\n",
      "lightgbm                           3.3.2\r\n",
      "lightning-bolts                    0.7.0\r\n",
      "lightning-lite                     1.8.6\r\n",
      "lightning-utilities                0.11.2\r\n",
      "lit                                18.1.4\r\n",
      "llvmlite                           0.39.1\r\n",
      "lxml                               4.6.3\r\n",
      "lz4                                4.3.2\r\n",
      "mailchecker                        5.0.9\r\n",
      "Markdown                           3.4.3\r\n",
      "markdown-it-py                     2.2.0\r\n",
      "MarkupSafe                         2.1.1\r\n",
      "matplotlib                         3.7.1\r\n",
      "matplotlib-inline                  0.1.6\r\n",
      "mccabe                             0.7.0\r\n",
      "mdurl                              0.1.2\r\n",
      "mistune                            3.0.1\r\n",
      "mkl-fft                            1.3.1\r\n",
      "mkl-random                         1.2.2\r\n",
      "mkl-service                        2.4.0\r\n",
      "mpmath                             1.3.0\r\n",
      "multidict                          6.0.4\r\n",
      "multivolumefile                    0.2.3\r\n",
      "munkres                            1.1.4\r\n",
      "nanotime                           0.5.2\r\n",
      "nbclassic                          1.0.0\r\n",
      "nbclient                           0.8.0\r\n",
      "nbconvert                          7.6.0\r\n",
      "nbformat                           5.9.1\r\n",
      "nest-asyncio                       1.5.6\r\n",
      "networkx                           3.1\r\n",
      "ngboost                            0.3.12\r\n",
      "notebook                           6.5.4\r\n",
      "notebook_shim                      0.2.3\r\n",
      "numba                              0.56.2\r\n",
      "numexpr                            2.8.4\r\n",
      "numpy                              1.23.3\r\n",
      "nvidia-cublas-cu11                 11.10.3.66\r\n",
      "nvidia-cublas-cu12                 12.1.3.1\r\n",
      "nvidia-cuda-cupti-cu11             11.7.101\r\n",
      "nvidia-cuda-cupti-cu12             12.1.105\r\n",
      "nvidia-cuda-nvrtc-cu11             11.7.99\r\n",
      "nvidia-cuda-nvrtc-cu12             12.1.105\r\n",
      "nvidia-cuda-runtime-cu11           11.7.99\r\n",
      "nvidia-cuda-runtime-cu12           12.1.105\r\n",
      "nvidia-cudnn-cu11                  8.5.0.96\r\n",
      "nvidia-cudnn-cu12                  8.9.2.26\r\n",
      "nvidia-cufft-cu11                  10.9.0.58\r\n",
      "nvidia-cufft-cu12                  11.0.2.54\r\n",
      "nvidia-curand-cu11                 10.2.10.91\r\n",
      "nvidia-curand-cu12                 10.3.2.106\r\n",
      "nvidia-cusolver-cu11               11.4.0.1\r\n",
      "nvidia-cusolver-cu12               11.4.5.107\r\n",
      "nvidia-cusparse-cu11               11.7.4.91\r\n",
      "nvidia-cusparse-cu12               12.1.0.106\r\n",
      "nvidia-ml-py                       12.535.77\r\n",
      "nvidia-nccl-cu11                   2.14.3\r\n",
      "nvidia-nccl-cu12                   2.20.5\r\n",
      "nvidia-nvjitlink-cu12              12.4.127\r\n",
      "nvidia-nvtx-cu11                   11.7.91\r\n",
      "nvidia-nvtx-cu12                   12.1.105\r\n",
      "oauthlib                           3.2.2\r\n",
      "onnx                               1.14.0\r\n",
      "onnxconverter-common               1.13.0\r\n",
      "OpenEye-toolkits-python3-linux-x64 2020.2.4\r\n",
      "openpyxl                           3.0.9\r\n",
      "opentracing                        2.4.0\r\n",
      "overrides                          7.3.1\r\n",
      "package-list-difference            2.0\r\n",
      "packaging                          23.0\r\n",
      "pandarallel                        1.5.1\r\n",
      "pandas                             1.3.4\r\n",
      "pandocfilters                      1.5.0\r\n",
      "paramiko                           2.9.2\r\n",
      "parso                              0.7.1\r\n",
      "pathspec                           0.9.0\r\n",
      "pdfkit                             1.0.0\r\n",
      "Pebble                             5.0.3\r\n",
      "peewee                             3.13.2\r\n",
      "pexpect                            4.8.0\r\n",
      "pglast                             4.0\r\n",
      "phonenumbers                       8.13.16\r\n",
      "pickleshare                        0.7.5\r\n",
      "pika                               1.3.2\r\n",
      "Pillow                             9.3.0\r\n",
      "pip                                23.1.2\r\n",
      "pkgutil_resolve_name               1.3.10\r\n",
      "pkl2std                            2.3\r\n",
      "platformdirs                       3.8.1\r\n",
      "plotly                             5.11.0\r\n",
      "pluggy                             1.0.0\r\n",
      "ply                                3.11\r\n",
      "pmc-designer                       2.9\r\n",
      "polling2                           0.5.0\r\n",
      "pox                                0.3.2\r\n",
      "pretty-errors                      1.2.25\r\n",
      "prometheus-client                  0.13.1\r\n",
      "prompt-toolkit                     3.0.39\r\n",
      "protobuf                           3.20.3\r\n",
      "psutil                             5.9.5\r\n",
      "psycopg2                           2.8.5\r\n",
      "ptyprocess                         0.7.0\r\n",
      "pure-eval                          0.2.2\r\n",
      "py-mini-racer                      0.6.0\r\n",
      "py3nvml                            0.2.7\r\n",
      "py4j                               0.10.9.5\r\n",
      "py7zr                              0.16.1\r\n",
      "pyarrow                            12.0.1\r\n",
      "pyasn1                             0.5.0\r\n",
      "pyasn1-modules                     0.3.0\r\n",
      "pycairo                            1.23.0\r\n",
      "pycodestyle                        2.10.0\r\n",
      "pycparser                          2.21\r\n",
      "pycryptodome                       3.18.0\r\n",
      "pycryptodomex                      3.18.0\r\n",
      "pydeck                             0.8.1b0\r\n",
      "pyDeprecate                        0.3.1\r\n",
      "pydot                              1.4.2\r\n",
      "pyerfa                             2.0.0.3\r\n",
      "pyflakes                           3.0.1\r\n",
      "pygit2                             1.10.1\r\n",
      "Pygments                           2.14.0\r\n",
      "pygtrie                            2.5.0\r\n",
      "pyhcl                              0.4.4\r\n",
      "pyinotify                          0.9.6\r\n",
      "pylint                             2.17.4\r\n",
      "pymongo                            4.4.0\r\n",
      "Pympler                            1.0.1\r\n",
      "PyNaCl                             1.5.0\r\n",
      "pyOpenSSL                          21.0.0\r\n",
      "pyparsing                          3.0.9\r\n",
      "pypng                              0.20220715.0\r\n",
      "pyppmd                             0.18.0\r\n",
      "pyrsistent                         0.19.3\r\n",
      "pysftp                             0.2.9\r\n",
      "PySocks                            1.7.1\r\n",
      "pytest                             7.3.1\r\n",
      "python-benedict                    0.24.3\r\n",
      "python-dateutil                    2.8.2\r\n",
      "python-dotenv                      0.21.0\r\n",
      "python-fsutil                      0.10.0\r\n",
      "python-json-logger                 2.0.7\r\n",
      "python-jsonrpc-server              0.4.0\r\n",
      "python-language-server             0.36.2\r\n",
      "python-Levenshtein                 0.12.2\r\n",
      "python-slugify                     8.0.1\r\n",
      "pytorch-lightning                  1.9.5\r\n",
      "pytorch-lightning-bolts            0.3.2.post1\r\n",
      "pytz                               2022.7\r\n",
      "PyYAML                             5.4.1\r\n",
      "pyzmq                              25.1.0\r\n",
      "pyzstd                             0.14.4\r\n",
      "qtconsole                          5.4.3\r\n",
      "QtPy                               2.3.1\r\n",
      "reactlake-utils                    2.11.3\r\n",
      "redis                              4.4.2\r\n",
      "regex                              2023.6.3\r\n",
      "reportlab                          3.5.67\r\n",
      "requests                           2.28.1\r\n",
      "requests-oauthlib                  1.3.1\r\n",
      "requests-toolbelt                  0.10.1\r\n",
      "rfc3339-validator                  0.1.4\r\n",
      "rfc3986                            1.5.0\r\n",
      "rfc3986-validator                  0.1.1\r\n",
      "rich                               13.3.2\r\n",
      "rsa                                4.9\r\n",
      "ruamel.yaml                        0.17.32\r\n",
      "ruamel.yaml.clib                   0.2.7\r\n",
      "rxnmapper                          0.1.5.9\r\n",
      "s3fs                               2023.6.0\r\n",
      "s3transfer                         0.6.1\r\n",
      "sacremoses                         0.0.53\r\n",
      "safetensors                        0.4.3\r\n",
      "sagecipher                         0.7.5\r\n",
      "scikit-learn                       1.0.2\r\n",
      "scikit-learn-intelex               2021.20220215.212714\r\n",
      "scipy                              1.8.0\r\n",
      "scmrepo                            0.0.7\r\n",
      "seaborn                            0.11.2\r\n",
      "SecretStorage                      3.3.3\r\n",
      "Send2Trash                         1.8.2\r\n",
      "setproctitle                       1.2.2\r\n",
      "setuptools                         59.5.0\r\n",
      "shortuuid                          1.0.11\r\n",
      "shtab                              1.6.2\r\n",
      "simpervisor                        1.0.0\r\n",
      "simplejson                         3.17.6\r\n",
      "six                                1.16.0\r\n",
      "smmap                              5.0.0\r\n",
      "sniffio                            1.3.0\r\n",
      "soupsieve                          2.4.1\r\n",
      "SQLAlchemy                         1.4.39\r\n",
      "stack-data                         0.6.2\r\n",
      "std2pkl                            2.0\r\n",
      "streamlit                          1.22.0\r\n",
      "striprtf                           0.0.13\r\n",
      "structlog                          23.1.0\r\n",
      "sympy                              1.12\r\n",
      "tabulate                           0.9.0\r\n",
      "tenacity                           8.2.2\r\n",
      "tensorboard                        2.14.0\r\n",
      "tensorboard-data-server            0.7.2\r\n",
      "tensorboard-plugin-wit             1.8.1\r\n",
      "termcolor                          2.4.0\r\n",
      "terminado                          0.17.1\r\n",
      "text-unidecode                     1.3\r\n",
      "texttable                          1.6.7\r\n",
      "threadpoolctl                      2.2.0\r\n",
      "tinycss2                           1.2.1\r\n",
      "tokenizers                         0.19.1\r\n",
      "toml                               0.10.2\r\n",
      "tomli                              2.0.1\r\n",
      "tomlkit                            0.11.8\r\n",
      "toolz                              0.12.0\r\n",
      "torch                              2.0.0\r\n",
      "torch_geometric                    2.5.3\r\n",
      "torchaudio                         0.13.1\r\n",
      "torchmetrics                       1.4.0\r\n",
      "torchtyping                        0.1.4\r\n",
      "torchvision                        0.15.1\r\n",
      "tornado                            6.3.2\r\n",
      "tqdm                               4.63.0\r\n",
      "traitlets                          5.9.0\r\n",
      "transformers                       4.40.2\r\n",
      "triton                             2.0.0\r\n",
      "typeguard                          2.13.3\r\n",
      "typing_extensions                  4.11.0\r\n",
      "tzlocal                            5.0.1\r\n",
      "ujson                              5.8.0\r\n",
      "unwanted-filters                   2.8.1\r\n",
      "uri-template                       1.3.0\r\n",
      "urllib3                            1.26.8\r\n",
      "uWSGI                              2.0.18\r\n",
      "validators                         0.20.0\r\n",
      "voluptuous                         0.13.1\r\n",
      "watchdog                           3.0.0\r\n",
      "watchdog-gevent                    0.1.1\r\n",
      "wcwidth                            0.2.6\r\n",
      "webcolors                          1.13\r\n",
      "webencodings                       0.5.1\r\n",
      "websocket-client                   1.6.1\r\n",
      "websockets                         10.4\r\n",
      "Werkzeug                           2.2.2\r\n",
      "wheel                              0.38.4\r\n",
      "widgetsnbextension                 4.0.8\r\n",
      "wrapt                              1.15.0\r\n",
      "xgboost                            1.6.0\r\n",
      "xlrd                               2.0.1\r\n",
      "xmltodict                          0.12.0\r\n",
      "yarl                               1.9.2\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zc.lockfile                        3.0.post1\r\n",
      "zipp                               3.11.0\r\n",
      "zope.event                         5.0\r\n",
      "zope.interface                     6.0\r\n",
      "zstandard                          0.19.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1a20c54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_name = f\"tmp_1\"\n",
    "\n",
    "dir_load_model = None\n",
    "log_dir_folder = '/mnt/code/logs/'\n",
    "log_dir_folder = os.path.join(log_dir_folder, dir_name)\n",
    "if os.path.exists(log_dir_folder):\n",
    "    if os.path.exists(os.path.join(log_dir_folder, \"last.ckpt\")):\n",
    "        dir_load_model = os.path.join(log_dir_folder, \"last.ckpt\")\n",
    "    csv_path = os.path.join(log_dir_folder, \"metrics.csv\")\n",
    "    while os.path.exists(csv_path):\n",
    "        csv_path = csv_path + '.bak'\n",
    "    if os.path.exists(os.path.join(log_dir_folder, \"metrics.csv\")):\n",
    "        os.rename(os.path.join(log_dir_folder, \"metrics.csv\"), csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "50c750fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_to_monitor = \"val_epoch_loss\"\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=log_dir_folder,\n",
    "    monitor=metric_to_monitor,\n",
    "    mode = 'min',\n",
    "    save_top_k=1,\n",
    "    save_last=True,\n",
    "    every_n_epochs=5,\n",
    "    filename=\"best-model-{epoch}-{val_epoch_loss:.4f}\",\n",
    ")\n",
    "\n",
    "early_stopping = early_stop_callback = EarlyStopping(\n",
    "        monitor=metric_to_monitor,  # The metric you want to monitor\n",
    "        patience=config.patience,  # Number of epochs with no improvement after which training will be stopped\n",
    "        verbose=True,\n",
    "        mode='min'  # Minimizing the validation loss\n",
    "    )\n",
    "\n",
    "tb_logger = TensorBoardLogger(log_dir_folder, name=\"tensorbord\")#, version=\"\", default_hp_metric=False)\n",
    "csv_logger = CSVLogger(log_dir_folder, name=\"\", version=\"\")\n",
    "\n",
    "model_params = dict(\n",
    "    devices=1, #args['ngpus'],\n",
    "    accelerator='gpu', #args['accelerator'],\n",
    "    default_root_dir=log_dir_folder, #args['log_dir'],\n",
    "    logger=[tb_logger, csv_logger],\n",
    "    enable_progress_bar=True)\n",
    "\n",
    "\n",
    "model_params.update(dict(\n",
    "    max_epochs=config.num_epochs,#1000,\n",
    "    callbacks=[checkpoint_callback, early_stopping],\n",
    "    gradient_clip_val=10,#args['clip_norm'],\n",
    "    #precision=\"16-mixed\",\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "843bb182",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision('medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "787f4d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type    | Params\n",
      "------------------------------------\n",
      "0 | net     | LSTM    | 1.5 M \n",
      "1 | loss_fn | MSELoss | 0     \n",
      "------------------------------------\n",
      "1.5 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.5 M     Total params\n",
      "6.171     Total estimated model params size (MB)\n",
      "/opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages/lightning_fabric/loggers/csv_logs.py:183: UserWarning: Experiment logs directory /mnt/code/logs/tmp_1/ exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0048372745513916016,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Sanity Checking",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 192 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 192 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1609: PossibleUserWarning: The number of training batches (17) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.004137754440307617,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Training",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccacd3a407574ee3aa4503974a75653d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.005807638168334961,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Validation",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_epoch_loss improved. New best score: 0.964\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot pickle 'getset_descriptor' object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mTrainer(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_params)\u001b[38;5;66;03m#, profiler=profiler)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdir_load_model\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:608\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    606\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_unwrap_optimized(model)\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m_lightning_module \u001b[38;5;241m=\u001b[39m model\n\u001b[0;32m--> 608\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages/pytorch_lightning/trainer/call.py:38\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 38\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     41\u001b[0m     trainer\u001b[38;5;241m.\u001b[39m_call_teardown_hook()\n",
      "File \u001b[0;32m/opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:650\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    643\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m ckpt_path \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresume_from_checkpoint\n\u001b[1;32m    644\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_set_ckpt_path(\n\u001b[1;32m    645\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    646\u001b[0m     ckpt_path,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    647\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    648\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    649\u001b[0m )\n\u001b[0;32m--> 650\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    652\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    653\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1112\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39mrestore_training_state()\n\u001b[1;32m   1110\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39mresume_end()\n\u001b[0;32m-> 1112\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1114\u001b[0m log\u001b[38;5;241m.\u001b[39mdetail(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1115\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_teardown()\n",
      "File \u001b[0;32m/opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1191\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredicting:\n\u001b[1;32m   1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_predict()\n\u001b[0;32m-> 1191\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1214\u001b[0m, in \u001b[0;36mTrainer._run_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1211\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_loop\u001b[38;5;241m.\u001b[39mtrainer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m   1213\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1214\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages/pytorch_lightning/loops/loop.py:200\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madvance(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 200\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_advance_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py:295\u001b[0m, in \u001b[0;36mFitLoop.on_advance_end\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch_progress\u001b[38;5;241m.\u001b[39mincrement_processed()\n\u001b[1;32m    294\u001b[0m \u001b[38;5;66;03m# call train epoch end hooks\u001b[39;00m\n\u001b[0;32m--> 295\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_callback_hooks\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mon_train_epoch_end\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39m_call_lightning_module_hook(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_train_epoch_end\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39m_logger_connector\u001b[38;5;241m.\u001b[39mon_epoch_end()\n",
      "File \u001b[0;32m/opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1394\u001b[0m, in \u001b[0;36mTrainer._call_callback_hooks\u001b[0;34m(self, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1392\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callable(fn):\n\u001b[1;32m   1393\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Callback]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcallback\u001b[38;5;241m.\u001b[39mstate_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1394\u001b[0m             \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlightning_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1396\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pl_module:\n\u001b[1;32m   1397\u001b[0m     \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m   1398\u001b[0m     pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m/opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:305\u001b[0m, in \u001b[0;36mModelCheckpoint.on_train_epoch_end\u001b[0;34m(self, trainer, pl_module)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_every_n_epochs \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m (trainer\u001b[38;5;241m.\u001b[39mcurrent_epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_every_n_epochs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_topk_checkpoint(trainer, monitor_candidates)\n\u001b[0;32m--> 305\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_last_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmonitor_candidates\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:650\u001b[0m, in \u001b[0;36mModelCheckpoint._save_last_checkpoint\u001b[0;34m(self, trainer, monitor_candidates)\u001b[0m\n\u001b[1;32m    648\u001b[0m \u001b[38;5;66;03m# set the last model path before saving because it will be part of the state.\u001b[39;00m\n\u001b[1;32m    649\u001b[0m previous, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_model_path, filepath\n\u001b[0;32m--> 650\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    651\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m previous \u001b[38;5;129;01mand\u001b[39;00m previous \u001b[38;5;241m!=\u001b[39m filepath:\n\u001b[1;32m    652\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_remove_checkpoint(trainer, previous)\n",
      "File \u001b[0;32m/opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:366\u001b[0m, in \u001b[0;36mModelCheckpoint._save_checkpoint\u001b[0;34m(self, trainer, filepath)\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_save_checkpoint\u001b[39m(\u001b[38;5;28mself\u001b[39m, trainer: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpl.Trainer\u001b[39m\u001b[38;5;124m\"\u001b[39m, filepath: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 366\u001b[0m     \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_weights_only\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    368\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_global_step_saved \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mglobal_step\n\u001b[1;32m    370\u001b[0m     \u001b[38;5;66;03m# notify loggers\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1939\u001b[0m, in \u001b[0;36mTrainer.save_checkpoint\u001b[0;34m(self, filepath, weights_only, storage_options)\u001b[0m\n\u001b[1;32m   1934\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1935\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   1936\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaving a checkpoint is only possible if a model is attached to the Trainer. Did you call\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1937\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `Trainer.save_checkpoint()` before calling `Trainer.\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124mfit,validate,test,predict}`?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1938\u001b[0m     )\n\u001b[0;32m-> 1939\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_checkpoint_connector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py:511\u001b[0m, in \u001b[0;36mCheckpointConnector.save_checkpoint\u001b[0;34m(self, filepath, weights_only, storage_options)\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Save model/training states as a checkpoint file through state-dump and file-write.\u001b[39;00m\n\u001b[1;32m    504\u001b[0m \n\u001b[1;32m    505\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;124;03m    storage_options: parameter for how to save to storage, passed to ``CheckpointIO`` plugin\u001b[39;00m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    510\u001b[0m _checkpoint \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdump_checkpoint(weights_only)\n\u001b[0;32m--> 511\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_checkpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py:466\u001b[0m, in \u001b[0;36mStrategy.save_checkpoint\u001b[0;34m(self, checkpoint, filepath, storage_options)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Save model/training states as a checkpoint file through state-dump and file-write.\u001b[39;00m\n\u001b[1;32m    459\u001b[0m \n\u001b[1;32m    460\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;124;03m    storage_options: parameter for how to save to storage, passed to ``CheckpointIO`` plugin\u001b[39;00m\n\u001b[1;32m    464\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_global_zero:\n\u001b[0;32m--> 466\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheckpoint_io\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages/lightning_fabric/plugins/io/torch_io.py:54\u001b[0m, in \u001b[0;36mTorchCheckpointIO.save_checkpoint\u001b[0;34m(self, checkpoint, path, storage_options)\u001b[0m\n\u001b[1;32m     51\u001b[0m fs\u001b[38;5;241m.\u001b[39mmakedirs(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(path), exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;66;03m# write the checkpoint dictionary on the file\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m     \u001b[43m_atomic_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;66;03m# todo: is this try catch necessary still?\u001b[39;00m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;66;03m# https://github.com/Lightning-AI/lightning/pull/431\u001b[39;00m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;66;03m# TODO(fabric): Fabric doesn't support hyperparameters in the checkpoint, so this should be refactored\u001b[39;00m\n\u001b[1;32m     59\u001b[0m     key \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhyper_parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages/lightning_fabric/utilities/cloud_io.py:70\u001b[0m, in \u001b[0;36m_atomic_save\u001b[0;34m(checkpoint, filepath)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Saves a checkpoint atomically, avoiding the creation of incomplete checkpoints.\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \n\u001b[1;32m     62\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;124;03m        This points to the file that the checkpoint will be stored in.\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     69\u001b[0m bytesbuffer \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mBytesIO()\n\u001b[0;32m---> 70\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbytesbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m fsspec\u001b[38;5;241m.\u001b[39mopen(filepath, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     72\u001b[0m     f\u001b[38;5;241m.\u001b[39mwrite(bytesbuffer\u001b[38;5;241m.\u001b[39mgetvalue())\n",
      "File \u001b[0;32m/opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages/torch/serialization.py:628\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    626\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m--> 628\u001b[0m         \u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_disable_byteorder_record\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    629\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/envs/chem38_c20230703/lib/python3.8/site-packages/torch/serialization.py:840\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    838\u001b[0m pickler \u001b[38;5;241m=\u001b[39m pickle_module\u001b[38;5;241m.\u001b[39mPickler(data_buf, protocol\u001b[38;5;241m=\u001b[39mpickle_protocol)\n\u001b[1;32m    839\u001b[0m pickler\u001b[38;5;241m.\u001b[39mpersistent_id \u001b[38;5;241m=\u001b[39m persistent_id\n\u001b[0;32m--> 840\u001b[0m \u001b[43mpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    841\u001b[0m data_value \u001b[38;5;241m=\u001b[39m data_buf\u001b[38;5;241m.\u001b[39mgetvalue()\n\u001b[1;32m    842\u001b[0m zip_file\u001b[38;5;241m.\u001b[39mwrite_record(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m, data_value, \u001b[38;5;28mlen\u001b[39m(data_value))\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot pickle 'getset_descriptor' object"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(**model_params)#, profiler=profiler)\n",
    "trainer.fit(model, datamodule=data, ckpt_path=dir_load_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9bff1c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.utilities.parsing import is_picklable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ba90d5ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.360118865966797"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(model.logging_info[\"train_loss\"]).mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "90de3575",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.66825008392334"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(model.logging_info[\"train_loss\"]).mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5fa189",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e9f7f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d090a7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = generate_checkpoint_filename()\n",
    "early_stopping = EarlyStopping(patience=config.patience, path=checkpoint_path)\n",
    "print(f'Checkpoint path: {checkpoint_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bb2800",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74ed650",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)\n",
    "\n",
    "if config.scheduler == 'ReduceLROnPlateau':\n",
    "    scheduler = ReduceLROnPlateau(\n",
    "        optimizer, verbose=True, **asdict(config.reduce_lr_on_plateau))\n",
    "elif config.scheduler == 'CosineAnnealingLR':\n",
    "    scheduler = CosineAnnealingLR(\n",
    "        optimizer, T_max=config.num_epochs, verbose=True, **asdict(config.cosine_annealing_lr))\n",
    "elif config.scheduler == 'LinearWarmupCosineAnnealingLR':\n",
    "    scheduler = LinearWarmupCosineAnnealingLR(\n",
    "        optimizer, **asdict(config.linear_warmup_cosine_annealing_lr))\n",
    "else:\n",
    "    scheduler = None\n",
    "\n",
    "best_val_error = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848e9551",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(config.num_epochs):\n",
    "    loss = train(train_loader)\n",
    "    if scheduler is not None:\n",
    "        scheduler.step(loss)\n",
    "    valid_error = eval(valid_loader)\n",
    "\n",
    "    early_stopping(valid_error, model)\n",
    "    if early_stopping.counter == 0:\n",
    "        test_error = eval(test_loader)\n",
    "    if early_stopping.early_stop:\n",
    "        print('Early stopping...')\n",
    "        break\n",
    "\n",
    "    writer.add_scalar(f'Loss_{config.model1d.model}/{config.model1d.input_type}'\n",
    "                      f'/{config.dataset}/{config.target}/train', loss, epoch)\n",
    "    writer.add_scalar(f'Loss_{config.model1d.model}/{config.model1d.input_type}'\n",
    "                      f'/{config.dataset}/{config.target}/valid', valid_error, epoch)\n",
    "    writer.add_scalar(f'Loss_{config.model1d.model}/{config.model1d.input_type}'\n",
    "                      f'/{config.dataset}/{config.target}/test', test_error, epoch)\n",
    "    print(f'Progress: {epoch}/{config.num_epochs}/{loss:.5f}/{valid_error:.5f}/{test_error:.5f}')\n",
    "    res_dict[dataname][algo][d]['loss'].append(loss)\n",
    "    res_dict[dataname][algo][d]['valid_error'].append(valid_error)\n",
    "    res_dict[dataname][algo][d]['test_error'].append(test_error)\n",
    "\n",
    "model.load_state_dict(torch.load(checkpoint_path))\n",
    "test_error = eval(test_loader)\n",
    "test_r2 = eval_r2(test_loader)\n",
    "print(f'Best validation error: {-early_stopping.best_score:.7f}')\n",
    "print(f'Test error: {test_error:.7f}')\n",
    "print(f'Test r2: {test_r2:.7f}')\n",
    "\n",
    "res_dict[dataname][algo][d]['Test error'] = test_error\n",
    "res_dict[dataname][algo][d]['Test r2'] = test_r2\n",
    "res_dict[dataname][algo][d]['checkpoint']=checkpoint_path\n",
    "\n",
    "\n",
    "#os.remove(checkpoint_path)\n",
    "writer.close()\n",
    "\n",
    "import pickle\n",
    "with open(\"res_dict_1d.pkl\", \"wb\") as f:\n",
    "pickle.dump(res_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcca3d98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
